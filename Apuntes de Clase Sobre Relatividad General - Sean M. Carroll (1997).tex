\documentclass[11pt,b5paper,openany,twoside]{book}
%\documentclass[12pt,b5paper,onecolumn,twoside,notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{physics}


\usepackage[left=1.50cm, right=1.50cm, top=2.00cm, bottom=2.00cm]{geometry}
\usepackage[spanish]{babel}

% -----------------------------------------------------------------------
\usepackage[usenames]{color}
\definecolor{AzulEnlace}{rgb}{0.000,0.078,0.518}
\definecolor{GrisAzul}{RGB}{10,10,40}

\definecolor{OcreA}{rgb}{0.941, 0.906, 0.800}
\definecolor{OcreB}{rgb}{0.984, 0.969, 0.965}
\definecolor{OcreC}{rgb}{0.980, 0.965, 0.918}
\definecolor{OcreD}{rgb}{0.996, 0.973, 0.902}

% -----------------------------------------------------------------------
\usepackage{float}
\usepackage{latexsym}
\usepackage{hyphenat}
\usepackage{array}
\usepackage{eso-pic}

% -----------------------------------------------------------------------
\newcommand*{\bigone}{\text{\usefont{U}{bbold}{m}{n}1}}
\newcommand*{\vcol}[1]{\left\lgroup\begin{matrix} #1 \end{matrix}\right\rgroup}

% -----------------------------------------------------------------------
\usepackage{fancyhdr}

\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\bfseries\rightmark}
\fancyhead[RE]{\bfseries\leftmark}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\addtolength{\headheight}{0.5pt}
\setlength{\footskip}{0in}
\renewcommand{\footruleskip}{0pt}
\fancypagestyle{plain}{%
    \fancyhead{}
    \renewcommand{\headrulewidth}{0pt}
}

% -----------------------------------------------------------------------
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mdframed}
%\colorlet{shadecolor}{orange!15}
%\definecolor{RojoLadrillo}{RGB}{148,49,43}
%\definecolor{VerdeDifuso}{rgb}{0.698, 1.0, 0.698}

\newmdenv[linewidth = 0pt,
leftmargin = -5,
rightmargin = -5,
linewidth = 0.5pt,
topline=true,
bottomline=true,
leftline = false,
rightline = false,
linecolor = OcreA,
backgroundcolor = OcreB,
innertopmargin = \topskip,
innerbottommargin = \topskip,
splittopskip = \topskip,
footnoteinside = false,]{CajaColor}

% -----------------------------------------------------------------------
\usepackage{titlesec}
\titleformat{\section}{\color{GrisAzul}\normalfont\Large\bfseries\raggedright}{\color{GrisAzul}\thesection}{0.8em}{}


\newcommand{\mn}{{\mu\nu}}
\newcommand{\e}[1]{\hat{e}_{(#1)}}

\let\t\relax
%\newcommand{\t}[1]{{\hat{\theta}}^{(#1)}}
\newcommand{\ztheta}[1]{{\hat{\theta}}^{(#1)}}
\newcommand{\p}[1]{{\partial_{#1}}}
\newcommand{\R}{\mathbb{R}}

%\def\d{{\rm d}}
\def\g{{\sqrt{-g}}}
\def\lie{\pounds}
\def\bh{{\bar h}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}

\let\bigone\relax
\newcommand*{\bigone}{\text{\usefont{U}{bbold}{m}{n}1}}

\extrafloats{100}

% -----------------------------------------------------------------------
\usepackage{hyperref}            %Modifica el aspecto de enlaces, para navegar dentro del libro
\hypersetup{colorlinks=true,breaklinks=true,citecolor=AzulEnlace,linkcolor=AzulEnlace,filecolor=AzulEnlace,urlcolor=AzulEnlace}

% -----------------------------------------------------------------------

\title{\textcolor{GrisAzul}{{\Huge \textbf{Apuntes de Clase sobre\\[6mm] Relatividad General}}}}

\author{Sean M. Carroll\hspace{2pt}\thanks{e-mail: \href{carroll@itp.ucsb.edu}{carroll@itp.ucsb.edu}} \\[14mm]
Instituto de Física Teórica, Universidad de California \\
Santa Bárbara, CA 93106, EE.UU \\[2mm]
Diciembre de 1997 \\[10mm]
%{\small e-mail: \href{carroll@itp.ucsb.edu}{carroll@itp.ucsb.edu}}
\parbox{10.6cm}{Estas notas representan aproximadamente un semestre de conferencias sobre introducción a la Relatividad General para estudiantes principiantes de posgrado en física.
Los temas incluyen variedades, geometría de Riemann, ecuaciones de Einstein y tres aplicaciones: radiación gravitacional, agujeros negros y cosmología.
Los capítulos individuales y las versiones potencialmente actualizadas se pueden encontrar en \href{http://itp.ucsb.edu/carroll/notes/}{http://itp.ucsb.edu/carroll/notes/}.}
%\makebox[1in]{} \\
}

\date{}


\begin{document}

%\thispagestyle{plain}


\noindent
%w{{NSF-ITP/97-147}\hfill{gr-qc/9712019}}

% -----------------------------------------------------------------------------------------
\thispagestyle{plain}

\frontmatter

\maketitle

\begin{quote}
{\footnotesize
\noindent
arXiv:gr-qc/9712019v13Dec1997 \\ \href{https://arxiv.org/abs/gr-qc/9712019}{https://arxiv.org/abs/gr-qc/9712019}

\noindent
\textbf{Apuntes de Clase sobre Relatividad General}\\
\textit{Autor}: Sean M. Carroll \\
\textit{Comentarios}: 238 páginas, numerosas figuras. \\
\textit{Número de informe}: NSF-ITP/97-147

{{NSF-ITP/97-147}\hfill{gr-qc/9712019}}


arXiv.org - Asumida licencia no exclusiva para distribuir.

El URI \href{http://arxiv.org/licenses/assumed-1991-2003/}{http://arxiv.org/licenses/assumed-1991-2003/} se utiliza para indicar esta licencia asumida.

Antes de enero de 2004, no existía ninguna certificación/concesión de licencia explícita como parte del proceso de envío de archivos de impresión electrónica de arXiv.org (anteriormente xxx.lanl.gov).

Se asumió que por la acción intencional de envío y páginas de ayuda que establecen las políticas de disponibilidad de archivos de los envíos, se otorgó una licencia equivalente a http://arxiv.org/licenses/nonexclusive-distrib/1.0/ mediante el acto de envío. Por lo tanto, operamos arXiv.org bajo el supuesto de que para estos envíos:

\begin{itemize}
\item arXiv.org tiene una licencia perpetua y no exclusiva para distribuir este artículo.
\item El remitente tenía derecho a otorgar esta licencia.
\item Que los envíos no se eliminarán por completo una vez aceptados.
\item arXiv.org tiene el derecho de reclasificar cualquier envío.
\end{itemize}

\textit{Revisión histórica}

14/08/1991 - Primer envío a arXiv (luego recién hep-th) realizado (hep-th/9108001)\\
21/06/2007 - Esta página HTML se creó 16/12/2008 - Agregar URI para esta licencia explícitamente
}
\end{quote}
%\newpage

{\normalfont\fontsize{9.4pt}{1em}\selectfont
    \thispagestyle{plain}
    \begin{quote}
        \tableofcontents
        \thispagestyle{plain}
    \end{quote}
    \normalfont\selectfont}

\mainmatter

\chapter*{Prólogo}
\addcontentsline{toc}{chapter}{Prólogo}

Estas conferencias representan un curso de posgrado introductorio a la Relatividad General, tanto sus fundamentos como sus aplicaciones.
Son una versión ligeramente editada de notas que entregué mientras enseñaba Física 8.962, el curso de posgrado en Relatividad General en el MIT, durante la primavera de 1996.
Aunque se les llama apropiadamente ``notas de clase'', el nivel de detalle es bastante alto, ya sea incluyendo todos los pasos necesarios o dejando espacios en blanco que el lector puede completar fácilmente.
Sin embargo, hay varios aspectos en los que estas notas se diferencian de un libro de texto; Lo más importante es que no están organizados en secciones cortas que puedan abordarse en varios órdenes, sino que deben repasarse de principio a fin.
Se ha hecho un esfuerzo especial por mantener un tono conversacional, en un intento de ir un poco más allá de los resultados en sí mismos y entrar en el contexto al que pertenecen.

La pregunta principal que enfrenta cualquier tratamiento introductorio de la Relatividad General es el nivel de rigor matemático con el cual operar.
No existe una solución única y adecuada, ya que diferentes estudiantes responderán con diferentes niveles de comprensión y entusiasmo a diferentes enfoques.
Reconociendo esto, he tratado de ofrecer algo para todos.
Las conferencias no rehuyen el formalismo detallado (como por ejemplo en la introducción a las variedades), pero también intentan incluir ejemplos concretos y discusiones informales de los conceptos bajo consideración.

Como se anuncian como apuntes de conferencias y no como textos originales, a veces los he robado descaradamente de varios libros existentes sobre el tema (especialmente los de Schutz, Wald, Weinberg y Misner, Thorne y Wheeler).
Mi filosofía nunca fue intentar buscar la originalidad por sí misma; sin embargo, a veces la originalidad aparecía simplemente porque pensaba que podía ser más claro que los tratamientos existentes.
Nada del contenido sustancial de estas notas es nuevo; la única razón para leerlos es si un lector individual encuentra las explicaciones aquí más fáciles de entender que las de otros lugares.

Las limitaciones de tiempo durante el semestre me impidieron cubrir algunos temas con la profundidad que merecían, un ejemplo obvio es el tratamiento de la cosmología.
Si llega el tiempo y la motivación, podré ampliar y revisar las notas existentes; Las versiones actualizadas estarán disponibles en \href{http://itp.ucsb.edu/carroll/notes/}{http://itp.ucsb.edu/carroll/notes/}.
Por supuesto agradeceré que me llamen la atención sobre cualquier error tipográfico o científico, así como sugerencias de mejora de todo tipo.

Numerosas personas han contribuido enormemente a mi propia comprensión de la Relatividad General y a estas notas en particular, demasiadas para reconocerlas con la esperanza de que estén completas.
Debo un agradecimiento especial a Ted Pyne, quien aprendió el tema junto conmigo, me enseñó mucho y colaboró en un predecesor de este curso que impartimos como seminario en el departamento de astronomía de Harvard.
Nick Warner impartió el curso de posgrado en el MIT que yo tomé antes de impartirlo, y sus notas fueron (como revelará la comparación) una influencia importante en ellos.
George Field me ofreció muchos consejos y aliento mientras aprendía el tema y me esforzaba por enseñarlo.
Tamás Hauer trabajó conmigo como asistente de enseñanza durante 8.962 y fue una ayuda invaluable.
Todos los estudiantes de 8.962 merecen gracias por tolerar mis idiosincrasias y empujarme a niveles cada vez más altos de precisión.

Mientras escribía estas notas conté con el apoyo de EE.UU.
Contrato del Departamento de Energía no. DE-AC02-76ER03069 y subvenciones de la Fundación Nacional de Ciencias PHY/92-06867 y PHY/94-07195.

% -----------------------------------------------------------------------------------------
%\newpage

\chapter{Relatividad Especial y Espacio-Tiempo Plano}
%\addcontentsline{toc}{chapter}{Relatividad Especial y Espacio-Tiempo Plano}

Comenzaremos con un recorrido relámpago por la Relatividad Especial y la vida en el Espacio-Tiempo plano.
El punto será recordar de qué se trata Relatividad Especial e introducir tensores y conceptos relacionados que serán cruciales más adelante, sin las complicaciones adicionales de la curvatura además de todo lo demás.
Por lo tanto, para esta sección siempre trabajaremos en Espacio-Tiempo plano y además solo usaremos coordenadas ortonormales (de tipo cartesiano).
Ni que decir tiene que es posible hacer Relatividad Especial en cualquier sistema de coordenadas que se desee, pero resulta que introducir las herramientas necesarias para hacerlo nos llevaría de todos modos a la mitad del camino hacia los espacios curvos, así que lo dejaremos para más adelante.

A menudo se dice que la Relatividad Especial es una teoría del Espacio-Tiempo de 4 dimensiones: tres de espacio, una de tiempo.
Pero, por supuesto, el mundo de la mecánica newtoniana anterior a la Relatividad Especial presentaba tres dimensiones espaciales y un parámetro temporal.
Sin embargo, no hubo mucha tentación de considerarlos como aspectos diferentes de un único Espacio-Tiempo de 4 dimensiones.
¿Por qué no?

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/one1.pdf}
\end{figure}

% \begin{figure}
% \centering
% \includegraphics[width=0.7\linewidth]{imagenes/eight1}
% \caption{dd}
% \label{fig:eight1}
% \end{figure}

Considere un plano bidimensional tipo jardín.
Normalmente resulta conveniente etiquetar los puntos en dicho plano introduciendo coordenadas, por ejemplo definiendo los ejes ortogonales $x$ y $y$ y proyectando cada punto sobre estos ejes de la forma habitual.
Sin embargo, está claro que la mayoría de los datos geométricos interesantes sobre el plano son independientes de nuestra elección de coordenadas.
Como ejemplo simple, podemos considerar la distancia entre dos puntos, dada por
\begin{equation}
(\Delta s)^2 = (\Delta x)^2 + (\Delta y)^2\,.\label{1.1}
\end{equation}
En un sistema de coordenadas cartesiano diferente, definido por los ejes $x'$ y $y'$ que están girados con respecto a los originales, la fórmula de la distancia no se modifica:
\begin{equation}
(\Delta s)^2 = (\Delta x')^2 + (\Delta y')^2\,.\label{1.2}
\end{equation}
Por tanto decimos que la distancia es invariante ante tales cambios de coordenadas.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/one2.pdf}
\end{figure}

Por eso es útil pensar en el plano como bidimensional: aunque usamos dos números distintos para etiquetar cada punto, los números no son la esencia de la geometría, ya que podemos rotar los ejes entre sí dejando distancias, etc. adelante sin cambios.
En la física newtoniana este no es el caso del espacio y el tiempo; no existe una noción útil de rotar el espacio y el tiempo entre sí.
Más bien, la noción de ``todo el espacio en un único instante en el tiempo'' tiene un significado independiente de las coordenadas.

Éste no es el caso en Relatividad Especial.
Consideremos las coordenadas $(t,x,y,z)$ en el Espacio-Tiempo, configuradas de la siguiente manera.
Las coordenadas espaciales $(x,y,z)$ comprenden un sistema cartesiano estándar, construido por ejemplo soldando entre sí varillas rígidas que se encuentran en ángulo recto.
Las varillas deben moverse libremente y sin aceleración.
La coordenada temporal está definida por un conjunto de relojes que no se mueven con respecto a las coordenadas espaciales.
(Dado que se trata de un experimento mental, imaginamos que las varillas son infinitamente largas y que hay un reloj en cada punto del espacio).
Los relojes se sincronizan en el siguiente sentido: si viajas de un punto del espacio a cualquier otro en línea recta a velocidad constante, la diferencia horaria entre los relojes al final de tu viaje es la misma que si hubieras hecho el mismo viaje, a la misma velocidad, en la otra dirección.
El sistema de coordenadas así construido es un {\bf sistema inercial}.

Un {\bf evento} se define como un instante único en el espacio y el tiempo, caracterizado únicamente por $(t,x,y,z)$.
Luego, sin ninguna motivación por ahora, introduzcamos el {\bf intervalo espacio-temporal} entre dos eventos:
\begin{equation}
(\Delta s)^2 = -(c\Delta t)^2 + (\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2\,.
\label{1.3}
\end{equation}
(Observe que puede ser positivo, negativo o cero incluso para dos puntos no idénticos).
Aquí, $c$ es un factor de conversión fijo entre espacio y tiempo; es decir, una velocidad fija.
Por supuesto, resultará ser la velocidad de la luz; Lo importante, sin embargo, no es que los fotones viajen a esa velocidad, sino que existe un $c$ tal que el intervalo espacio-temporal sea invariante ante cambios de coordenadas.
En otras palabras, si configuramos un nuevo marco inercial $(t',x',y',z')$ repitiendo nuestro procedimiento anterior, pero permitiendo un desplazamiento en la posición inicial, el ángulo y la velocidad entre las nuevas barras y las antiguas, el intervalo no cambia:
\begin{equation}
(\Delta s)^2 = -(c\Delta t')^2 + (\Delta x')^2 + (\Delta y')^2 + (\Delta z')^2\,.
\label{1.4}
\end{equation}
Por eso tiene sentido pensar en la Relatividad Especial como una teoría del Espacio-Tiempo de 4 dimensiones, conocida como {\bf espacio de Minkowski}.
(Este es un caso especial de una variedad de 4 dimensiones, que trataremos en detalle más adelante).
Como veremos, las transformaciones de coordenadas que hemos definido implícitamente, en cierto sentido, rotan el espacio y el tiempo entre sí.
No existe una noción absoluta de ``eventos simultáneos''; Que dos cosas ocurran al mismo tiempo depende de las coordenadas utilizadas.
Por lo tanto, la división del espacio de Minkowski en espacio y tiempo es una elección que hacemos para nuestros propios fines, no algo intrínseco a la situación.

Casi todas las ``paradojas'' asociadas con la RS resultan de una obstinada persistencia de las nociones newtonianas de una coordenada temporal única y la existencia de ``espacio en un único instante en el tiempo''. espacio y tiempo juntos, estas paradojas tienden a desaparecer.

Introduzcamos alguna notación conveniente.
Las coordenadas en el Espacio-Tiempo se indicarán mediante letras con índices de superíndice griego que van desde $0$ hasta $3$, donde $0$ generalmente indica la coordenada de tiempo.
De este modo,
\begin{equation}
x^\mu :\quad \mqty{x^0 = ct  \\  x^1 = x \\  x^2 = y \\  x^3 = z \\ }
% \pmqty{matrix element 1 & matrix element 2 \\ matrix element 3 & matrix element 4}
\label{1.5}
\end{equation}
(No empieces a pensar en los superíndices como exponentes).
Además, en aras de la simplicidad elegiremos unidades en las que
\begin{equation}
c = 1 \ ;\label{1.6}
\end{equation}
por lo tanto, omitiremos factores de $c$ en todas las fórmulas posteriores.
Empíricamente sabemos que $c$ es la velocidad de la luz, $3\times 10^8$ metros por segundo; por lo tanto, estamos trabajando en unidades donde 1 segundo equivale a $3\times 10^8$ metros.
A veces será útil hacer referencia a los componentes de espacio y tiempo de $x^\mu$ por separado, por lo que usaremos superíndices latinos para representar solo los componentes de espacio:
\begin{equation}
x^i :\quad \mqty{x^1 = x \\  x^2 = y \\  x^3 = z \\ }\label{1.7}
\end{equation}

También es conveniente escribir el intervalo espacio-temporal en una forma más compacta.
Por lo tanto, introducimos una matriz $4\times 4$, la {\bf métrica}, que escribimos usando dos índices inferiores:
\begin{equation}
\eta_{\mn} = \left(\mqty{-1 &0&0&0 \\  0&1&0&0 \\
0&0&1&0  \\  0&0&0&1 \\ }\right)\,.\label{1.8}
\end{equation}
(Algunas referencias, especialmente libros de teoría de campos, definen la métrica con el signo opuesto, así que tenga cuidado).
Entonces tenemos la buena fórmula.
\begin{equation}
s^2 = \eta_\mn \Delta x^\mu \Delta x^\nu\,. \label{1.9}
\end{equation}
Observe que utilizamos la {\bf convención de sumatoria}, en la que los índices que aparecen como superíndices y subíndices se suman.
Por tanto, el contenido de (1.9) es exactamente el mismo que el de (1.3).

Ahora podemos considerar las transformaciones de coordenadas en el Espacio-Tiempo a un nivel algo más abstracto que antes.
¿Qué tipo de transformaciones dejan invariante el intervalo (1.9)? Una variedad simple son las traslaciones, que simplemente cambian las coordenadas:
\begin{equation}
x^\mu \rightarrow x^{\mu'} = x^\mu + a^\mu\ ,\label{1.10}
\end{equation}
donde $a^\mu$ es un conjunto de cuatro números fijos.
(Observe que colocamos el número primo en el índice, no en $x$.)
Las traducciones dejan las diferencias $\Delta x^\mu$ sin cambios, por lo que no es destacable que el intervalo no cambie.
El único otro tipo de transformación lineal es multiplicar $x^\mu$ por una matriz (independiente del Espacio-Tiempo):
\begin{equation}
x^{\mu'} = \Lambda^{\mu'}{}_\nu x^\nu\ , \label{1.11}
\end{equation}
o, en notación matricial más convencional,
\begin{equation}
x' = \Lambda x\,.\label{1.12}
\end{equation}
Estas transformaciones no dejan las diferencias $\Delta x^\mu$ sin cambios, sino que las multiplican también por la matriz $\Lambda$.
¿Qué tipo de matrices dejarán invariante el intervalo? Siguiendo con la notación matricial, lo que nos gustaría es
\begin{align}
s^2 = (\Delta x)^{\rm T} \eta (\Delta x)
& = (\Delta x')^{\rm T} \eta (\Delta x')\nonumber \\
& = (\Delta x)^{\rm T} \Lambda^{\rm T} \eta \Lambda (\Delta x)\ ,
\label{1.13}
\end{align}
y por lo tanto
\begin{equation}
\eta = \Lambda^{\rm T} \eta \Lambda \ ,\label{1.14}
\end{equation}
o
\begin{equation}
\eta_{\rho\sigma} = \Lambda^{\mu'}{}_{\rho}\Lambda^{\nu'}{}_{\sigma}
\eta_{\mu'\nu'}\,. \label{1.15}
\end{equation}
Queremos encontrar las matrices $\Lambda^{\mu'}{}_\nu$ tales que los componentes de la matriz $\eta_{\mu'\nu'}$ sean los mismos que los de $\eta_{\rho\sigma}$; eso es lo que significa que el intervalo sea invariante bajo estas transformaciones.

Las matrices que satisfacen (1.14) se conocen como {\bf transformaciones de Lorentz}; el conjunto de ellos forma un grupo bajo multiplicación de matrices, conocido como {\bf grupo de Lorentz}.
Existe una estrecha analogía entre este grupo y O(3), el grupo de rotación en el espacio tridimensional.
El grupo de rotación se puede considerar como matrices $3\times 3$ $R$ que satisfacen
\begin{equation}
\bigone = R^{\rm T} \bigone  R\ ,\label{1.16}
\end{equation}
donde \bigone es la matriz identidad $3\times 3$.
La similitud con (1.14) debería ser clara; la única diferencia es el signo menos en el primer término de la métrica $\eta$, que indica la dirección temporal.
Por lo tanto, al grupo de Lorentz se le suele denominar O(3,1).
% (
La matriz de identidad $3\times 3$ es simplemente la métrica del espacio plano ordinario.
Una métrica de este tipo, en la que todos los valores propios son positivos, se denomina {\bf euclidiana}, mientras que aquellas como (1.8) que presentan un único signo menos se denominan {\bf lorentziana}.
% )

Las transformaciones de Lorentz se dividen en varias categorías.
Primero están las {\bf rotaciones} convencionales, como una rotación en el plano $x$-$y$:
\begin{equation}
\Lambda^{\mu'}{}_\nu = \left(\mqty{ 1&0&0&0 \\
0& \cos\theta & \sin\theta &0 \\  0& -\sin\theta & \cos\theta &0 \\
0&0&0&1 \\ }\right)\,.\label{1.17}
\end{equation}
El ángulo de rotación $\theta$ es una variable periódica con período $2\pi$.
También hay un {\bf momentum}, que puede considerarse como ``rotaciones entre las direcciones del espacio y del tiempo''.
\begin{equation}
\Lambda^{\mu'}{}_\nu = \left(\mqty{ \cosh\phi&-\sinh\phi &0&0 \\
-\sinh\phi&\cosh\phi &0&0 \\  0&0&1&0 \\
0&0&0&1 \\ }\right)\,.\label{1.18}
\end{equation}
El parámetro de momentum $\phi$, a diferencia del ángulo de rotación, se define de $-\infty$ a $\infty$.
También hay transformaciones discretas que invierten la dirección del tiempo o una o más de las direcciones espaciales.
(Cuando se excluyen, tenemos el grupo de Lorentz adecuado, SO(3,1).)
Se puede obtener una transformación general multiplicando las transformaciones individuales; La expresión explícita de esta matriz de seis parámetros (tres momentos, tres rotaciones) no es lo suficientemente bonita ni útil como para molestarse en escribirla.
En general, las transformaciones de Lorentz no conmutarán, por lo que el grupo de Lorentz no es abeliano.
El conjunto de traslaciones y transformaciones de Lorentz es un grupo no abeliano de diez parámetros, el {\bf grupo de Poincaré}.

No debería sorprenderte saber que los momentos corresponden a cambios de coordenadas al moverte a un marco que viaja a una velocidad constante, pero veámoslo más explícitamente.
Para la transformación dada por (1.18), las coordenadas transformadas $t'$ y $x'$ vendrán dadas por
\begin{align}
t' &=  t\cosh\phi - x \sinh\phi \nonumber \\
x' &=  -t \sinh\phi + x\cosh\phi\,.\label{1.19}
\end{align}
De esto vemos que el punto definido por $x'=0$ se está moviendo; tiene una velocidad
\begin{equation}
v = {x\over t} = {{\sinh\phi}\over{\cosh\phi}} = \tanh\phi\,.
\label{1.20}
\end{equation}
Para traducir a una notación más peatonal, podemos reemplazar $\phi = \tanh^{-1}v$ para obtener
\begin{align}
t' &=  \gamma(t-vx)\nonumber \\
x' &=  \gamma(x-vt)\label{1.21}
\end{align}
donde $\gamma =1/\sqrt{1-v^2}$.
De hecho, nuestro enfoque abstracto ha recuperado las expresiones convencionales para las transformaciones de Lorentz.
La aplicación de estas fórmulas conduce a la dilatación del tiempo, la contracción de la longitud, etc.

Una herramienta extremadamente útil es el {\bf diagrama Espacio-Tiempo}, así que consideremos el espacio de Minkowski desde este punto de vista.
Podemos comenzar representando los ejes iniciales $t$ y $x$ en (lo que convencionalmente se considera) ángulos rectos y suprimiendo los ejes $y$ y $z$.
Luego, según (1.19), bajo un momentum en el plano $x$-$t$ el eje $x'$ ($t' = 0$) está dado por $t = x \tanh\phi$, mientras que el eje $t'$ ($x' = 0$) está dado por $t = x/\tanh\phi$.
Por lo tanto, vemos que los ejes espacio y tiempo giran entre sí, aunque se dividen entre sí en lugar de permanecer ortogonales en el sentido euclidiano tradicional.
(Como veremos, los ejes, de hecho, siguen siendo ortogonales en el sentido lorentziano.)
Esto no debería sorprendernos, ya que si el Espacio-Tiempo se comportara como una versión cuatridimensional del espacio, el mundo sería un lugar muy diferente.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/one3.pdf}
\end{figure}

También es esclarecedor considerar los caminos correspondientes a viajar a la velocidad $c=1$.
Estos están dados en el sistema de coordenadas original por $x=\pm t$.
En el nuevo sistema, una inmediata reflexión revela que las rutas definidas por $x' = \pm t'$ son precisamente las mismas que las definidas por $x=\pm t$; estas trayectorias quedan invariantes bajo las transformaciones de Lorentz.
Por supuesto que sabemos que la luz viaja a esta velocidad; Por tanto, hemos descubierto que la velocidad de la luz es la misma en cualquier sistema inercial.
Un conjunto de puntos que están conectados a un solo evento mediante líneas rectas que se mueven a la velocidad de la luz se llama {\bf cono de luz}; todo este conjunto es invariante bajo transformaciones de Lorentz.
Los conos de luz se dividen naturalmente en futuros y pasados; el conjunto de todos los puntos dentro de los conos de luz futuros y pasados de un punto $p$ se llaman {\bf separados en el tiempo} de $p$, mientras que los que están fuera de los conos de luz están {\bf separados en el espacio} y los que están sobre los conos están {\bf separados en la luz} o {\bf separados nulos} de $p$.
Volviendo a (1.3), vemos que el intervalo entre puntos separados en forma de tiempo es negativo, entre puntos separados en forma de espacio es positivo y entre puntos separados en forma nula es cero.
(El intervalo se define como $s^2$, no la raíz cuadrada de esta cantidad).
Note la distinción entre esta situación y la del mundo newtoniano; aquí, es imposible decir (de forma independiente de las coordenadas) si un punto que está separado espacialmente de $p$ está en el futuro de $p$, en el pasado de $p$ o ``al mismo tiempo''. '.

Para estudiar la estructura del espacio de Minkowski con más detalle, es necesario introducir los conceptos de vectores y tensores.
Comenzaremos con vectores, que deberían resultarle familiares.
Por supuesto, en el Espacio-Tiempo los vectores son de cuatro dimensiones y a menudo se les denomina {\bf cuatro vectores}.
Esto resulta marcar una gran diferencia; por ejemplo, no existe un producto cruzado entre dos cuatro vectores.

Más allá del simple hecho de la dimensionalidad, lo más importante a destacar es que cada vector se ubica en un punto determinado del Espacio-Tiempo.
Es posible que esté acostumbrado a pensar que los vectores se extienden de un punto a otro en el espacio, e incluso que los vectores ``libres'' pueden deslizarse descuidadamente de un punto a otro.
Estos no son conceptos útiles en relatividad.
Más bien, a cada punto $p$ del Espacio-Tiempo le asociamos el conjunto de todos los vectores posibles ubicados en ese punto; este conjunto se conoce como {\bf espacio tangente} en $p$ o $T_p$.
El nombre está inspirado en pensar que el conjunto de vectores unidos a un punto en un espacio bidimensional curvo simple comprende un plano que es tangente al punto.
Pero dejando a un lado la inspiración, es importante pensar que estos vectores están ubicados en un solo punto, en lugar de extenderse de un punto a otro.
(Aunque esto no nos impedirá dibujarlos como flechas en diagramas de Espacio-Tiempo).

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/one4.pdf}
\end{figure}

Más adelante relacionaremos el espacio tangente en cada punto con cosas que podemos construir a partir del propio espaciotiempo.
Por ahora, piense en $T_p$ como un espacio vectorial abstracto para cada punto en el Espacio-Tiempo.
Un {\bf espacio vectorial (real)} es una colección de objetos (``vectores'') que, en términos generales, se pueden sumar y multiplicar por números reales de forma lineal.
Por lo tanto, para dos vectores cualesquiera $V$ y $W$ y números reales $a$ y $b$, tenemos
\begin{equation}
(a+b)(V+W) = aV+bV+aW+bW\,.\label{1.22}
\end{equation}
Todo espacio vectorial tiene un origen, es decir, un vector cero que funciona como elemento de identidad en la suma de vectores.
En muchos espacios vectoriales hay operaciones adicionales, como tomar un producto interno (punto), pero esto es una estructura adicional por encima del concepto elemental de espacio vectorial.

Un vector es un objeto geométrico perfectamente bien definido, al igual que un {\bf campo vectorial}, definido como un conjunto de vectores con exactamente uno en cada punto del Espacio-Tiempo.
(El conjunto de todos los espacios tangentes de una variedad $M$ se llama {\bf paquete tangente}, $T(M)$.)
Sin embargo, suele ser útil para propósitos concretos descomponer los vectores en componentes con respecto a algún conjunto de vectores base.
Una {\bf base} es cualquier conjunto de vectores que abarca el espacio vectorial (cualquier vector es una combinación lineal de vectores base) y es linealmente independiente (ningún vector en la base es una combinación lineal de otros vectores base).
Para cualquier espacio vectorial dado, habrá un número infinito de bases legítimas, pero cada base estará formada por el mismo número de vectores, conocido como dimensión del espacio.
(Para un espacio tangente asociado con un punto en el espacio de Minkowski, la dimensión es, por supuesto, cuatro).

Imaginemos que en cada espacio tangente establecemos una base de cuatro vectores $\e\mu$, siendo $\mu\in\{0,1,2,3\}$ como de costumbre.
De hecho digamos que cada base se adapta a las coordenadas $x^\mu$; es decir, el vector base $\e1$ es lo que normalmente pensaríamos que apunta a lo largo del eje $x$, etc.
No es en absoluto necesario que elijamos una base que se adapte a cualquier sistema de coordenadas, aunque suele ser conveniente.
(Realmente podríamos ser más precisos aquí, pero más adelante repetiremos la discusión con un nivel de precisión insoportable, por lo que ahora es perdonable algo de descuido).
Entonces cualquier vector abstracto $A$ se puede escribir como una combinación lineal de vectores base:
\begin{equation}
A = A^\mu \e\mu\,.\label{1.23}
\end{equation}
Los coeficientes $A^\mu$ son los {\bf componentes} del vector $A$.
La mayoría de las veces olvidaremos la base por completo y nos referiremos un poco vagamente al ``vector $A^\mu$'', pero tenga en cuenta que esto es una abreviatura.
El vector real es una entidad geométrica abstracta, mientras que los componentes son sólo los coeficientes de los vectores base en alguna base conveniente.
% (
Dado que normalmente suprimiremos los vectores de base explícitos, los índices normalmente etiquetarán componentes de vectores y tensores.
Es por eso que hay paréntesis alrededor de los índices de los vectores base, para recordarnos que se trata de una colección de vectores, no de componentes de un solo vector.
% )

Un ejemplo estándar de vector en el Espacio-Tiempo es el vector tangente a una curva.
Una curva o ruta parametrizada a través del Espacio-Tiempo se especifica mediante las coordenadas en función del parámetro, {\it eg} $x^\mu(\lambda)$.
El vector tangente $V(\lambda)$ tiene componentes
\begin{equation}
V^\mu = {{dx^\mu}\over{d\lambda}}\,.\label{1.24}
\end{equation}
Por tanto, el vector completo es $V=V^\mu\e\mu$.
Bajo una transformación de Lorentz las coordenadas $x^\mu$ cambian según (1.11), mientras que la parametrización $\lambda$ permanece inalterada; por lo tanto podemos deducir que las componentes del vector tangente deben cambiar como
\begin{equation}
V^\mu \rightarrow V^{\mu'} = \Lambda^{\mu'}{}_\nu V^\nu\,.\label{1.25}
\end{equation}
Sin embargo, el vector en sí (a diferencia de sus componentes en algún sistema de coordenadas) es invariante bajo las transformaciones de Lorentz.
Podemos utilizar este hecho para derivar las propiedades de transformación de los vectores base.
Nos referiremos al conjunto de vectores base en el sistema de coordenadas transformado como $\e{\nu'}$.
Como el vector es invariante, tenemos
\begin{align}
V = V^\mu\e\mu &=  V^{\nu'}\e{\nu'}\nonumber \\
&=  \Lambda^{\nu'}{}_\mu V^\mu\e{\nu'}\,.\label{1.26}
\end{align}
Pero esta relación debe mantenerse sin importar cuáles sean los valores numéricos de los componentes $V^\mu$.
Por lo tanto podemos decir
\begin{equation}
\e\mu = \Lambda^{\nu'}{}_\mu\e{\nu'}\,.\label{1.27}
\end{equation}
Para obtener la nueva base $\e{\nu'}$ en términos de la antigua $\e\mu$ debemos multiplicar por la inversa de la transformación de Lorentz $\Lambda^{\nu'}{}_\mu$.
Pero la inversa de una transformación de Lorentz de las coordenadas no preparadas a las coordenadas preparadas es también una transformación de Lorentz, esta vez de los sistemas preparados a los no preparados.
Por lo tanto, introduciremos una notación algo sutil, escribiendo usando el mismo símbolo para ambas matrices, solo que con los índices primos y no primos ajustados.
Eso es,
\begin{equation}
(\Lambda^{-1})^{\nu'}{}_\mu = \Lambda_{\nu'}{}^\mu\ ,\label{1.28}
\end{equation}
o
\begin{equation}
\Lambda_{\nu'}{}^\mu \Lambda^{\sigma'}{}_\mu = \delta^{\sigma'}_{\nu'}
\ ,\quad \Lambda_{\nu'}{}^\mu \Lambda^{\nu'}{}_\rho =
\delta^{\mu}_{\rho}\ ,\label{1.29}
\end{equation}
donde $\delta^{\mu}_{\rho}$ es el símbolo tradicional delta de Kronecker en cuatro dimensiones.
(Tenga en cuenta que Schutz usa una convención diferente, siempre ordenando los dos índices noroeste/sureste; lo importante es dónde van los números primos).
De (1.27) obtenemos la regla de transformación para vectores base:
\begin{equation}
\e{\nu'} = \Lambda_{\nu'}{}^\mu\e\mu\,.\label{1.30}
\end{equation}
Por lo tanto, el conjunto de vectores base se transforma mediante la transformación inversa de Lorentz de las coordenadas o componentes del vector.

Vale la pena detenerse, hacer una pausa para asimilar todo esto.
Introdujimos coordenadas etiquetadas por índices superiores, que se transformaron de cierta manera bajo las transformaciones de Lorentz.
Luego consideramos los componentes vectoriales que también estaban escritos con índices superiores, lo cual tenía sentido ya que se transformaban de la misma manera que las funciones de coordenadas.
(En un sistema de coordenadas fijo, cada una de las cuatro coordenadas $x^\mu$ puede considerarse como una función en el Espacio-Tiempo, al igual que cada uno de los cuatro componentes de un campo vectorial).
Los vectores base asociados con el sistema de coordenadas se transformaron mediante la matriz inversa y se etiquetaron con un índice más bajo.
Esta notación aseguró que la transformación dejara sin cambios el objeto invariante construido sumando los componentes y los vectores base, tal como lo desearíamos.
Probablemente no sea demasiado revelador decir que este seguirá siendo el caso para objetos más complicados con múltiples índices (tensores).

Una vez que hemos configurado un espacio vectorial, hay un espacio vectorial asociado (de igual dimensión) que podemos definir inmediatamente, conocido como {\bf espacio vectorial dual}.
El espacio dual generalmente se indica con un asterisco, de modo que el espacio dual al espacio tangente $T_p$ se denomina {\bf espacio cotangente} y se denota $T^*_p$.
El espacio dual es el espacio de todas las aplicaciones lineales desde el espacio vectorial original hasta los números reales; en la jerga matemática, si $\omega\in T_p^*$ es un vector dual, entonces actúa como un mapa tal que:
\begin{equation}
\omega(aV+bW) = a\omega(V) + b\omega(W) \in \mathbb{R}\; ,\label{1.31}
\end{equation}
donde $V$, $W$ son vectores y $a$, $b$ son números reales.
Lo bueno de estos mapas es que ellos mismos forman un espacio vectorial; por lo tanto, si $\omega$ y $\eta$ son vectores duales, tenemos
\begin{equation}
(a\omega+b\eta)(V) = a\omega(V) + b\eta(V)\,.\label{1.32}
\end{equation}
Para hacer esta construcción algo más concreta, podemos introducir un conjunto de vectores duales de base $\ztheta{\nu}$ exigiendo
\begin{equation}
\ztheta{\nu}(\e{\mu}) = \delta^{\nu}_{\mu}\, . \label{1.33}
\end{equation}
Entonces cada vector dual se puede escribir en términos de sus componentes, que etiquetamos con índices más bajos:
\begin{equation}
\omega = \omega_\mu \ztheta{\mu}\,.\label{1.34}
\end{equation}
En perfecta analogía con los vectores, normalmente escribiremos simplemente $\omega_\mu$ para representar el vector dual completo.
De hecho, en algún instante verá elementos de $T_p$ (lo que hemos llamado vectores) denominados {\bf vectores contravariantes}, y elementos de $T_p^*$ (lo que hemos llamado vectores duales) denominados {\bf covariante vectores}.
En realidad, si simplemente nos referimos a los vectores ordinarios como vectores con índices superiores y a los vectores duales como vectores con índices inferiores, nadie debería ofenderse.
Otro nombre para los vectores duales es {\bf one-forms}, una designación algo misteriosa que pronto quedará más clara.

La notación de componentes conduce a una forma sencilla de escribir la acción de un vector dual sobre un vector:
\begin{align}
\omega(V) &=  \omega_\mu V^\nu \ztheta{\mu}(\e\nu)\nonumber \\
&=  \omega_\mu V^\nu \delta^\mu_\nu\nonumber \\
&=  \omega_\mu V^\mu \in \mathbb{R} \,. \label{1.35}
\end{align}
Esta es la razón por la que rara vez es necesario escribir explícitamente los vectores base (y los vectores duales); los componentes hacen todo el trabajo.
La forma de (1.35) también sugiere que podemos pensar en los vectores como aplicaciones lineales en vectores duales, definiendo
\begin{equation}
V(\omega) \equiv \omega(V) = \omega_\mu V^\mu\,.\label{1.36}
\end{equation}
Por lo tanto, el espacio dual del espacio vectorial dual es el propio espacio vectorial original.

Por supuesto, en el Espacio-Tiempo no nos interesará un espacio vectorial único, sino campos de vectores y vectores duales.
(El conjunto de todos los espacios cotangentes sobre $M$ es el {\bf paquete cotangente}, $T^*(M)$.)
En ese caso, la acción de un campo vectorial dual sobre un campo vectorial no es un número único, sino un {\bf escalar} (o simplemente una ``función'') en el Espacio-Tiempo.
Un escalar es una cantidad sin índices, que no cambia bajo las transformaciones de Lorentz.

Podemos usar los mismos argumentos que usamos anteriormente para los vectores para derivar las propiedades de transformación de vectores duales.
Las respuestas son, para los componentes,
\begin{equation}
\omega_{\mu'} = \Lambda_{\mu'}{}^\nu\omega_\nu\ ,\label{1.37}
\end{equation}
y para vectores duales de base,
\begin{equation}
 \ztheta{\rho'} = \Lambda^{\rho'}{}_\sigma \ztheta{\sigma}\,.\label{1.38}
\end{equation}
Esto es justo lo que esperaríamos de la colocación de índices; los componentes de un vector dual se transforman bajo la transformación inversa de los de un vector.
Tenga en cuenta que esto garantiza que el escalar (1.35) sea invariante bajo transformaciones de Lorentz, tal como debería ser.

Consideremos algunos ejemplos de vectores duales, primero en otros contextos y luego en el espacio de Minkowski.
Imagine el espacio de los vectores de columna de componentes $n$, para algún número entero $n$.
Entonces el espacio dual es el de los vectores fila de componentes $n$ y la acción es la multiplicación de matrices ordinaria:
\begin{align}
V &= \left(\mqty{V^1  \\  V^2  \\  \cdot \\  \cdot  \\  \cdot  \\
V^{n} \\ }\right)\ ,\quad
\omega = \left(\omega_1\ \omega_2\ \cdots\ \omega_{n}\right)\ ,\nonumber \\
\omega(V) &= \left(\omega_1\ \omega_2\ \cdots\ \omega_{n}\right)
\left(\mqty{V^1  \\  V^2  \\  \cdot \\  \cdot  \\  \cdot \\
V^{n} \\ }\right) = \omega_i V^i\,. \label{1.39}
\end{align}
Otro ejemplo familiar ocurre en la mecánica cuántica, donde los vectores en el espacio de Hilbert están representados por kets, $|\psi\rangle$.
En este caso el espacio dual es el espacio de sujetadores, $\langle\phi |$, y la acción da el número $\langle \phi |\psi\rangle$.
(Este es un número complejo en mecánica cuántica, pero la idea es exactamente la misma).

En el Espacio-Tiempo, el ejemplo más simple de un vector dual es el {\bf gradiente} de una función escalar, el conjunto de derivadas parciales con respecto a las coordenadas del Espacio-Tiempo, que denotamos con ``d'':
\begin{equation}
{\rm d}\phi = {{\p{}\phi}\over{\p{}x^\mu}}  \ztheta{\mu}\,.\label{1.40}
\end{equation}
La regla de la cadena convencional utilizada para transformar derivadas parciales equivale en este caso a la regla de transformación de componentes de vectores duales:
\begin{align}
{{\p{}\phi}\over{\p{}x^{\mu'}}} &=
{{\p{}x^\mu}\over{\p{}x^{\mu'}}}{{\p{}\phi}\over{\p{}x^\mu}}\nonumber \\
&= \Lambda_{\mu'}{}^\mu {{\p{}\phi}\over{\p{}x^\mu}}\ ,
\label{1.41}
\end{align}
donde hemos utilizado (1.11) y (1.28) para relacionar la transformación de Lorentz con las coordenadas.
El hecho de que el gradiente sea un vector dual lleva a las siguientes notaciones abreviadas para derivadas parciales:
\begin{equation}
{{\p{}\phi}\over{\p{}x^\mu}}=\p\mu\phi = \phi,{}_\mu
\,.\label{1.42}
\end{equation}
(En términos muy generales, ``$x^\mu$ tiene un índice superior, pero cuando está en el denominador de una derivada implica un índice inferior en el objeto resultante''). No soy un gran admirador de la notación de coma, pero usaremos $\p\mu$ todo el tiempo.
Tenga en cuenta que, de hecho, el gradiente actúa de forma natural en el ejemplo que dimos anteriormente de un vector, el vector tangente a una curva.
El resultado es la derivada ordinaria de la función a lo largo de la curva:
\begin{equation}
\p\mu\phi {{\partial x^\mu}\over{\partial \lambda}}
= {{d\phi}\over{d\lambda}}\,.\label{1.43}
\end{equation}

Como nota final sobre los vectores duales, existe una manera de representarlos como imágenes que es consistente con la imagen de los vectores como flechas.
Véase la discusión en Schutz o en MTW (donde se lleva a extremos vertiginosos).

Una generalización sencilla de vectores y vectores duales es la noción de {\bf tensor}.
Así como un vector dual es un mapa lineal de vectores a $\mathbb{R}$, un tensor $T$ de tipo (o rango) $(k,l)$ es un mapa multilineal de una colección de vectores duales y vectores a $\mathbb{R}$:
\begin{align}
T:~ &T^*_p\times\cdots\times T^*_p\times T_p\times\cdots
\times T_p \rightarrow \mathbb{R}\nonumber \\
& (k~{\rm times})\qquad\quad (l~{\rm times}) \label{1.44}
\end{align}
Aquí, ``$\times$'' denota el producto cartesiano, de modo que, por ejemplo, $T_p\times T_p$ es el espacio de pares ordenados de vectores.
Multilinealidad significa que el tensor actúa linealmente en cada uno de sus argumentos; por ejemplo, para un tensor de tipo $(1,1)$, tenemos
\begin{equation}
T(a\omega+b\eta, cV+dW) = acT(\omega,V)+adT(\omega,W)+bcT(\eta,V)
+bdT(\eta,W)\,.\label{1.45}
\end{equation}
Desde este punto de vista, un escalar es un tensor de tipo $(0,0)$, un vector es un tensor de tipo $(1,0)$ y un vector dual es un tensor de tipo $(0,1)$.

El espacio de todos los tensores de tipo fijo $(k,l)$ forma un espacio vectorial; se pueden sumar y multiplicar por números reales.
Para construir una base para este espacio, necesitamos definir una nueva operación conocida como {\bf producto tensorial}, denotada por $\otimes$.
Si $T$ es un tensor $(k,l)$ y $S$ es un tensor $(m,n)$, definimos un tensor $(k+m,l+n)$ $T\otimes S$ por
\begin{align}
\lefteqn{T\otimes S(\omega^{(1)},\ldots ,\omega^{(k)},\ldots
,\omega^{(k+m)},V^{(1)},\ldots ,V^{(l)},\ldots ,V^{(l+n)})} \nonumber \\
& =
T(\omega^{(1)},\ldots ,\omega^{(k)},V^{(1)},\ldots ,V^{(l)})
S(\omega^{(k+1)},\ldots ,\omega^{(k+m)},V^{(l+1)},\ldots ,V^{(l+n)})
\,.\label{1.46}
\end{align}
(Tenga en cuenta que $\omega^{(i)}$ y $V^{(i)}$ son vectores y vectores duales distintos, no componentes de los mismos).
En otras palabras, primero actúe $T$ sobre el conjunto apropiado de vectores duales y vectores, luego actúe $S$ sobre el resto y luego multiplique las respuestas.
Tenga en cuenta que, en general, $T\otimes S \neq S\otimes T$.

Ahora es sencillo construir una base para el espacio de todos los tensores $(k,l)$ tomando productos tensoriales de vectores base y vectores duales; esta base estará formada por todos los tensores de la forma
\begin{equation}
\e{\mu_1}\otimes\cdots\otimes\e{\mu_k}\otimes
 \ztheta{\nu_1}\otimes\cdots\otimes \ztheta{\nu_l}\,.\label{1.47}
\end{equation}
En un Espacio-Tiempo de 4 dimensiones habrá $4^{k+l}$ tensores de base en total.
En notación de componentes escribimos nuestro tensor arbitrario como
\begin{equation}
T = T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l}
\e{\mu_1}\otimes\cdots\otimes\e{\mu_k}\otimes
 \ztheta{\nu_1}\otimes\cdots\otimes \ztheta{\nu_l}\,.\label{1.48}
\end{equation}
Alternativamente, podríamos definir los componentes actuando el tensor sobre vectores base y vectores duales:
\begin{equation}
T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l} =
T( \ztheta{\mu_1},\ldots,  \ztheta{\mu_k},\e{\nu_1},\ldots,\e{\nu_l})
\,.\label{1.49}
\end{equation}
Puedes comprobar por ti mismo, usando (1.33), etc., que todas estas ecuaciones encajan correctamente.

Al igual que con los vectores, normalmente tomaremos el atajo de denotar el tensor $T$ por sus componentes $T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l}$.
La acción de los tensores sobre un conjunto de vectores y vectores duales sigue el patrón establecido en (1.35):
\begin{equation}
T(\omega^{(1)},\ldots ,\omega^{(k)},V^{(1)},\ldots ,V^{(l)}) =
T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l}
\omega^{(1)}_{\mu_1}\cdots\omega^{(k)}_{\mu_k}V^{(1)\nu_1}\cdots
V^{(l)\nu_l}\,.\label{1.50}
\end{equation}
El orden de los índices es obviamente importante, ya que no es necesario que el tensor actúe de la misma manera en sus diversos argumentos.
Finalmente, la transformación de componentes tensoriales bajo las transformaciones de Lorentz se puede derivar aplicando lo que ya sabemos sobre la transformación de vectores base y vectores duales.
La respuesta es exactamente lo que se esperaría de la colocación del índice,
\begin{equation}
T^{\mu_1' \cdots \mu_k'}{}_{\nu_1'\cdots\nu_l'} =
\Lambda^{\mu_1'}{}_{\mu_1}\cdots\Lambda^{\mu_k'}{}_{\mu_k}
\Lambda_{\nu_1'}{}^{\nu_1}\cdots\Lambda_{\nu_l'}{}^{\nu_l}
T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l} \,.\label{1.51}
\end{equation}
Por lo tanto, cada índice superior se transforma como un vector y cada índice inferior se transforma como un vector dual.

Aunque hemos definido los tensores como aplicaciones lineales de conjuntos de vectores y vectores tangentes a $\mathbb{R}$, no hay nada que nos obligue a actuar sobre una colección completa de argumentos.
Por tanto, un tensor $(1,1)$ también actúa como un mapa de vectores a vectores:
\begin{equation}
T^\mu{}_\nu :~ V^\nu\rightarrow T^\mu{}_\nu V^\nu\,.\label{1.52}
\end{equation}
Puedes comprobar por ti mismo que $T^\mu{}_\nu V^\nu$ es un vector ({\it ie} obedece la ley de transformación vectorial).
De manera similar, podemos actuar un tensor sobre (todo o parte de) otro tensor para obtener un tercer tensor.
Por ejemplo,
\begin{equation}
U^\mu{}_\nu = T^{\mu\rho}{}_\sigma S^{\sigma}{}_{\rho\nu}\label{1.53}
\end{equation}
es un tensor $(1,1)$ perfectamente bueno.

Quizás le preocupe que esta introducción a los tensores haya sido demasiado breve, dada la naturaleza esotérica del material.
De hecho, dominar la noción de tensores no requiere mucho esfuerzo; es sólo una cuestión de mantener los índices en orden y las reglas para manipularlos son muy naturales.
De hecho, a varios libros les gusta \textit{definir} los tensores como colecciones de números que se transforman según (1.51).
Si bien esto es operativamente útil, tiende a oscurecer el significado más profundo de los tensores como entidades geométricas con una vida independiente de cualquier sistema de coordenadas elegido.
Sin embargo, hay una sutileza que hemos pasado por alto.
Las nociones de vectores duales, tensores, bases y aplicaciones lineales pertenecen al ámbito del álgebra lineal y son apropiadas siempre que tengamos a mano un espacio vectorial abstracto.
En el caso que nos interesa, no tenemos sólo un espacio vectorial, sino un espacio vectorial en cada punto del Espacio-Tiempo.
La mayoría de las veces estamos interesados en campos tensoriales, que pueden considerarse funciones tensoriales en el Espacio-Tiempo.
Afortunadamente, a ninguna de las manipulaciones que definimos anteriormente realmente le importa si estamos tratando con un único espacio vectorial o una colección de espacios vectoriales, uno para cada evento.
Podremos salirnos con la nuestra simplemente llamando funciones de $x^\mu$ cuando sea apropiado.
Sin embargo, conviene mantener clara la independencia lógica de las nociones que hemos introducido y su aplicación específica al espaciotiempo y la relatividad.

Ahora veamos algunos ejemplos de tensores.
Primero consideramos el ejemplo anterior de vectores columna y sus duales, vectores fila.
En este sistema, un tensor $(1,1)$ es simplemente una matriz, $M^i{}_j$.
Su acción sobre un par $(\omega,V)$ viene dada por la multiplicación de matrices habitual:
\begin{equation}
M(\omega,V) = \left(\omega_1\ \omega_2\ \cdots\ \omega_{n}\right)
\left(\mqty{M^1{}_1 & M^1{}_2 & \cdots & M^1{}_n  \\
M^2{}_1 & M^2{}_2 & \cdots & M^2{}_n  \\
\cdot &\cdot & \cdots &\cdot  \\  \cdot &\cdot & \cdots &\cdot  \\
\cdot &\cdot & \cdots &\cdot  \\
M^n{}_1 & M^n{}_2 & \cdots & M^n{}_n  \\ }\right)
\left(\mqty{V^1  \\  V^2  \\  \cdot \\  \cdot  \\  \cdot \\
V^{n} \\ }\right) = \omega_i M^i{}_j V^j\,.\label{1.54}
\end{equation}
Si lo desea, siéntase libre de pensar en los tensores como ``matrices con un número arbitrario de índices''.

En el Espacio-Tiempo ya hemos visto algunos ejemplos de tensores sin llamarlos así.
El ejemplo más familiar de un tensor $(0,2)$ es la métrica $\eta_\mn$.
La acción de la métrica sobre dos vectores es tan útil que recibe su propio nombre, {\bf producto interno} (o producto escalar):
\begin{equation}
\eta(V,W) = \eta_\mn V^\mu W^\nu = V\cdot W\,.\label{1.55}
\end{equation}
Al igual que con el producto escalar euclidiano convencional, nos referiremos a dos vectores cuyo producto escalar desaparece como {\bf ortogonal}.
Dado que el producto escalar es un escalar, se deja invariante bajo las transformaciones de Lorentz; por lo tanto, los vectores base de cualquier sistema inercial cartesiano, que se eligen como ortogonales por definición, siguen siendo ortogonales después de una transformación de Lorentz (a pesar del ``tijeraje'' que notamos anteriormente).
La {\bf norma} de un vector se define como el producto interno del vector consigo mismo; a diferencia del espacio euclidiano, este número no es definido positivo:
\begin{equation*}
{\rm ~if~~}\eta_\mn V^\mu V^\nu
{\rm ~~is~}\left\{\mqty{<0\ ,\ V^\mu {\rm ~is~timelike}\hfill \\
=0\ ,\ V^\mu {\rm ~is~lightlike~or~null}\hfill \\
>0\ ,\ V^\mu {\rm ~is~spacelike}\,.\hfill \\ }\right.
\end{equation*}
(Un vector puede tener norma cero sin ser el vector cero).
Notarás que la terminología es la misma que usamos anteriormente para clasificar la relación entre dos puntos en el Espacio-Tiempo; Por supuesto, no es casualidad y entraremos en más detalles más adelante.

Otro tensor es el delta de Kronecker $\delta^\mu_\nu$, de tipo $(1,1)$, del que ya conoces los componentes.
Relacionado con esto y la métrica está la {\bf métrica inversa} $\eta^\mn$, un tensor de tipo $(2,0)$ definido como el inverso de la métrica:
\begin{equation}
\eta^\mn\eta_{\nu\rho} = \eta_{\rho\nu}\eta^{\nu\mu}
= \delta^\rho_\mu\,.\label{1.56}
\end{equation}
De hecho, como puedes comprobar, la métrica inversa tiene exactamente los mismos componentes que la métrica misma.
(Esto sólo es cierto en un espacio plano en coordenadas cartesianas y no se cumplirá en situaciones más generales).
También está el {\bf tensor de Levi-Civita}, un tensor $(0,4)$:
\begin{equation}
\epsilon_{\mu\,\nu\,\rho\,\sigma} =
\left\{\hspace{2mm}
\begin{aligned}
+1 \quad &\text{si $\,\mu\,\nu\,\rho\,\sigma\,$ es una permutación par de 0123} \\
-1  \quad &\text{si $\,\mu\,\nu\,\rho\,\sigma\,$ es una permutación impar de 0123} \\
0  \quad &\text{otros casos.}
\end{aligned} \right.
\label{1.57}
\end{equation}
Aquí, una ``permutación de 0123'' es un orden de los números 0, 1, 2, 3 que se puede obtener comenzando con 0123 e intercambiando dos de los dígitos; una permutación par se obtiene mediante un número par de tales intercambios, y una permutación impar se obtiene mediante un número impar.
Así, por ejemplo, $\epsilon_{0321}=-1$.

Es una propiedad notable de los tensores anteriores (la métrica, la métrica inversa, el delta de Kronecker y el tensor de Levi-Civita) que, aunque todos se transforman de acuerdo con la ley de transformación del tensor (1.51), sus componentes permanecen sin cambios en \textit{cualquier} sistema de coordenadas cartesiano en el Espacio-Tiempo plano.
En cierto sentido, esto los convierte en malos ejemplos de tensores, ya que la mayoría de los tensores no tienen esta propiedad.
De hecho, incluso estos tensores no tienen esta propiedad una vez que pasamos a sistemas de coordenadas más generales, con la única excepción del delta de Kronecker.
Este tensor tiene exactamente los mismos componentes en cualquier sistema de coordenadas en cualquier Espacio-Tiempo.
Esto tiene sentido a partir de la definición de tensor como aplicación lineal; El tensor de Kronecker puede considerarse como el mapa de identidad de vectores a vectores (o de vectores duales a vectores duales), que claramente debe tener los mismos componentes independientemente del sistema de coordenadas.
Los otros tensores (la métrica, su inversa y el tensor de Levi-Civita) caracterizan la estructura del Espacio-Tiempo y todos dependen de la métrica.
Por lo tanto, tendremos que tratarlos con más cuidado cuando abandonemos nuestra suposición de un Espacio-Tiempo plano.

Un ejemplo más típico de tensor es el {\bf tensor de intensidad de campo electromagnético}.
Todos sabemos que los campos electromagnéticos están formados por el vector de campo eléctrico $E_i$ y el vector de campo magnético $B_i$.
(Recuerde que usamos índices latinos para los componentes espaciales 1,2,3.)
En realidad, estos son sólo ``vectores'' bajo rotaciones en el espacio, no bajo el grupo de Lorentz completo.
De hecho son componentes de un tensor $(0,2)$ $F_{\mu\nu}$, definido por
\begin{equation}
F_{\mu\nu} = \left(\mqty{0&-E_1 &-E_2 & -E_3 \\  E_1 & 0 & B_3 & -B_2 \\
E_2 & -B_3 & 0 & B_1 \\  E_3 & B_2 & -B_1 & 0  \\ }\right)
=-F_{\nu\mu}\,.\label{1.58}
\end{equation}
Desde este punto de vista, es fácil transformar los campos electromagnéticos de un sistema de referencia en los de otro, aplicando (1.51).
El poder unificador del formalismo tensorial es evidente: en lugar de una colección de dos vectores cuya relación y propiedades de transformación son bastante misteriosas, tenemos un único campo tensorial para describir todo el electromagnetismo.
(Por otro lado, no te dejes llevar; a veces es más conveniente trabajar en un único sistema de coordenadas utilizando los vectores de campo eléctrico y magnético).

Con algunos ejemplos en la mano ahora podemos ser un poco más sistemáticos acerca de algunas propiedades de los tensores.
Primero considere la operación de {\bf contracción}, que convierte un tensor $(k,l)$ en un tensor $(k-1,l-1)$.
La contracción se produce sumando un índice superior y otro inferior:
\begin{equation}
S^{\mu\rho}{}_\sigma = T^{\mu\nu\rho}{}_{\sigma\nu}\,.\label{1.59}
\end{equation}
Puedes comprobar que el resultado es un tensor bien definido.
Por supuesto, sólo está permitido contratar un índice superior con un índice inferior (a diferencia de dos índices del mismo tipo).
Tenga en cuenta también que el orden de los índices es importante, por lo que puede obtener diferentes tensores contrayéndose de diferentes maneras; de este modo,
\begin{equation}
T^{\mu\nu\rho}{}_{\sigma\nu}\neq T^{\mu\rho\nu}{}_{\sigma\nu}
\label{1.60}
\end{equation}
en general.

La métrica y la métrica inversa se pueden utilizar para {\bf subir y bajar índices} en tensores.
Es decir, dado un tensor $T^{\alpha\beta}{}_{\gamma\delta}$, podemos usar la métrica para definir nuevos tensores que elegimos denotar con la misma letra $T$:
\begin{align}
T^{\alpha\beta\mu}{}_\delta &= \eta^{\mu\gamma}
T^{\alpha\beta}{}_{\gamma\delta}\ , \nonumber \\
T_\mu{}^\beta{}_{\gamma\delta} &= \eta_{\mu\alpha}
T^{\alpha\beta}{}_{\gamma\delta}\ , \nonumber \\
T_{\mu\nu}{}^{\rho\sigma} &= \eta_{\mu\alpha}\eta_{\nu\beta}
\eta^{\rho\gamma}\eta^{\sigma\delta}
T^{\alpha\beta}{}_{\gamma\delta}\ , \label{1.61}
\end{align}
Etcétera.
Observe que subir y bajar no cambia la posición de un índice en relación con otros índices, y también que los índices ``libres'' (que no se suman) deben ser los mismos en ambos lados de una ecuación, mientras que los índices ``ficticios''. Los índices (que se suman) solo aparecen en un lado.
Como ejemplo, podemos convertir vectores y vectores duales entre sí subiendo y bajando índices:
\begin{align}
V_\mu &= \eta_\mn V^\nu\nonumber \\
\omega^\mu &= \eta^\mn \omega_\nu\,. \label{1.62}
\end{align}
Esto explica por qué el gradiente en el espacio euclidiano plano tridimensional suele considerarse como un vector ordinario, aunque hemos visto que surge como un vector dual; en el espacio euclidiano (donde la métrica es diagonal con todas las entradas $+1$) un vector dual se convierte en un vector con exactamente los mismos componentes cuando aumentamos su índice.
Quizás entonces se pregunte por qué hemos insistido tanto en la distinción.
Una razón sencilla, por supuesto, es que en un Espacio-Tiempo lorentziano los componentes no son iguales:
\begin{equation}
\omega^\mu = (-\omega_0, \omega_1, \omega_2, \omega_3)\,.\label{1.63}
\end{equation}
En un Espacio-Tiempo curvo, donde la forma de la métrica es generalmente más complicada, la diferencia es bastante más dramática.
Pero hay una razón más profunda, a saber, que los tensores generalmente tienen una definición ``natural'' que es independiente de la métrica.
Aunque siempre tendremos una métrica disponible, es útil conocer el estado lógico de cada objeto matemático que introducimos.
El gradiente, y su acción sobre los vectores, está perfectamente definido independientemente de cualquier métrica, mientras que el ``gradiente con índices superiores'' no lo está.
(Como ejemplo, eventualmente querremos tomar variaciones de funcionales con respecto a la métrica y, por lo tanto, tendremos que saber exactamente cómo depende el funcional de la métrica, algo que la notación de índice oscurece fácilmente).

Continuando con nuestra recopilación de jerga tensorial, nos referimos a un tensor como {\bf simétrico} en cualquiera de sus índices si no cambia bajo el intercambio de esos índices.
Así, si
\begin{equation}
S_{\mu\nu\rho} = S_{\nu\mu\rho}\ ,\label{1.64}
\end{equation}
decimos que $S_{\mu\nu\rho}$ es simétrico en sus dos primeros índices, mientras que si
\begin{equation}
S_{\mu\nu\rho} = S_{\mu\rho\nu} = S_{\rho\mu\nu} = S_{\nu\mu\rho}
= S_{\nu\rho\mu} = S_{\rho\nu\mu} \ ,\label{1.65}
\end{equation}
decimos que $S_{\mu\nu\rho}$ es simétrico en sus tres índices.
De manera similar, un tensor es {\bf antisimétrico} (o ``sesgado-simétrico'') en cualquiera de sus índices si cambia de signo cuando esos índices se intercambian; de este modo,
\begin{equation}
A_{\mu\nu\rho} = -A_{\rho\nu\mu}\label{1.66}
\end{equation}
significa que $A_{\mu\nu\rho}$ es antisimétrico en su primer y tercer índice (o simplemente ``antisimétrico en $\mu$ y $\rho$'').
Si un tensor es (anti) simétrico en todos sus índices, nos referimos a él como simplemente (anti) simétrico (a veces con el modificador redundante ``completamente'').
Como ejemplos, la métrica $\eta_{\mn}$ y la métrica inversa $\eta^{\mu\nu}$ son simétricas, mientras que el tensor de Levi-Civita $\epsilon_{\mu\nu\rho\sigma}$ y el tensor de intensidad del campo electromagnético $F_{\mu\nu}$ son antisimétricos.
(Compruebe usted mismo que si sube o baja un conjunto de índices que son simétricos o antisimétricos, permanecen así).
Tenga en cuenta que no tiene sentido intercambiar índices superior e inferior entre sí, así que no sucumba a la tentación de pensar que el delta de Kronecker $\delta^\alpha_\beta$ es simétrico.
Por otro lado, el hecho de que reducir un índice en $\delta^\alpha_\beta$ dé un tensor simétrico (de hecho, la métrica) significa que el orden de los índices realmente no importa, razón por la cual no realizamos un seguimiento de la ubicación de los índices para este tensor.

Dado cualquier tensor, podemos {\bf simetrizar} (o antisimetrizar) cualquier número de sus índices superiores o inferiores.
Para simetrizar, tomamos la suma de todas las permutaciones de los índices relevantes y la dividimos por el número de términos:
\begin{equation}
T_{(\mu_1\mu_2\cdots\mu_n)\rho}{}^\sigma = {1\over {n!}}
\left(T_{\mu_1\mu_2\cdots\mu_n\rho}{}^\sigma +
{\rm sum~over~permutations~of~indices~}\mu_1\cdots\mu_n \right)
\ ,\label{1.67}
\end{equation}
mientras que la antisimetrización proviene de la suma alterna:
\begin{equation}
T_{[\mu_1\mu_2\cdots\mu_n]\rho}{}^\sigma = {1\over {n!}}
\left(T_{\mu_1\mu_2\cdots\mu_n\rho}{}^\sigma +
{\rm alternating~sum~over~permutations~of~indices~ }
\mu_1\cdots\mu_n \right)\,.\label{1.68}
\end{equation}
Por ``suma alterna'' queremos decir que las permutaciones que son el resultado de un número impar de intercambios reciben un signo menos, así:
\begin{equation}
T_{[\mu\nu\rho]\sigma} = {1\over 6}\left(T_{\mu\nu\rho\sigma}
- T _{\mu\rho\nu\sigma} + T_{\rho\mu\nu\sigma} - T_{\nu\mu\rho\sigma}
+ T_{\nu\rho\mu\sigma} - T_{\rho\nu\mu\sigma}
\right)\,.\label{1.69}
\end{equation}
Observe que los corchetes/corchetes denotan simetrización/antisimetrización.
Además, es posible que a veces queramos (anti)simetrizar índices que no están uno al lado del otro, en cuyo caso usamos barras verticales para indicar índices no incluidos en la suma:
\begin{equation}
T_{(\mu |\nu |\rho)} = {1\over 2}\left(T_{\mu\nu\rho}
+ T_{\rho\nu\mu}\right)\,.\label{1.70}
\end{equation}
Finalmente, algunas personas usan una convención en la que se omite el factor $1/n!$.
El utilizado aquí es bueno, ya que (por ejemplo) un tensor simétrico satisface
\begin{equation}
S_{\mu_1 \cdots \mu_n} = S_{(\mu_1 \cdots \mu_n)} \ ,\label{1.71}
\end{equation}
y lo mismo para tensores antisimétricos.

Hasta ahora hemos tenido mucho cuidado en distinguir claramente entre cosas que siempre son verdaderas (en una variedad con métrica arbitraria) y cosas que solo son verdaderas en el espacio de Minkowski en coordenadas cartesianas.
Una de las distinciones más importantes surge con {\bf derivadas parciales}.
Si estamos trabajando en un Espacio-Tiempo plano con coordenadas cartesianas, entonces la derivada parcial de un tensor $(k,l)$ es un tensor $(k,l+1)$; eso es,
\begin{equation}
T_\alpha{}^\mu{}_\nu = \partial_\alpha R^\mu{}_\nu \label{1.72}
\end{equation}
se transforma adecuadamente bajo transformaciones de Lorentz.
Sin embargo, esto ya no será cierto en espacios-tiempos más generales, y tendremos que definir una ``derivada covariante'' para reemplazar a la derivada parcial.
Sin embargo, todavía podemos utilizar el hecho de que las derivadas parciales nos dan tensor en este caso especial, siempre y cuando mantengamos nuestro ingenio.
(La única excepción a esta advertencia es la derivada parcial de un escalar, $\partial_\alpha\phi$, que es un tensor [el gradiente] perfectamente bueno en cualquier Espacio-Tiempo).

Ahora hemos acumulado suficiente conocimiento sobre tensores para ilustrar algunos de estos conceptos utilizando la física real.
Específicamente, examinaremos las {\bf ecuaciones de Maxwell} de la electrodinámica.
En notación del siglo $19^{\rm th}$, estos son
\begin{align}
\nabla\times{\bf B} - \p{t}{\bf E} &= 4\pi{\bf J}\nonumber \\
\nabla\cdot{\bf E} &= 4\pi\rho\nonumber \\
\nabla\times{\bf E} + \p{t}{\bf B} &= 0\nonumber \\
\nabla\cdot{\bf B} &= 0\,. \label{1.73}
\end{align}
Aquí, {\bf E} y {\bf B} son los 3 vectores del campo eléctrico y magnético, {\bf J} es la corriente, $\rho$ es la densidad de carga y $\nabla\times$ y $\nabla\cdot$ son los rizo y divergencia convencionales.
Estas ecuaciones son invariantes bajo transformaciones de Lorentz, por supuesto; Así empezó todo el negocio.
Pero no parecen obviamente invariantes; nuestra notación tensorial puede solucionarlo.
Comencemos escribiendo estas ecuaciones en una notación ligeramente diferente,
\begin{align}
\epsilon^{ijk}\p{j}B_k - \p0 E^i &=
4\pi J^i\nonumber \\
\p{i}E^i &= 4\pi J^0\nonumber \\
\epsilon^{ijk}\p{j}E_k + \p0 B^i &= 0\nonumber \\
\p{i}B^i &= 0\,. \label{1.74}
\end{align}
En estas expresiones, los índices espaciales se han subido y bajado con abandono, sin ningún intento de mantenerse recto donde aparece la métrica.
Esto se debe a que $\delta_{ij}$ es la métrica en 3 espacios planos, con $\delta^{ij}$ su inversa (son iguales como matrices).
Por lo tanto, podemos subir y bajar los índices a voluntad, ya que los componentes no cambian.
Mientras tanto, el tensor tridimensional de Levi-Civita $\epsilon^{ijk}$ se define igual que el tetradimensional, aunque con un índice menos.
Hemos reemplazado la densidad de carga por $J^0$; esto es legítimo porque la densidad y la corriente juntas forman el {\bf 4 vectores actuales}, $J^\mu = (\rho,J^1,J^2,J^3)$.

A partir de estas expresiones y de la definición (1.58) del tensor de intensidad de campo $F_{\mu\nu}$, es fácil obtener una versión completamente tensorial del siglo $20^{\rm th}$ de las ecuaciones de Maxwell.
Comience observando que podemos expresar la intensidad del campo con índices superiores como
\begin{align}
F^{0i} &= E^i\nonumber \\ F^{ij} &= \epsilon^{ijk}B_k\,. \label{1.75}
\end{align}
(Para comprobar esto, tenga en cuenta, por ejemplo, que $F^{01} = \eta^{00}\eta^{11} F_{01}$ y $F^{12} = \epsilon^{123}B_3$.)
Entonces las dos primeras ecuaciones de (1.74) se convierten en
\begin{align}
\p{j}F^{ij} - \p0 F^{0i} &= 4\pi J^i\nonumber \\
\p{i}F^{0i} &= 4\pi J^0\,. \label{1.76}
\end{align}
Usando la antisimetría de $F^{\mn}$, vemos que estos se pueden combinar en la ecuación tensorial única
\begin{equation}
\p\mu F^{\nu\mu} = 4\pi J^\nu\,.\label{1.77}
\end{equation}
Una línea de razonamiento similar, que se le deja como ejercicio, revela que la tercera y cuarta ecuaciones en (1.74) se pueden escribir
\begin{equation}
\p{[\mu} F_{\nu\lambda]}= 0\,.\label{1.78}
\end{equation}
Las cuatro ecuaciones tradicionales de Maxwell se sustituyen así por dos, lo que demuestra la economía de la notación tensorial.
Sin embargo, lo más importante es que ambos lados de las ecuaciones (1.77) y (1.78) se transforman manifiestamente en tensores; por lo tanto, si son verdaderas en un marco inercial, deben serlo en cualquier marco transformado de Lorentz.
Esta es la razón por la que los tensores son tan útiles en relatividad: a menudo queremos expresar relaciones sin recurrir a ningún marco de referencia, y es necesario que las cantidades a cada lado de una ecuación se transformen de la misma manera bajo cambios de coordenadas.
Como cuestión de jerga, a veces nos referiremos a cantidades que se escriben en términos de tensores como {\bf covariante} (que no tiene nada que ver con ``covariante'' en contraposición a ``contravariante'').
Por tanto, decimos que (1.77) y (1.78) juntas sirven como forma covariante de las ecuaciones de Maxwell, mientras que (1.73) o (1.74) son no covariantes.

Introduzcamos ahora una clase especial de tensores, conocida como {\bf formas diferenciales} (o simplemente ``formas'').
Una forma diferencial $p$ es un tensor $(0,p)$ que es completamente antisimétrico.
Por lo tanto, los escalares son automáticamente formas 0 y los vectores duales son automáticamente formas uno (explicando así esta terminología de hace un tiempo).
También tenemos el $F_{\mu\nu}$ de 2 formas y el $\epsilon_{\mu\nu\rho\sigma}$ de 4 formas.
El espacio de todos los formularios $p$ se denomina $\Lambda^p$ y el espacio de todos los campos del formulario $p$ sobre una variedad $M$ se denomina $\Lambda^p(M)$.
Un ejercicio semisencillo de combinatoria revela que el número de formas $p$ linealmente independientes en un espacio vectorial de dimensión $n$ es $n!/(p!(n-p)!)$.
Entonces, en un punto de un Espacio-Tiempo de 4 dimensiones hay una forma 0 linealmente independiente, cuatro formas 1, seis formas 2, cuatro formas 3 y una forma 4.
No hay formas $p$ para $p>n$, ya que todos los componentes serán automáticamente cero por antisimetría.

¿Por qué deberíamos preocuparnos por las formas diferenciales? Esta es una pregunta difícil de responder sin más trabajo, pero la idea básica es que las formas pueden diferenciarse e integrarse, sin la ayuda de ninguna estructura geométrica adicional.
Retrasaremos la teoría de la integración para más adelante, pero veremos cómo diferenciar formas en breve.

Dada una forma $p$ $A$ y una forma $q$ $B$, podemos formar una forma $(p+q)$ conocida como {\bf producto cuña} $A\wedge B$ tomando el producto tensor antisimetrizado:
\begin{equation}
(A\wedge B)_{\mu_1 \cdots \mu_{p+q}} = {{(p+q)!}\over{p!\ q!}}
A_{[\mu_1\cdots \mu_p} B_{\mu_{p+1}\cdots\mu_{p+q}]}\,.\label{1.79}
\end{equation}
Así, por ejemplo, el producto cuña de dos formas 1 es
\begin{equation}
(A\wedge B)_{\mu\nu} = 2A_{[\mu}B_{\nu]} = A_\mu B_\nu
- A_\nu B_\mu\,.\label{1.80}
\end{equation}
Tenga en cuenta que
\begin{equation}
A\wedge B = (-1)^{pq} B\wedge A\ ,\label{1.81}
\end{equation}
por lo que puedes alterar el orden de un producto en cuña si tienes cuidado con los carteles.

La {\bf derivada exterior} ``d'' nos permite diferenciar campos de formulario $p$ para obtener campos de formulario $(p+1)$.
Se define como una derivada parcial antisimétrica apropiadamente normalizada:
\begin{equation}
({\rm d}A)_{\mu_1\cdots\mu_{p+1}} = (p+1)\p{[\mu_1}
A_{\mu_2\cdots \mu_{p+1}]}\,.\label{1.82}
\end{equation}
El ejemplo más simple es el gradiente, que es la derivada exterior de una forma 1:
\begin{equation}
( d\, \phi)_\mu = \p{\mu} \phi\,.\label{1.83}
\end{equation}
La razón por la que la derivada exterior merece especial atención es que \textit{es un tensor}, incluso en espacios-tiempos curvos, a diferencia de su prima la derivada parcial.
Como aún no hemos estudiado espacios curvos, no podemos probar esto, pero (1.82) define un tensor honesto sin importar cuáles sean la métrica y las coordenadas.

Otro hecho interesante sobre la diferenciación exterior es que, para cualquier forma $A$,
\begin{equation}
\d( d\,A) =0\ ,\label{1.84}
\end{equation}
que a menudo se escribe $ d\,^2 =0$.
Esta identidad es consecuencia de la definición de $ d\,$ y del hecho de que las derivadas parciales conmutan, $\p\alpha\p\beta = \p\beta\p\alpha$ (actuando sobre cualquier cosa).
Esto nos lleva al siguiente aparte matemático, sólo por diversión.
Definimos una forma $p$ $A$ como {\bf cerrada} si $ d\,A=0$, y {\bf exacta} si $A =  d\,B$ para alguna forma $(p-1)$ $B$.
Obviamente, todas las formas exactas son cerradas, pero lo contrario no es necesariamente cierto.
En una variedad $M$, las formas cerradas $p$ comprenden un espacio vectorial $Z^p(M)$ y las formas exactas comprenden un espacio vectorial $B^p(M)$.
Defina un nuevo espacio vectorial como módulo de formas cerradas de las formas exactas:
\begin{equation}
H^p(M) = {{Z^p(M)}\over{B^p(M)}}\,.\label{1.85}
\end{equation}
Esto se conoce como el $p$-ésimo espacio vectorial de cohomología de Rham y depende únicamente de la topología de la variedad $M$.
% (
El espacio de Minkowski es topológicamente equivalente a $\mathbb{R}^4$, lo cual no es interesante, de modo que todos los $H^p(M)$ desaparecen para $p>0$; para $p=0$ tenemos $H^0(M)=\mathbb{R}$.
Por tanto, en el espacio de Minkowski todas las formas cerradas son exactas excepto las formas cero; Las formas cero no pueden ser exactas ya que no hay formas $-1$ de las que puedan ser derivadas exteriores.
% )
Llama la atención que de esta manera se pueda extraer información sobre la topología, lo que esencialmente implica las soluciones de ecuaciones diferenciales.
La dimensión $b_p$ del espacio $H^p(M)$ se denomina $p$-ésimo número de Betti de $M$, y la característica de Euler viene dada por la suma alterna
\begin{equation}
\chi(M) = \sum_{p=0}^{n} (-1)^p b_p\,.\label{1.86}
\end{equation}
La teoría de la cohomología es la base de gran parte de la topología diferencial moderna.

Volviendo a la realidad, la operación final sobre formas diferenciales que introduciremos es {\bf dualidad de Hodge}.
Definimos el ``operador estrella de Hodge'' en una variedad de dimensiones $n$ como un mapa de formas $p$ a formas $(n-p)$.
\begin{equation}
(*A)_{\mu_1\cdots\mu_{n-p}} = {1\over {p!}}
\epsilon^{\nu_1\cdots\nu_p}{}_{\mu_1\cdots\mu_{n-p}}
A_{\nu_1\cdots\nu_p}\ ,\label{1.87}
\end{equation}
asignando $A$ a ``$A$ dual''.
A diferencia de nuestras otras operaciones sobre formas, el dual de Hodge depende de la métrica de la variedad (lo cual debería ser obvio, ya que tuvimos que elevar algunos índices en el tensor de Levi-Civita para definir (1.87)).
Al aplicar la estrella de Hodge dos veces se obtiene más o menos la forma original:
\begin{equation}
**A = (-1)^{s+p(n-p)}A\ ,\label{1.88}
\end{equation}
donde $s$ es el número de signos menos en los valores propios de la métrica (para el espacio de Minkowski, $s=1$).

Dos hechos sobre el dual de Hodge: primero, la ``dualidad'' en el sentido de Hodge es diferente de la relación entre vectores y vectores duales, aunque ambos pueden considerarse como el espacio de aplicaciones lineales desde el espacio original hasta $\mathbb{R}$ .
Observe que la dimensionalidad del espacio de $(n-p)$-forms es igual a la del espacio de $p$-forms, por lo que esto tiene al menos una posibilidad de ser cierto.
En el caso de las formas, el mapa lineal definido por una forma $(n-p)$ que actúa sobre una forma $p$ viene dado por el dual del producto de cuña de las dos formas.
Por lo tanto, si $A^{(n-p)}$ es una forma $(n-p)$ y $B^{(p)}$ es una forma $p$ en algún instante del Espacio-Tiempo, tenemos
\begin{equation}
*(A^{(n-p)}\wedge B^{(p)}) \in \mathbb{R}\,.\label{1.89}
\end{equation}
El segundo hecho se refiere a las formas diferenciales en el espacio euclidiano tridimensional.
El dual de Hodge del producto cuña de dos formas 1 da otra forma 1:
\begin{equation}
*(U\wedge V)_i = \epsilon_i{}^{jk}U_j V_k\,.\label{1.90}
\end{equation}
(Todos los prefactores se cancelan).
Dado que las formas 1 en el espacio euclidiano son como vectores, tenemos una aplicación de dos vectores a un solo vector.
Debes convencerte de que este es solo el producto cruzado convencional y que la aparición del tensor de Levi-Civita explica por qué el producto cruzado cambia de signo bajo paridad (intercambio de dos coordenadas o, de manera equivalente, vectores de base).
Esta es la razón por la que el producto cruzado sólo existe en tres dimensiones, porque sólo en tres dimensiones tenemos un mapa interesante de dos vectores duales a un tercer vector dual.
Si quisiera, podría definir un mapa de $n-1$ one-forms a un one-form único, pero no estoy seguro de que sea de alguna utilidad.

La electrodinámica proporciona un ejemplo especialmente convincente del uso de formas diferenciales.
De la definición de la derivada exterior, queda claro que la ecuación (1.78) se puede expresar de manera concisa como cierre de la forma dual $F_{\mu\nu}$:
\begin{equation}
 d\,F = 0\,.\label{1.91}
\end{equation}
¿Significa esto que $F$ también es exacto? Sí; Como hemos señalado, el espacio de Minkowski es topológicamente trivial, por lo que todas las formas cerradas son exactas.
Por lo tanto, debe haber un $A_\mu$ de forma única tal que
\begin{equation}
F =  d\,A\,.\label{1.92}
\end{equation}
Esta forma única es el familiar {\bf potencial vectorial} del electromagnetismo, con el componente $0$ dado por el potencial escalar, $A_0 = \phi$.
Si se parte de la opinión de que $A_\mu$ es el campo fundamental del electromagnetismo, entonces (1.91) se sigue como una identidad (a diferencia de una ley dinámica, una ecuación de movimiento).
La invariancia de calibre se expresa mediante la observación de que la teoría es invariante bajo $A \rightarrow A + d\,\lambda$ para algún escalar (forma cero) $\lambda$, y esto también es inmediato de la relación (1.92).
La otra de las ecuaciones de Maxwell, (1.77), se puede expresar como una ecuación entre tres formas:
\begin{equation}
\d(*F) = 4\pi(* J)\ ,\label{1.93}
\end{equation}
donde la forma única actual $J$ es solo el vector de cuatro actual con el índice reducido.
Usted debe completar los detalles.

Como curiosidad, la dualidad de Hodge es la base de uno de los temas más candentes de la física teórica actual.
Es difícil no darse cuenta de que las ecuaciones (1.91) y (1.93) parecen muy similares.
De hecho, si establecemos $J_\mu=0$, las ecuaciones son invariantes bajo las ``transformaciones de dualidad''
\begin{align}
F &\rightarrow \ast F\ ,\nonumber \\
\ast F &\rightarrow -F\,. \label{1.94}
\end{align}
Por lo tanto decimos que las ecuaciones de vacío de Maxwell son invariantes de dualidad, mientras que la invariancia se estropea en presencia de cargas.
Podríamos imaginar que en la naturaleza existieran monopolos magnéticos y eléctricos; entonces podríamos agregar un término de corriente magnética $4\pi(*J_M)$ al lado derecho de (1.91), y las ecuaciones serían invariantes bajo transformaciones de dualidad más el reemplazo adicional $J \leftrightarrow J_M$.
(Por supuesto, un lado derecho distinto de cero de (1.91) es inconsistente con $F= d\,A$, por lo que esta idea solo funciona si $A_\mu$ no es una variable fundamental).
Hace mucho tiempo, Dirac consideró la idea de los monopolos magnéticos y demostró que una condición necesaria para su existencia es que la carga fundamental del monopolo sea inversamente proporcional a la carga eléctrica fundamental.
Ahora bien, la carga eléctrica fundamental es un número pequeño; La electrodinámica está ``débilmente acoplada'', razón por la cual la teoría de la perturbación tiene tanto éxito en la electrodinámica cuántica (QED).
Pero la condición de Dirac sobre las cargas magnéticas implica que una transformación de dualidad lleva una teoría de cargas eléctricas débilmente acopladas a una teoría de monopolos magnéticos fuertemente acoplados (y viceversa).
Por desgracia, los monopolos no existen (que sepamos), así que estas ideas no son directamente aplicables al electromagnetismo; pero hay algunas teorías (como las teorías gauge supersimétricas no abelianas) para las que se ha conjeturado durante mucho tiempo que puede existir algún tipo de simetría de dualidad.
Si así fuera, tendríamos la oportunidad de analizar una teoría que parecía fuertemente acoplada (y por lo tanto difícil de resolver) observando la versión dual débilmente acoplada.
Un trabajo reciente de Seiberg, Witten y otros ha proporcionado pruebas muy sólidas de que esto es exactamente lo que sucede en determinadas teorías.
La esperanza es que estas técnicas nos permitan explorar diversos fenómenos que sabemos que existen en teorías de campos cuánticos fuertemente acoplados, como el confinamiento de quarks en hadrones.

Ahora hemos repasado esencialmente todo lo que hay que saber sobre el cuidado y alimentación de los tensores.
En la siguiente sección veremos más detenidamente las definiciones rigurosas de variedades y tensores, pero la mecánica básica ya está bastante bien cubierta.
Antes de pasar a matemáticas más abstractas, repasemos cómo funciona la física en el Espacio-Tiempo de Minkowski.

Comience con la línea mundial de una sola partícula.
Esto se especifica mediante un mapa $\mathbb{R} \rightarrow M$, donde $M$ es la variedad que representa el Espacio-Tiempo; Normalmente pensamos en la ruta como una curva parametrizada $x^\mu(\lambda)$.
Como se mencionó anteriormente, el vector tangente a esta ruta es $dx^\mu/d\lambda$ (tenga en cuenta que depende de la parametrización).
Un objeto de principal interés es la norma del vector tangente, que sirve para caracterizar la trayectoria; si el vector tangente es temporal/nulo/espacial en algún valor de parámetro $\lambda$, decimos que la ruta es temporal/nula/espacial en ese punto.
Esto explica por qué se usan las mismas palabras para clasificar vectores en el espacio tangente y los intervalos entre dos puntos, porque una línea recta que conecta, digamos, dos puntos separados en forma temporal será en sí misma temporal en cada punto a lo largo del camino.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/one5.pdf}
\end{figure}

Sin embargo, es importante ser conscientes del juego de manos que se está utilizando aquí.
La métrica, como tensor $(0,2)$, es una máquina que actúa sobre dos vectores (o dos copias del mismo vector) para producir un número.
Por tanto, es muy natural clasificar los vectores tangentes según el signo de su norma.
Pero el intervalo entre dos puntos no es algo tan natural; depende de una elección específica de camino (una ``línea recta'') que conecta los puntos, y esta elección a su vez depende del hecho de que el Espacio-Tiempo es plano (lo que permite una elección única de línea recta entre los puntos).
Un objeto más natural es el {\bf elemento de línea}, o intervalo infinitesimal:
\begin{equation}
ds^2 = \eta_\mn dx^\mu dx^\nu\,.\label{1.95}
\end{equation}
A partir de esta definición resulta tentador extraer la raíz cuadrada e integrar a lo largo de una trayectoria para obtener un intervalo finito.
Pero como $ds^2$ no tiene por qué ser positivo, definimos diferentes procedimientos para diferentes casos.
Para caminos espaciales definimos la {\bf longitud del camino}
\begin{equation}
\Delta s = \int \sqrt{\eta_\mn{{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}}\ d\lambda\ ,\label{1.96}
\end{equation}
donde se toma la integral sobre el camino.
Para rutas nulas, el intervalo es cero, por lo que no se requiere ninguna fórmula adicional.
Para caminos temporales definimos el {\bf tiempo propio}
\begin{equation}
\Delta \tau = \int \sqrt{-\eta_\mn{{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}}\ d\lambda\ ,\label{1.97}
\end{equation}
lo cual será positivo.
Por supuesto, podemos considerar trayectorias que sean temporales en algunos lugares y espaciales en otros, pero afortunadamente rara vez es necesario, ya que las trayectorias de las partículas físicas nunca cambian de carácter (las partículas masivas se mueven por trayectorias temporales, las partículas sin masa se mueven por trayectorias nulas).
Además, la frase ``tiempo adecuado'' es especialmente apropiada, ya que $\tau$ \textit{en realidad mide el tiempo transcurrido en un reloj físico transportado a lo largo del camino}.
Este punto de vista deja muy clara la ``paradoja de los gemelos'' y enigmas similares; Dos líneas de mundo, no necesariamente rectas, que se cruzan en dos eventos diferentes en el Espacio-Tiempo tendrán tiempos adecuados medidos por la integral (1.97) a lo largo de los caminos apropiados, y estos dos números serán en general diferentes incluso si las personas que viajan a lo largo de ellos nacieron en al mismo tiempo.

Pasemos de la consideración de las trayectorias en general a las trayectorias de partículas masivas (que siempre serán temporales).
Dado que el tiempo adecuado se mide mediante un reloj que viaja en una línea temporal similar al tiempo, es conveniente utilizar $\tau$ como parámetro a lo largo de la ruta.
Es decir, usamos (1.97) para calcular $\tau(\lambda)$, que (si $\lambda$ es un buen parámetro en primer lugar) podemos invertir para obtener $\lambda(\tau)$, después de lo cual podemos pensar en la ruta como $x^\mu(\tau)$ .
El vector tangente en esta parametrización se conoce como {\bf cuatro velocidades}, $U^\mu$:
\begin{equation}
U^\mu = {{dx^\mu}\over{d\tau}}\,.\label{1.98}
\end{equation}
Desde $d\tau^2 = -\eta_\mn dx^\mu dx^\nu$, las cuatro velocidades se normalizan automáticamente:
\begin{equation}
\eta_\mn U^\mu U^\nu = -1\,.\label{1.99}
\end{equation}
(Siempre será negativo, ya que sólo lo estamos definiendo para trayectorias temporales.
También podrías definir un vector análogo para caminos espaciales; Las rutas nulas dan algunos problemas adicionales ya que la norma es cero.)
En el sistema de reposo de una partícula, su cuatro velocidades tiene componentes $U^\mu = (1,0,0,0)$.

Un vector relacionado es el {\bf cuatro vectores de energía-momentum}, definido por
\begin{equation}
p^\mu = m U^\mu\ ,\label{1.100}
\end{equation}
donde $m$ es la masa de la partícula.
La masa es una cantidad fija independiente del sistema inercial; lo que usted puede estar acostumbrado a considerar como ``masa en reposo''. Resulta mucho más conveniente tomar esto como la masa de una vez por todas, en lugar de pensar que la masa depende de la velocidad.
La {\bf energía} de una partícula es simplemente $p^0$, la componente temporal de su vector energía-momentum.
Dado que es sólo un componente de un cuatro vectores, no es invariante bajo las transformaciones de Lorentz; Sin embargo, eso es de esperarse, ya que la energía de una partícula en reposo no es la misma que la de la misma partícula en movimiento.
En el sistema de reposo de la partícula tenemos $p^0 =m$; Recordando que hemos establecido $c=1$, encontramos que hemos encontrado la ecuación que convirtió a Einstein en una celebridad, $E=mc^2$.
(Las ecuaciones de campo de la Relatividad General son en realidad mucho más importantes que ésta, pero ``$R_\mn - {1\over 2} Rg_\mn = 8\pi G T_\mn$'' no provoca la reacción visceral que se obtiene con ``$E=mc^2$''.)
En un marco móvil podemos encontrar los componentes de $p^\mu$ realizando una transformación de Lorentz; para una partícula que se mueve con (tres) velocidad $v$ a lo largo del eje $x$ tenemos
\begin{equation}
p^\mu = (\gamma m, v\gamma m, 0 ,0)\ ,\label{1.101}
\end{equation}
donde $\gamma = 1/\sqrt{1-v^2}$.
Para $v$ pequeño, esto da $p^0 = m+{1\over 2}mv^2$ (lo que normalmente consideramos energía en reposo más energía cinética) y $p^1 = mv$ (lo que normalmente consideramos momentum [newtoniano]).
De modo que el vector energía-momentum hace honor a su nombre.

La pieza central de la física anterior a la relatividad es la Segunda Ley de Newton, o ${\bf f}=m{\bf a} = d{\bf p}/dt$.
Una ecuación análoga debería ser válida en Relatividad Especial, y el requisito de que sea tensorial nos lleva directamente a introducir una fuerza de cuatro vectores $f^\mu$ que satisfaga
\begin{equation}
f^\mu = m{{d^2}\over{d\tau^2}}x^\mu(\tau) = {{d}\over{d\tau}}
p^\mu(\tau)\,.\label{1.102}
\end{equation}
El ejemplo más simple de fuerza en la física newtoniana es la fuerza debida a la gravedad.
En relatividad, sin embargo, la gravedad no se describe por una fuerza, sino por la curvatura del propio Espacio-Tiempo.
En lugar de ello, consideremos el electromagnetismo.
La fuerza de Lorentz tridimensional viene dada por ${\bf f} = q({\bf E} + {\bf v}\times{\bf B})$, donde $q$ es la carga de la partícula.
Nos gustaría una generalización tensorial de esta ecuación.
Resulta que hay una respuesta única:
\begin{equation}
f^\mu = qU^\lambda F_\lambda{}^\mu\,.\label{1.103}
\end{equation}
Puedes comprobar por ti mismo que esto se reduce a la versión newtoniana en el límite de pequeñas velocidades.
Observe cómo el requisito de que la ecuación sea tensorial, que es una forma de garantizar la invariancia de Lorentz, restringió severamente las posibles expresiones que podríamos obtener.
Este es un ejemplo de un fenómeno muy general, en el que un pequeño número de una variedad aparentemente infinita de posibles leyes físicas son seleccionadas por las exigencias de la simetría.

Aunque $p^\mu$ proporciona una descripción completa de la energía y el momentum de una partícula, para sistemas extendidos es necesario ir más allá y definir el {\bf tensor energía-momentum} (a veces llamado tensor tensión-energía), $T^\mn$ .
Este es un tensor simétrico $(2,0)$ que nos dice todo lo que necesitamos saber sobre los aspectos similares a la energía de un sistema: densidad de energía, presión, tensión, etc.
Una definición general de $T^\mn$ es ``el flujo de cuatro momentos $p^\mu$ a través de una superficie de constante $x^\nu$''.
Para hacer esto más concreto, consideremos la categoría muy general de materia que puede caracterizarse como un {\bf fluido} --- un continuo de materia descrito por cantidades macroscópicas como temperatura, presión, entropía, viscosidad, etc.
De hecho esta definición es tan general que resulta de poca utilidad.
En la Relatividad General, esencialmente todos los tipos interesantes de materia pueden considerarse como {\bf fluidos perfectos}, desde las estrellas hasta los campos electromagnéticos y el universo entero.
Schutz define un fluido perfecto como aquel sin conducción de calor y sin viscosidad, mientras que Weinberg lo define como un fluido que parece isotrópico en su estado de reposo; Estos dos puntos de vista resultan ser equivalentes.
Operacionalmente, se debe pensar en un fluido perfecto como aquel que puede caracterizarse completamente por su presión y densidad.

Para entender los fluidos perfectos, comencemos con el ejemplo aún más simple de {\bf polvo}.
El polvo se define como un conjunto de partículas en reposo unas con respecto a otras o, alternativamente, como un fluido perfecto con presión cero.
Dado que todas las partículas tienen la misma velocidad en cualquier sistema inercial fijo, podemos imaginar un ``campo de cuatro velocidades'' $U^\mu(x)$ definido en todo el Espacio-Tiempo.
(De hecho, sus componentes son los mismos en cada punto).
Defina el {\bf flujo de números de cuatro vectores} como
\begin{equation}
N^\mu = n U^\mu\ ,\label{1.104}
\end{equation}
donde $n$ es la densidad numérica de las partículas medida en su estado de reposo.
Entonces $N^0$ es la densidad numérica de partículas medida en cualquier otro cuadro, mientras que $N^i$ es el flujo de partículas en la dirección $x^i$.
Imaginemos ahora que cada una de las partículas tiene la misma masa $m$.
Luego, en el cuadro de reposo, la densidad de energía del polvo viene dada por
\begin{equation}
\rho = nm\,.\label{1.105}
\end{equation}
Por definición, la densidad de energía especifica completamente el polvo.
Pero $\rho$ sólo mide la densidad de energía en el marco de reposo; ¿Qué pasa con otros marcos? Observamos que tanto $n$ como $m$ son componentes $0$ de cuatro vectores en su marco de reposo; específicamente, $N^\mu = (n,0,0,0)$ y $p^\mu = (m,0,0,0)$.
Por lo tanto, $\rho$ es el componente $\mu = 0$, $\nu =0$ del tensor $p\otimes N$ medido en su marco de reposo.
Por tanto, nos vemos obligados a definir el tensor de energía-momentum del polvo:
\begin{equation}
T^\mn_{\rm dust} = p^\mu N^\nu = nm U^\mu U^\nu = \rho U^\mu U^\nu
\ ,\label{1.106}
\end{equation}
donde $\rho$ se define como la densidad de energía en el marco de reposo.

Una vez dominado el polvo, los fluidos perfectos más generales no son mucho más complicados.
Recuerde que ``perfecto'' puede entenderse como ``isotrópico en su sistema de reposo''. Esto a su vez significa que $T^\mn$ es diagonal: no hay flujo neto de ningún componente del momentum en una dirección ortogonal.
Además, todos los componentes espaciales distintos de cero deben ser iguales, $T^{11} = T^{22}=T^{33}$.
Los dos únicos números independientes son, por tanto, $T^{00}$ y uno de los $T^{ii}$; Podemos optar por llamar a la primera densidad de energía $\rho$ y a la segunda presión $p$.
(Lamento que sea la misma letra que el momentum).
Por lo tanto, el tensor de energía-momentum de un fluido perfecto toma la siguiente forma en su sistema de reposo:
\begin{equation}
T^\mn = \left(\mqty{\rho&0&0&0 \\  0&p&0&0 \\  0&0&p&0 \\
0&0&0&p \\ }\right)\,.\label{1.107}
\end{equation}
Naturalmente nos gustaría una fórmula que fuera buena en cualquier contexto.
Para el polvo teníamos $T^\mn = \rho U^\mu U^\nu$, por lo que podríamos comenzar adivinando $(\rho+p)U^\mu U^\nu$, lo que da
\begin{equation}
\left(\mqty{\rho+p &0&0&0 \\  0&0&0&0 \\  0&0&0&0 \\
0&0&0&0 \\ }\right)\,.\label{1.108}
\end{equation}
Por lo tanto, para obtener la respuesta que queremos debemos sumar
\begin{equation}
\left(\mqty{-p&0&0&0 \\  0&p&0&0 \\  0&0&p&0 \\
0&0&0&p \\ }\right)\,.\label{1.109}
\end{equation}
Afortunadamente, esto tiene una generalización covariante obvia, a saber, $p\eta^\mn$.
Por tanto, la forma general del tensor de energía-momentum para un fluido perfecto es
\begin{equation}
T^\mn = (\rho+p) U^\mu U^\nu + p\eta^\mn\,.\label{1.110}
\end{equation}
Esta es una fórmula importante para aplicaciones como la estructura estelar y la cosmología.

Como ejemplos adicionales, consideremos los tensores de energía-momentum del electromagnetismo y la teoría de campos escalares.
Sin ninguna explicación alguna, estos están dados por
\begin{equation}
T^\mn_{\rm e+m} = {-1\over{4\pi}}(F^{\mu\lambda}F^\nu{}_\lambda
-{1\over 4}\eta^{\mu\nu} F^{\lambda\sigma}F_{\lambda\sigma})
\ ,\label{1.111}
\end{equation}
y
\begin{equation}
T^\mn_{\rm scalar} = \eta^{\mu\lambda}\eta^{\nu\sigma}
\p\lambda\phi\p\sigma\phi - {1\over 2}\eta^\mn (\eta^{\lambda\sigma}
\p\lambda\phi\p\sigma\phi + m^2\phi^2)\,.\label{1.112}
\end{equation}
Puedes comprobar por ti mismo que, por ejemplo, $T^{00}$ en cada caso es igual a lo que esperarías que fuera la densidad de energía.

Además de ser simétrico, $T^\mn$ tiene la propiedad aún más importante de ser {\it conservado}.
En este contexto, la conservación se expresa como la desaparición de la ``divergencia'':
\begin{equation}
\p\mu T^\mn =0\,.\label{1.113}
\end{equation}
Este es un conjunto de cuatro ecuaciones, una para cada valor de $\nu$.
La ecuación $\nu =0$ corresponde a la conservación de la energía, mientras que $\p\mu T^{\mu k}=0$ expresa la conservación de la componente $k^{\rm th}$ del momentum.
No vamos a probar esto en general; la prueba se sigue para cualquier fuente individual de materia a partir de las ecuaciones de movimiento que obedece esa clase de materia.
De hecho, una forma de definir $T^\mn$ sería ``un tensor $(2,0)$ con unidades de energía por volumen, que se conserva''. Puedes demostrar la conservación del tensor de energía-momentum para el electromagnetismo, por ejemplo, tomando la divergencia de (1.111) y usando las ecuaciones de Maxwell como se analizó anteriormente.

Un último comentario: ya hemos mencionado que en la Relatividad General la gravitación no cuenta como una ``fuerza''. Como punto relacionado, el campo gravitacional tampoco tiene un tensor de energía-momentum.
De hecho, es muy difícil encontrar una expresión local sensata para la energía de un campo gravitacional; Se han hecho varias sugerencias, pero todas tienen sus inconvenientes.
Aunque no existe una respuesta ``correcta'', es una cuestión importante desde el punto de vista de formular preguntas aparentemente razonables como ``¿Cuál es la energía emitida por segundo por un púlsar binario como resultado de la radiación gravitacional?''



\chapter{Colectores}
%\addcontentsline{toc}{chapter}{Colectores}

Después de la invención de la Relatividad Especial, Einstein intentó durante varios años inventar una teoría de la gravedad invariante de Lorentz, sin éxito.
Su eventual avance fue reemplazar el Espacio-Tiempo de Minkowski con un Espacio-Tiempo curvo, donde la curvatura era creada por (y reaccionaba contra) la energía y el momentum.
Antes de explorar cómo sucede esto, tenemos que aprender un poco sobre las matemáticas de los espacios curvos.
Primero veremos las variedades en general y luego, en la siguiente sección, estudiaremos la curvatura.
En aras de la generalidad, normalmente trabajaremos en $n$ dimensiones, aunque puede tomar $n=4$ si lo desea.

Una variedad (o, a veces, una variedad diferenciable) es uno de los conceptos más fundamentales en matemáticas y física.
Todos conocemos las propiedades del espacio euclidiano de $n$-dimensional, $\mathbb{R}^n$, el conjunto de $n$-tuplas $(x^1,\ldots,x^n)$.
La noción de variedad captura la idea de un espacio que puede ser curvo y tener una topología complicada, pero que en regiones locales se parece a $\mathbb{R}^n$.
(Aquí con ``parece'' no queremos decir que la métrica sea la misma, sino sólo nociones básicas de análisis como conjuntos abiertos, funciones y coordenadas).
Todo el colector se construye cosiendo suavemente estas regiones locales.
Ejemplos de variedades incluyen:

\begin{itemize}
\item $\mathbb{R}^n$ en sí, incluida la línea ($\mathbb{R}$), el avión ($\mathbb{R}^2$), etc.
Esto debería ser obvio, ya que $\mathbb{R}^n$ se parece a $\mathbb{R}^n$ no sólo a nivel local sino también global.
\item La esfera $n$, $S^n$.
Esto se puede definir como el lugar geométrico de todos los puntos a una distancia fija del origen en $\mathbb{R}^{n+1}$.
El círculo es, por supuesto, $S^1$, y la dos esferas $S^2$ será uno de nuestros ejemplos favoritos de variedad.

\item El $n$-toro $T^n$ resulta de tomar un cubo de $n$-dimensional e identificar los lados opuestos.
Por tanto, $T^2$ es la superficie tradicional de un donut.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two01.pdf}
\end{figure}

\item Una superficie de Riemann del género $g$ es esencialmente un dos toros con agujeros $g$ en lugar de solo uno.
$S^2$ puede considerarse como una superficie de Riemann de género cero.
Para aquellos de ustedes que saben lo que significan las palabras, cada variedad bidimensional ``compacta, orientable y sin límites'' es una superficie de Riemann de algún género.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two02.pdf}
\end{figure}

\item De manera más abstracta, un conjunto de transformaciones continuas, como rotaciones en $\mathbb{R}^n$, forma una variedad.
Los grupos de mentiras son variedades que también tienen una estructura de grupo.

\item El producto directo de dos variedades es una variedad.
Es decir, dadas las variedades $M$ y $M'$ de dimensión $n$ y $n'$, podemos construir una variedad $M\times M'$, de dimensión $n+n'$, que consta de pares ordenados $(p,p')$ para todos los $p\in M$ y $p'\in M'$.

\end{itemize}

Con todos estos ejemplos, la noción de variedad puede parecer vacía; ¿Qué no es una variedad? Hay muchas cosas que no son variedades, porque en algún lugar no se parecen localmente a $\mathbb{R}^n$.
Los ejemplos incluyen una línea unidimensional que desemboca en un plano bidimensional y dos conos pegados entre sí en sus vértices.
(Un solo cono está bien; puedes imaginarte suavizando el vértice).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two03.pdf}
\end{figure}

Nos acercaremos ahora a la definición rigurosa de esta sencilla idea, que requiere una serie de definiciones preliminares.
Muchos de ellos son bastante claros de todos modos, pero es bueno estar completos.

La noción más elemental es la de un {\bf map} entre dos conjuntos.
(Asumimos que sabes qué es un conjunto).
Dados dos conjuntos $M$ y $N$, un mapa $\phi: M\rightarrow N$ es una relación que asigna, a cada elemento de $M$, exactamente un elemento de $N$.
Por tanto, un mapa es simplemente una simple generalización de una función.
La imagen canónica de un mapa se ve así:

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/two04.pdf}
\end{figure}

Dados dos mapas $\phi: A\rightarrow B$ y $\psi:B\rightarrow C$, definimos la {\bf composición} $\psi\circ\phi: A\rightarrow C$ mediante la operación $(\psi\circ\phi)(a)=\psi(\phi(a))$.
Entonces $a\in A$, $\phi(a)\in B$ y, por lo tanto, $(\psi\circ\phi)(a)\in C$.
El orden en el que están escritos los mapas tiene sentido, ya que el de la derecha actúa primero.
En fotos:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two05.pdf}
\end{figure}

Un mapa $\phi$ se llama {\bf uno a uno} (o ``inyectivo'') si cada elemento de $N$ tiene como máximo un elemento de $M$ asignado, y {\bf sobre} (o ``sobreyectivo'') si cada elemento de $N$ tiene al menos un elemento de $M$ asignado.
(Si lo piensas bien, un mejor nombre para ``uno a uno'' sería ``dos a dos''.)
Considere una función $\phi: \mathbb{R}\rightarrow\mathbb{R}$.
Entonces $\phi(x)=e^x$ es uno a uno, pero no concordante; $\phi(x)=x^3-x$ está en sintonía, pero no uno a uno; $\phi(x)=x^3$ es ambos; y $\phi(x)=x^2$ no es ninguno de los dos.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two06.pdf}
\end{figure}

El conjunto $M$ se conoce como {\bf dominio} del mapa $\phi$, y el conjunto de puntos en $N$ al que se asigna $M$ se llama {\bf imagen} de $\phi$.
Para algún subconjunto $U\subset N$, el conjunto de elementos de $M$ que se asignan a $U$ se denomina {\bf preimagen} de $U$ en $\phi$ o $\phi^{-1}(U)$.
Una aplicación que es tanto uno a uno como sobre se conoce como {\bf invertible} (o ``biyectiva'').
En este caso podemos definir la {\bf aplicación inversa} $\phi^{-1}:N\rightarrow M$ por $(\phi^{-1}\circ\phi)(a)=a$.
(Tenga en cuenta que se utiliza el mismo símbolo $\phi^{-1}$ tanto para la preimagen como para el mapa inverso, aunque la primera siempre está definida y el segundo solo se define en algunos casos especiales).
De este modo:

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two07.pdf}
\end{figure}

La noción de continuidad de un mapa entre espacios topológicos (y por lo tanto variedades) es en realidad muy sutil, cuya formulación precisa realmente no necesitaremos.
Sin embargo, las nociones intuitivas de continuidad y diferenciabilidad de aplicaciones $\phi:\mathbb{R}^m\rightarrow\mathbb{R}^n$ entre espacios euclidianos son útiles.
Un mapa de $\mathbb{R}^m$ a $\mathbb{R}^n$ toma una tupla $m$ $(x^1,x^2,\ldots,x^m)$ a una tupla $n$ $(y^1,y^2,\ldots,y^n)$ y, por lo tanto, puede considerarse como una colección de funciones $n$ $\phi^i$ de $m$ variables:
\begin{equation}
\mqty{y^1 =\phi^1(x^1,x^2,\ldots,x^m) \\
y^2=\phi^2(x^1,x^2,\ldots,x^m) \\
\cdot \\ \cdot \\ \cdot \\  y^n=\phi^n(x^1,x^2,\ldots,x^m)\,. \\ }
\label{2.1}
\end{equation}
Nos referiremos a cualquiera de estas funciones como $C^p$ si es continua y $p$ veces diferenciable, y nos referiremos al mapa completo $\phi:\mathbb{R}^m\rightarrow\mathbb{R}^n$ como $C^p$ si cada una de las funciones que la componen es al menos $C^p$ .
Por lo tanto, un mapa $C^0$ es continuo pero no necesariamente diferenciable, mientras que un mapa $C^\infty$ es continuo y puede diferenciarse tantas veces como desee.
Los mapas $C^\infty$ a veces se denominan {\bf smooth}.
Llamaremos a dos conjuntos $M$ y $N$ {\bf difeomorfismo} si existe un mapa $C^\infty$ $\phi:M\rightarrow N$ con una $C^\infty$ inversa $\phi^{-1}:N\rightarrow M$; el mapa $\phi$ se llama entonces difeomorfismo.

Aparte: la noción de que dos espacios son difeomorfos solo se aplica a variedades, donde la noción de diferenciabilidad se hereda del hecho de que el espacio se parece a $\mathbb{R}^n$ localmente.
Pero se puede definir la ``continuidad'' de aplicaciones entre espacios topológicos (no necesariamente variedades), y decimos que dos de esos espacios son ``homeomórficos'', lo que significa ``topológicamente equivalente a'', si hay una aplicación continua entre ellos con una inversa continua.
Por tanto, es concebible que existan espacios que sean homeomorfos pero no difeomorfos; topológicamente igual pero con distintas ``estructuras diferenciables''. En 1964, Milnor demostró que $S^7$ tenía 28 estructuras diferenciables diferentes; resulta que para $n<7$ solo hay una estructura diferenciable en $S^n$, mientras que para $n>7$ el número crece mucho.
$\mathbb{R}^4$ tiene infinitas estructuras diferenciables.

Una parte del cálculo convencional que necesitaremos más adelante es la {\bf regla de la cadena}.
Imaginemos que tenemos los mapas $f:\mathbb{R}^m\rightarrow \mathbb{R}^n$ y $g:\mathbb{R}^n\rightarrow \mathbb{R}^l$, y por tanto la composición $(g\circ f):\mathbb{R}^m\rightarrow\mathbb{R}^l$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two08.pdf}
\end{figure}

\noindent
Podemos representar cada espacio en términos de coordenadas: $x^a$ en $\mathbb{R}^m$, $y^b$ en $\mathbb{R}^n$ y $z^c$ en $\mathbb{R}^l$, donde los índices oscilan sobre los valores apropiados.
La regla de la cadena relaciona las derivadas parciales de la composición con las derivadas parciales de los mapas individuales:
\begin{equation}
{{\partial}\over{\partial x^a}}(g\circ f)^c =
\sum_b {{\partial f^b}\over{\partial x^a}}{{\partial g^c}
\over{\partial y^b}}\,.\label{2.2}
\end{equation}
Esto generalmente se abrevia como
\begin{equation}
{{\partial}\over{\partial x^a}} = \sum_b {{\partial y^b}\over
{\partial x^a}}{{\partial}\over{\partial y^b}}\,.\label{2.3}
\end{equation}
No hay nada ilegal o inmoral en utilizar esta forma de regla de la cadena, pero deberías poder visualizar los mapas que subyacen a la construcción.
Recuerde que cuando $m=n$ el determinante de la matriz $\partial y^b/\partial x^a$ se llama {\bf jacobiano} del mapa, el mapa es invertible siempre que el jacobiano sea distinto de cero.

Es de suponer que estas definiciones básicas le resultaban familiares, aunque las recordara vagamente.
Ahora los usaremos en la definición rigurosa de variedad.
Desafortunadamente, se requiere un procedimiento un tanto barroco para formalizar esta noción relativamente intuitiva.
Primero tendremos que definir la noción de conjunto abierto, en el que podemos colocar sistemas de coordenadas, y luego unir los conjuntos abiertos de manera adecuada.

Comience con la noción de una {\bf bola abierta}, que es el conjunto de todos los puntos $x$ en $\mathbb{R}^n$ tal que $|x-y|<r$ para algunos $y\in \mathbb{R}^n$ y $r\in \mathbb{R}$ fijos, donde $|x-y|=[\sum_i(x^i-y^i)^2]^{1/2}$.
Tenga en cuenta que esta es una desigualdad estricta: la bola abierta es el interior de una esfera $n$ de radio $r$ centrada en $y$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two09.pdf}
\end{figure}

\noindent
Un {\bf conjunto abierto} en $\mathbb{R}^n$ es un conjunto construido a partir de una unión arbitraria (tal vez infinita) de bolas abiertas.
En otras palabras, $V\subset \mathbb{R}^n$ está abierto si, para cualquier $y\in V$, hay una bola abierta centrada en $y$ que está completamente dentro de $V$.
En términos generales, un conjunto abierto es el interior de alguna superficie cerrada de dimensión $(n-1)$ (o la unión de varios de esos interiores).
Al definir una noción de conjuntos abiertos, hemos equipado a $\mathbb{R}^n$ con una topología, en este caso, la ``topología métrica estándar''.

Un {\bf chart} o {\bf sistema de coordenadas} consta de un subconjunto $U$ de un conjunto $M$, junto con un mapa uno a uno $\phi:U\rightarrow\mathbb{R}^n$, de modo que la imagen $\phi(U)$ está abierta en $\mathbb{R}$.
(Cualquier mapa está sobre su imagen, por lo que el mapa $\phi:U\rightarrow \phi(U)$ es invertible).
Entonces podemos decir que $U$ es un conjunto abierto en $M$.
(Por lo tanto, hemos inducido una topología en $M$, aunque no la exploraremos).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two10.pdf}
\end{figure}

\noindent
Un {\bf $C^\infty$ atlas} es una colección indexada de cartas $\{(U_\alpha,\phi_\alpha)\}$ que satisface dos condiciones:

\begin{enumerate}
\item La unión del $U_\alpha$ es igual a $M$; es decir, la tapa $U_\alpha$ $M$.
\item Los gráficos están cosidos suavemente.
Más precisamente, si dos gráficos se superponen, $U_\alpha\cap U_\beta\neq\emptyset$, entonces el mapa $(\phi_\alpha\circ\phi_\beta^{-1})$ toma puntos en $\phi_\beta(U_\alpha\cap U_\beta)\subset\mathbb{R}^n$ {\it sobre} $\phi_\alpha(U_\alpha\cap U_\beta)\subset\mathbb{R}^n$, y todos estos mapas deben estar $C^\infty$ donde están definidos.
Esto debería quedar más claro en las imágenes:

\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/two11.pdf}
\end{figure}

\noindent
Entonces, una carta es lo que normalmente consideramos un sistema de coordenadas en algún conjunto abierto, y un atlas es un sistema de cartas que se relacionan suavemente en sus superposiciones.

Entonces, por fin: una variedad {\bf} de dimensión $C^\infty$ $n$ (o variedad $n$ para abreviar) es simplemente un conjunto $M$ junto con un ``atlas máximo'', uno que contiene todas las cartas compatibles posibles.
(También podemos reemplazar $C^\infty$ por $C^p$ en todas las definiciones anteriores.
Para nuestros propósitos, el grado de diferenciabilidad de una variedad no es crucial; Siempre asumiremos que cualquier variedad es tan diferenciable como sea necesario para la aplicación en consideración).
El requisito de que el atlas sea máximo es que dos espacios equivalentes equipados con atlas diferentes no cuenten como variedades diferentes.
Esta definición captura en términos formales nuestra noción de un conjunto que se parece localmente a $\mathbb{R}^n$.
Por supuesto, rara vez tendremos que hacer uso de todo el poder de la definición, pero la precisión tiene su propia recompensa.

Una cosa buena de nuestra definición es que no se basa en una incrustación de la variedad en algún espacio euclidiano de dimensiones superiores.
De hecho, cualquier variedad $n$-dimensional puede estar incrustada en $\R^{2n}$ (``teorema de incrustación de Whitney''), y algunas veces haremos uso de este hecho (como en nuestra definición de la esfera anterior).
Pero es importante reconocer que la variedad tiene una existencia individual independiente de cualquier incrustación.
No tenemos motivos para creer, por ejemplo, que el Espacio-Tiempo de cuatro dimensiones esté atrapado en un espacio mayor.
(En realidad, varias personas, teóricos de cuerdas, etc., creen que nuestro mundo de cuatro dimensiones es parte de un Espacio-Tiempo de diez u once dimensiones, pero en lo que respecta a la Relatividad General, la visión de cuatro dimensiones es perfectamente adecuada).

¿Por qué era necesario ser tan meticuloso con los gráficos y sus superposiciones, en lugar de simplemente cubrir cada variedad con un solo gráfico? Porque la mayoría de las variedades no se pueden cubrir con un solo gráfico.
Considere el ejemplo más simple, $S^1$.
Hay un sistema de coordenadas convencional, $\theta: S^1\rightarrow\R$, donde $\theta=0$ está en la parte superior del círculo y gira hasta $2\pi$.
Sin embargo, en la definición de un gráfico hemos requerido que la imagen $\theta(S^1)$ esté abierta en $\R$.
Si incluimos $\theta=0$ o $\theta=2\pi$, tenemos un intervalo cerrado en lugar de uno abierto; si excluimos ambos puntos, no habremos cubierto todo el círculo.
Entonces necesitamos al menos dos gráficos, como se muestra.

\begin{figure}[h]
\centering
\includegraphics[width=0.3\linewidth]{imagenes/two12.pdf}
\end{figure}

Un ejemplo algo más complicado lo proporciona $S^2$, donde una vez más ningún gráfico cubrirá la variedad.
Una proyección de Mercator, utilizada tradicionalmente para mapas mundiales, no detecta los polos norte y sur (así como la línea de cambio de fecha internacional, lo que implica el mismo problema con $\theta$ que encontramos para $S^1$.)
Tomemos como $S^2$ el conjunto de puntos en $\R^3$ definido por $(x^1)^2 +(x^2)^2 +(x^3)^2 =1$.
Podemos construir un gráfico a partir de un conjunto abierto $U_1$, definido como la esfera menos el polo norte, mediante una ``proyección estereográfica'':

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two13.pdf}
\end{figure}

\noindent
Así, trazamos una línea recta desde el polo norte hasta el plano definido por $x^3 = -1$, y asignamos al punto $S^2$ interceptado por la línea las coordenadas cartesianas $(y^1,y^2)$ del punto apropiado en el plano.
Explícitamente, el mapa viene dado por
\begin{equation}
\phi_1(x^1,x^2,x^3) \equiv (y^1,y^2) = \left(
{{2x^1}\over{1-x^3}}\ ,\ {{2x^2}\over{1-x^3}}\right)\,.\label{2.4}
\end{equation}
Le recomendamos que compruebe esto usted mismo.
Otro gráfico $(U_2,\phi_2)$ se obtiene proyectando desde el polo sur al plano definido por $x^3 = +1$.
Las coordenadas resultantes cubren la esfera menos el polo sur y están dadas por
\begin{equation}
\phi_2(x^1,x^2,x^3) \equiv (z^1,z^2) = \left(
{{2x^1}\over{1+x^3}}\ ,\ {{2x^2}\over{1+x^3}}\right)\,.\label{2.5}
\end{equation}
Juntos, estos dos gráficos cubren toda la variedad y se superponen en la región $-1<x^3<+1$.
Otra cosa que puedes comprobar es que la composición $\phi_2\circ\phi_1^{-1}$ viene dada por
\begin{equation}
z^i = {{4y^i}\over{[(y^1)^2 +(y^2)^2]}}\ ,\label{2.6}
\end{equation}
y está $C^\infty$ en la región de superposición.
Mientras limitemos nuestra atención a esta región, (2.6) es justo lo que normalmente consideramos un cambio de coordenadas.

Por lo tanto, vemos la necesidad de cartas y atlas: muchas variedades no pueden cubrirse con un único sistema de coordenadas.
(Aunque algunos pueden, incluso aquellos con topología no trivial.
¿Se le ocurre un buen sistema de coordenadas que cubra el cilindro, $S^1\times\R$?) Sin embargo, a menudo es más conveniente trabajar con un solo gráfico y simplemente realizar un seguimiento del conjunto de puntos que no están incluidos.

El hecho de que las variedades se parezcan localmente a $\R^n$, lo que se manifiesta mediante la construcción de gráficos de coordenadas, introduce la posibilidad de análisis de variedades, incluidas operaciones como la diferenciación y la integración.
Considere dos variedades $M$ y $N$ de dimensiones $m$ y $n$, con gráficos de coordenadas $\phi$ en $M$ y $\psi$ en $N$.
Imaginemos que tenemos una función $f:M\rightarrow N$,

\begin{figure}[h]
\centering
\includegraphics[width=0.55\linewidth]{imagenes/two14.pdf}
\end{figure}

\noindent
Con solo pensar en $M$ y $N$ como conjuntos, no podemos diferenciar con indiferencia el mapa $f$, ya que no sabemos qué significa dicha operación.
Pero las cartas de coordenadas nos permiten construir el mapa $(\psi\circ f\circ\phi^{-1}):\R^m\rightarrow\R^n$.
(Siéntase libre de insertar las palabras ``donde se definen los mapas'' cuando sea apropiado, aquí y más adelante.)
Este es solo un mapa entre espacios euclidianos y se aplican todos los conceptos del cálculo avanzado.
Por ejemplo, $f$, considerada como una función con valor $N$ en $M$, se puede diferenciar para obtener ${\partial f}/ {\partial x^\mu}$, donde $x^\mu$ representa $\R^m$.
El punto es que esta notación es un atajo, y lo que realmente está pasando es
\begin{equation}
{{\partial f}\over{\p{} x^\mu}} \equiv {{\partial}\over{\p{} x^\mu}}
(\psi\circ f\circ\phi^{-1})(x^{\mu})\,.\label{2.7}
\end{equation}
Sería demasiado difícil de manejar (por no decir pedante) escribir los mapas de coordenadas explícitamente en todos los casos.
La notación abreviada del lado izquierdo será suficiente para la mayoría de los propósitos.

Habiendo construido este trabajo preliminar, ahora podemos proceder a introducir varios tipos de estructuras en variedades.
Comenzamos con vectores y espacios tangentes.
En nuestra discusión sobre la Relatividad Especial fuimos intencionalmente vagos acerca de la definición de vectores y su relación con el Espacio-Tiempo.
Un punto que se destacó fue la noción de un espacio tangente: el conjunto de todos los vectores en un único punto del Espacio-Tiempo.
La razón de este énfasis fue eliminar de sus mentes la idea de que un vector se extiende de un punto a otro de la variedad, sino que es solo un objeto asociado con un solo punto.
Lo que se pierde temporalmente al adoptar esta visión es una manera de darle sentido a afirmaciones como ``el vector apunta en la dirección $x$'' --- si el espacio tangente es meramente un espacio vectorial abstracto asociado con cada punto, es difícil para saber qué debería significar esto.
Ahora es la oportunidad para solucionar el problema.

Imaginemos que quisiéramos construir el espacio tangente en un punto $p$ en una variedad $M$, usando solo cosas que son intrínsecas a $M$ (sin incrustaciones en espacios de dimensiones superiores, etc.).
Una primera suposición podría ser utilizar nuestro conocimiento intuitivo de que hay objetos llamados ``vectores tangentes a curvas'' que pertenecen al espacio tangente.
Por lo tanto, podríamos considerar el conjunto de todas las curvas parametrizadas hasta $p$ --- es decir, el espacio de todos los mapas (no degenerados) $\gamma:\R\rightarrow M$ tales que $p$ esté en la imagen de $\gamma$.
La tentación es definir el espacio tangente simplemente como el espacio de todos los vectores tangentes a estas curvas en el punto $p$.
Pero esto es obviamente hacer trampa; se supone que el espacio tangente $T_p$ es el espacio de vectores en $p$, y antes de haberlo definido no tenemos una noción independiente de lo que se supone que significa ``el vector tangente a una curva''.
En algún sistema de coordenadas $x^\mu$, cualquier curva que pase por $p$ define un elemento de $\R^n$ especificado por los números reales $n$ $dx^\mu/d\lambda$ (donde $\lambda$ es el parámetro a lo largo de la curva), pero este mapa está claramente coordinado. -dependiente, que no es lo que queremos.

Sin embargo, estamos en el camino correcto, sólo tenemos que independizar las cosas de las coordenadas.
Con este fin, definimos ${\cal F}$ como el espacio de todas las funciones suaves en $M$ (es decir, $C^\infty$ asigna $f:M\rightarrow \R$).
Luego notamos que cada curva que pasa por $p$ define un operador en este espacio, la derivada direccional, que mapea $f\rightarrow df/d\lambda$ (en $p$).
Haremos la siguiente afirmación: {\it el espacio tangente $T_p$ se puede identificar con el espacio de operadores derivados direccionales a lo largo de curvas que pasan por $p$}.
Para establecer esta idea debemos demostrar dos cosas: primero, que el espacio de derivadas direccionales es un espacio vectorial, y segundo que es el espacio vectorial que queremos (tiene la misma dimensionalidad que $M$, da una idea natural de un vector que apunta en una determinada dirección, etc.).

La primera afirmación, que las derivadas direccionales forman un espacio vectorial, parece bastante sencilla.
Imagine dos operadores ${d\over{d\lambda}}$ y ${d\over{d\eta}}$ que representan derivados a lo largo de dos curvas que pasan por $p$.
No hay problema en sumarlos y escalar por números reales, para obtener un nuevo operador $a{d\over{d\lambda}}+ b{d\over{d\eta}}$.
Sin embargo, no es inmediatamente obvio que el espacio se cierre; {\it ie}, que el operador resultante es en sí mismo un operador derivativo.
Un buen operador derivativo es aquel que actúa linealmente sobre funciones y obedece la regla convencional de Leibniz (producto) sobre productos de funciones.
Nuestro nuevo operador es manifiestamente lineal, por lo que debemos verificar que obedece la regla de Leibniz.
Tenemos
\begin{align}
\left(a{d\over{d\lambda}}+ b{d\over{d\eta}}\right)(fg)
&= af{{dg}\over{d\lambda}} + ag{{df}\over{d\lambda}} +
bf{{dg}\over{d\eta}} + bg{{df}\over{d\eta}} \nonumber \\
&= \left(a{{df}\over{d\lambda}}+ b{{df}\over{d\eta}}\right)g +
\left(a{{dg}\over{d\lambda}}+ b{{dg}\over{d\eta}}\right)f\,.
\label{2.8}
\end{align}
Como esperábamos, se cumple la regla del producto y, por tanto, el conjunto de derivadas direccionales es un espacio vectorial.

¿Es el espacio vectorial el que nos gustaría identificar con el espacio tangente? La manera más fácil de convencerse es encontrar una base para el espacio.
Considere nuevamente un gráfico de coordenadas con coordenadas $x^\mu$.
Entonces hay un conjunto obvio de derivadas direccionales $n$ en $p$, concretamente las derivadas parciales $\p\mu$ en $p$.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/two15.pdf}
\end{figure}

\noindent
Ahora vamos a afirmar que los operadores de derivada parcial $\{\p\mu\}$ en $p$ forman una base para el espacio tangente $T_p$.
(Se deduce inmediatamente que $T_p$ es $n$-dimensional, ya que ese es el número de vectores base).
Para ver esto, mostraremos que cualquier derivada direccional se puede descomponer en una suma de números reales multiplicada por derivadas parciales.
De hecho, esta es sólo la expresión familiar para los componentes de un vector tangente, pero es bueno verlo desde el enfoque de la gran maquinaria.
Considere una variedad $n$ $M$, un gráfico de coordenadas $\phi:M\rightarrow \R^n$, una curva $\gamma:\R\rightarrow M$ y una función $f:M\rightarrow\R$.
Esto lleva a la siguiente maraña de mapas:

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/two16.pdf}
\end{figure}

\noindent
Si $\lambda$ es el parámetro a lo largo de $\gamma$, queremos expandir el vector/operador ${{d}\over{d\lambda}}$ en términos de los parciales $\p\mu$.
Usando la regla de la cadena (2.2), tenemos
\begin{align}
{d\over{d\lambda}}f &=  {d\over{d\lambda}}(f\circ\gamma)\nonumber \\
&=  {d\over{d\lambda}}[(f\circ\phi^{-1})\circ(\phi\circ\gamma)]\nonumber \\
&=  {{d(\phi\circ\gamma)^\mu}\over{d\lambda}}
{{\partial(f\circ\phi^{-1})}\over{\partial x^\mu}}\nonumber \\
&=  {{dx^\mu}\over{d\lambda}}\p\mu f\,. \label{2.9}
\end{align}
La primera línea simplemente toma la expresión informal del lado izquierdo y la reescribe como una derivada honesta de la función $(f\circ\gamma):\R\rightarrow\R$.
La segunda línea proviene simplemente de la definición del mapa inverso $\phi^{-1}$ (y la asociatividad de la operación de composición).
La tercera línea es la regla de la cadena formal (2.2) y la última línea es un retorno a la notación informal del inicio.
Como la función $f$ era arbitraria, tenemos
\begin{equation}
{d\over{d\lambda}} = {{dx^\mu}\over{d\lambda}}\p\mu\,.\label{2.10}
\end{equation}
Por lo tanto, los parciales $\{\p\mu\}$ representan una buena base para el espacio vectorial de derivadas direccionales, que por lo tanto podemos identificar con seguridad con el espacio tangente.

Por supuesto, el vector representado por ${d\over{d\lambda}}$ es uno que ya conocemos; es el vector tangente a la curva con parámetro $\lambda$.
Por lo tanto, se puede considerar (2.10) como una reformulación de (1.24), donde afirmamos que los componentes del vector tangente eran simplemente $dx^\mu/d\lambda$.
La única diferencia es que estamos trabajando en una variedad arbitraria y hemos especificado que nuestros vectores base sean $\e\mu=\p\mu$.

Esta base particular ($\e\mu=\p\mu$) se conoce como {\bf base de coordenadas} para $T_p$; es la formalización de la noción de establecer los vectores base para que apunten a lo largo de los ejes de coordenadas.
No hay ninguna razón por la que estemos limitados a bases de coordenadas cuando consideramos vectores tangentes; A veces es más conveniente, por ejemplo, utilizar bases ortonormales de algún tipo.
Sin embargo, la base de coordenadas es muy sencilla y natural, y la utilizaremos casi exclusivamente a lo largo del curso.

Una de las ventajas del punto de vista bastante abstracto que hemos adoptado respecto de los vectores es que la ley de transformación es inmediata.
Dado que los vectores base son $\e\mu=\p\mu$, los vectores base en algún nuevo sistema de coordenadas $x^{\mu'}$ están dados por la regla de la cadena (2.3) como
\begin{equation}
\p{\mu'} = {{\partial x^\mu}\over{\partial x^{\mu'}}}\p\mu\,.
\label{2.11}
\end{equation}
Podemos obtener la ley de transformación para componentes vectoriales mediante la misma técnica utilizada en el espacio plano, exigiendo que el vector $V=V^\mu\p\mu$ no se modifique mediante un cambio de base.
Tenemos
\begin{align}
V^\mu\p\mu &=  V^{\mu'}\p{\mu'}\nonumber \\
&=  V^{\mu'}{{\partial x^\mu}\over{\partial x^{\mu'}}}\p\mu\ ,
\label{2.12}
\end{align}
y por tanto (dado que la matriz $\partial x^{\mu'}/\partial x^\mu$ es la inversa de la matriz $\partial x^{\mu}/\partial x^{\mu'}$),
\begin{equation}
V^{\mu'} = {{\partial x^{\mu'}}\over{\partial x^{\mu}}}V^\mu
\,.\label{2.13}
\end{equation}
Dado que los vectores base generalmente no se escriben explícitamente, la regla (2.13) para transformar componentes es lo que llamamos la ``ley de transformación de vectores''. Observamos que es compatible con la transformación de componentes vectoriales en la Relatividad Especial bajo transformaciones de Lorentz, $V^{\mu'} = \Lambda^{\mu'}{}_\mu V^\mu$, ya que una transformación de Lorentz es un tipo especial de transformación de coordenadas, con $x^{\mu'} = \Lambda^{\mu'}{}_\mu x^\mu$.
Pero (2.13) es mucho más general, ya que abarca el comportamiento de los vectores ante cambios arbitrarios de coordenadas (y por tanto de bases), no sólo transformaciones lineales.
Como siempre, estamos tratando de enfatizar una distinción ontológica algo sutil: los componentes tensoriales no cambian cuando cambiamos las coordenadas, cambian cuando cambiamos la base en el espacio tangente, pero hemos decidido usar las coordenadas para definir nuestra base. .
Por tanto un cambio de coordenadas induce un cambio de base:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/two17.pdf}
\end{figure}

Habiendo explorado el mundo de los vectores, continuamos volviendo sobre los pasos que dimos en el espacio plano y ahora consideramos vectores duales (formas únicas).
Una vez más el espacio cotangente $T^*_p$ es el conjunto de aplicaciones lineales $\omega:T_p\rightarrow\R$.
El ejemplo canónico de una forma única es el gradiente de una función $f$, denominada $ d\,f$.
Su acción sobre un vector ${d\over{d\lambda}}$ es exactamente la derivada direccional de la función:
\begin{equation}
 d\,f\left({d\over{d\lambda}}\right)={{df}\over{d\lambda}}\,.\label{2.14}
\end{equation}
Es tentador pensar: ``¿por qué la función $f$ en sí misma no debería considerarse la forma única y $df/d\lambda$ su acción?''. La cuestión es que una forma única, como un vector, existe sólo en el punto está definido y no depende de la información de otros puntos en $M$.
Si conoces una función en alguna vecindad de un punto, puedes tomar su derivada, pero no sólo sabiendo su valor en el punto; el gradiente, por otro lado, codifica precisamente la información necesaria para tomar la derivada direccional a lo largo de cualquier curva que pase por $p$, cumpliendo su papel de vector dual.

Así como las derivadas parciales a lo largo de los ejes de coordenadas proporcionan una base natural para el espacio tangente, los gradientes de las funciones de coordenadas $x^\mu$ proporcionan una base natural para el espacio cotangente.
Recuerde que en el espacio plano construimos una base para $T^*_p$ exigiendo que $ \ztheta{\mu}(\e\nu)=\delta^\mu_\nu$.
Continuando con la misma filosofía en una variedad arbitraria, encontramos que (2.14) conduce a
\begin{equation}
 d\,x^\mu(\p\nu) = {{\partial x^\mu}\over{\partial x^\nu}}
=\delta^\mu_\nu\,.\label{2.15}
\end{equation}
Por lo tanto, los gradientes $\{ d\,x^\mu\}$ son un conjunto apropiado de formas unibase; una forma única arbitraria se expande en componentes como $\omega = \omega_\mu\, d\,x^\mu$.

Las propiedades de transformación de componentes y vectores duales de base se derivan de lo que ahora es el procedimiento habitual.
Obtenemos, para bases uno,
\begin{equation}
 d\,x^{\mu'} = {{\partial x^{\mu'}}\over{\partial x^{\mu}}}\, d\,x^\mu
\ ,\label{2.16}
\end{equation}
y para componentes,
\begin{equation}
\omega_{\mu'} = {{\partial x^{\mu}}\over{\partial x^{\mu'}}}\omega_\mu
\,.\label{2.17}
\end{equation}
Normalmente escribiremos los componentes $\omega_\mu$ cuando hablamos de un formulario único $\omega$.

La ley de transformación para tensores generales sigue este mismo patrón de reemplazar la matriz de transformación de Lorentz utilizada en el espacio plano con una matriz que representa transformaciones de coordenadas más generales.
Un tensor $(k,l)$ $T$ se puede expandir
\begin{equation}
T = T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l}
\p{\mu_1}\otimes\cdots\otimes\p{\mu_k}\otimes
 d\,x^{\nu_1}\otimes\cdots\otimes d\,x^{\nu_l}\ ,\label{2.18}
\end{equation}
y bajo una transformación de coordenadas los componentes cambian de acuerdo con
\begin{equation}
T^{\mu_1' \cdots \mu_k'}{}_{\nu_1'\cdots\nu_l'} =
{{\partial x^{\mu_1'}}\over{\partial x^{\mu_1}}}\cdots
{{\partial x^{\mu_k'}}\over{\partial x^{\mu_k}}}
{{\partial x^{\nu_1}}\over{\partial x^{\nu_1'}}}\cdots
{{\partial x^{\nu_l}}\over{\partial x^{\nu_l'}}}
T^{\mu_1 \cdots \mu_k}{}_{\nu_1\cdots\nu_l} \,.\label{2.19}
\end{equation}
Esta ley de transformación del tensor es sencilla de recordar, ya que realmente no hay nada más que pueda ser, dada la ubicación de los índices.
Sin embargo, a menudo es más fácil transformar un tensor tomando la identidad de los vectores base y las formas uniformes como derivadas parciales y gradientes al pie de la letra, y simplemente sustituyéndolos en la transformación de coordenadas.
Como ejemplo, considere un tensor $(0, 2)$ simétrico $S$ en una variedad bidimensional, cuyos componentes en un sistema de coordenadas $(x^1=x, x^2=y)$ están dados por
\begin{equation}
S_\mn = \left(\mqty{x&0 \\  0&1 \\ }\right)\,.\label{2.20}
\end{equation}
Esto se puede escribir de manera equivalente como
\begin{align}
S &=  S_\mn ( d\,x^\mu \otimes  d\,x^\nu)\nonumber \\
&=  x( d\,x)^2 + ( d\,y)^2\ , \label{2.21}
\end{align}
donde en la última línea los símbolos del producto tensorial se suprimen por motivos de brevedad.
Ahora considere nuevas coordenadas.
\begin{align}
x' &=  x^{1/3}\nonumber \\ y' &=  e^{x+y}\,. \label{2.22}
\end{align}
Esto lleva directamente a
\begin{align}
x &=  (x')^3\nonumber \\ y &=  \ln(y') - (x')^3\nonumber \\
 d\,x &=  3(x')^2 \, d\,x'\nonumber \\  d\,y &=  {1\over{y'}}\, d\,y'
- 3(x')^2\, d\,x'\,.
\label{2.23}
\end{align}
Sólo necesitamos insertar estas expresiones directamente en (2.21) para obtener (recordando que los productos tensoriales no conmutan, por lo que $ d\,x'\, d\,y' \neq  d\,y' \, d\,x'$):
\begin{equation}
S= 9(x')^4[1+(x')^3]( d\,x')^2 -3{{(x')^2}\over{y'}}( d\,x' \, d\,y'
+ d\,y' \, d\,x') + {1\over{(y')^2}}( d\,y')^2\ ,\label{2.24}
\end{equation}
o
\begin{equation}
S_{\mu'\nu'} = \left(\mqty{9(x')^4[1+(x')^3]&-3{{(x')^2}\over{y'}} \\
-3{{(x')^2}\over{y'}}&{1\over{(y')^2}} \\ }\right)\,.\label{2.25}
\end{equation}
Observe que todavía es simétrico.
No utilizamos la ley de transformación (2.19) directamente, pero hacerlo habría dado el mismo resultado, como puedes comprobar.

En su mayor parte, las diversas operaciones tensoriales que definimos en el espacio plano no se modifican en un entorno más general: contracción, simetrización, etc.
Hay tres excepciones importantes: las derivadas parciales, la métrica y el tensor de Levi-Civita.
Veamos primero la derivada parcial.

Lo desafortunado es que la derivada parcial de un tensor no es, en general, un nuevo tensor.
El gradiente, que es la derivada parcial de un escalar, es un tensor $(0,1)$ honesto, como hemos visto.
Pero la derivada parcial de tensores de rango superior no es tensorial, como podemos ver al considerar la derivada parcial de una forma única, $\p\mu W_\nu$, y cambiar a un nuevo sistema de coordenadas:
\begin{align}
{{\partial}\over{\partial x^{\mu'}}}W_{\nu'} &=
{{\partial x^{\mu}}\over{\partial x^{\mu'}}}
{{\partial}\over{\partial x^{\mu}}}\left({{\partial x^{\nu}}
\over{\partial x^{\nu'}}}W_\nu\right)\nonumber \\
&=  {{\partial x^{\mu}}\over{\partial x^{\mu'}}}
{{\partial x^{\nu}}\over{\partial x^{\nu'}}}
\left({{\partial}\over{\partial x^{\mu}}}W_\nu\right)
+ W_\nu{{\partial x^{\mu}}\over{\partial x^{\mu'}}}
{{\partial}\over{\partial x^\mu}}
{{\partial x^{\nu}}\over{\partial x^{\nu'}}}\,.
\label{2.26}
\end{align}
El segundo término de la última línea no debería estar allí si $\p\mu W_\nu$ se transformara como un tensor $(0,2)$.
Como puede ver, surge porque la derivada de la matriz de transformación no desaparece, como sucedió con las transformaciones de Lorentz en el espacio plano.

Por otro lado, el operador derivado exterior $ d\,$ forma un tensor antisimétrico $(0,p+1)$ cuando actúa sobre una forma $p$.
Para $p=1$ podemos ver esto en (2.26); el término no tensorial ofensivo se puede escribir
\begin{equation}
W_\nu{{\partial x^{\mu}}\over{\partial x^{\mu'}}}
{{\partial}\over{\partial x^\mu}}
{{\partial x^{\nu}}\over{\partial x^{\nu'}}} =
W_\nu{{\partial^2 x^{\nu}}\over{\partial x^{\mu'}
\partial x^{\nu'}}}\,.\label{2.27}
\end{equation}
Esta expresión es simétrica en $\mu'$ y $\nu'$, ya que las derivadas parciales conmutan.
Pero la derivada exterior se define como la derivada parcial antisimetrizada, por lo que este término desaparece (la parte antisimétrica de una expresión simétrica es cero).
Entonces nos quedamos con la ley de transformación del tensor correcta; La extensión a $p$ arbitraria es sencilla.
Entonces la derivada exterior es un operador tensorial legítimo; sin embargo, no es un sustituto adecuado de la derivada parcial, ya que sólo se define en formas.
En la siguiente sección definiremos una derivada covariante, que puede considerarse como la extensión de la derivada parcial a variedades arbitrarias.

El tensor métrico es un objeto tan importante en el espacio curvo que se le asigna un nuevo símbolo, $g_\mn$ (mientras que $\eta_\mn$ está reservado específicamente para la métrica de Minkowski).
Existen pocas restricciones sobre los componentes de $g_\mn$, aparte de que sea un tensor simétrico $(0,2)$.
Por lo general, se considera no degenerado, lo que significa que el determinante $g=|g_\mn|$ no desaparece.
Esto nos permite definir la métrica inversa $g^\mn$ mediante
\begin{equation}
g^\mn g_{\nu\sigma} = \delta^\mu_\sigma\,.\label{2.28}
\end{equation}
La simetría de $g_\mn$ implica que $g^\mn$ también es simétrica.
Al igual que en la Relatividad Especial, la métrica y su inversa se pueden utilizar para subir y bajar índices en tensores.

Tardaremos varias semanas en apreciar plenamente el papel de la métrica en todo su esplendor, pero a modo de inspiración podemos enumerar los diversos usos que se darán a $g_\mn$: (1) la métrica proporciona una noción de ``pasado'' y ``futuro''; (2) la métrica permite calcular la longitud del camino y el tiempo propio; (3) la métrica determina la ``distancia más corta'' entre dos puntos (y, por tanto, el movimiento de las partículas de prueba); (4) la métrica sustituye al campo gravitatorio newtoniano $\phi$; (5) la métrica proporciona una noción de marcos localmente inerciales y, por tanto, un sentido de ``no rotación (6) la métrica determina la causalidad, al definir la velocidad de la luz por encima de la cual no puede viajar ninguna señal; (7) la métrica sustituye al tradicional producto punto tridimensional euclidiano de la mecánica newtoniana; y así sucesivamente.
Obviamente estas ideas no son todas completamente independientes, pero tenemos una idea de la importancia de este tensor.

En nuestra discusión sobre las longitudes de los caminos en la Relatividad Especial, introdujimos (un poco a mano) el elemento de línea como $ds^2 = \eta_\mn dx^\mu dx^\nu$, que se usó para obtener la longitud de un camino.
Por supuesto, ahora que sabemos que $ d\,x^\mu$ es realmente un vector dual de base, resulta natural usar los términos ``métrica'' y ``elemento de línea'' indistintamente, y escribir
\begin{equation}
ds^2 = g_\mn \, d\,x^\mu \, d\,x^\nu\,.\label{2.29}
\end{equation}
(Para ser perfectamente coherente deberíamos escribir esto como ``$g$'', y algunas veces lo haremos, pero la mayoría de las veces se usa $g$ para el determinante $|g_\mn|$.)
Por ejemplo, sabemos que el elemento de línea euclidiana en un espacio tridimensional con coordenadas cartesianas es
\begin{equation}
ds^2 = ( d\,x)^2 + ( d\,y)^2 + ( d\,z)^2\,.\label{2.30}
\end{equation}
Ahora podemos cambiar a cualquier sistema de coordenadas que elijamos.
Por ejemplo, en coordenadas esféricas tenemos
\begin{align}
x &=  r\sin\theta \cos\phi\nonumber \\
y &=  r\sin\theta \sin\phi\nonumber \\ z &=  r\cos\theta\ , \label{2.31}
\end{align}
que conduce directamente a
\begin{equation}
ds^2 =  d\,r^2 + r^2 \, d\,\theta^2 + r^2\sin^2\theta\, d\,\phi^2\,.\label{2.32}
\end{equation}
Obviamente, los componentes de la métrica se ven diferentes a los de las coordenadas cartesianas, pero todas las propiedades del espacio permanecen inalteradas.

Quizás esta sea una excelente oportunidad para señalar que la mayoría de las referencias no son lo suficientemente exigentes como para distinguir entre ``$dx$'', la noción informal de un desplazamiento infinitesimal, y ``$ d\,x$'', la noción rigurosa de una base uno. -forma dada por el gradiente de una función de coordenadas.
De hecho, nuestra notación ``$ds^2$'' no se refiere a la derivada exterior de nada, ni al cuadrado de nada; es simplemente una abreviatura convencional para el tensor métrico.
Por otro lado, ``$( d\,x)^2$'' se refiere específicamente al tensor $(0,2)$ $ d\,x\otimes d\,x$.

Un buen ejemplo de un espacio con curvatura es el de dos esferas, que puede considerarse como el lugar geométrico de los puntos en $\R^3$ a una distancia 1 del origen.
La métrica en el sistema de coordenadas $(\theta, \phi)$ proviene de configurar $r=1$ y $ d\,r=0$ en (2.32):
\begin{equation}
ds^2 =  d\,\theta^2 + \sin^2\theta\, d\,\phi^2\,.\label{2.33}
\end{equation}
Esto es completamente consistente con la interpretación de $ds$ como una longitud infinitesimal, como se ilustra en la figura.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/two18.pdf}
\end{figure}

Como veremos, el tensor métrico contiene toda la información que necesitamos para describir la curvatura de la variedad (al menos en geometría de Riemann; de hecho, indicaremos enfoques algo más generales).
En el espacio de Minkowski podemos elegir coordenadas en las que las componentes de la métrica sean constantes; pero debe quedar claro que la existencia de curvatura es más sutil que hacer que la métrica dependa de las coordenadas, ya que en el ejemplo anterior mostramos cómo la métrica en el espacio euclidiano plano en coordenadas esféricas es función de $r$ y $\theta$ .
Más adelante veremos que la constancia de las componentes métricas es suficiente para que un espacio sea plano y, de hecho, siempre existe un sistema de coordenadas en cualquier espacio plano en el que la métrica sea constante.
Pero es posible que no queramos trabajar en un sistema de coordenadas así y que ni siquiera sepamos cómo encontrarlo; por lo tanto, querremos una caracterización más precisa de la curvatura, que se introducirá más adelante.

Se obtiene una caracterización útil de la métrica poniendo $g_\mn$ en su {\bf forma canónica}.
De esta forma, los componentes métricos se convierten en
\begin{equation}
g_\mn = {\rm ~diag~}(-1,-1,\ldots,-1,+1,+1,\ldots,+1,
0,0,\ldots,0)\ ,\label{2.34}
\end{equation}
donde ``diag'' significa una matriz diagonal con los elementos dados.
Si $n$ es la dimensión de la variedad, $s$ es el número de $+1$ en la forma canónica y $t$ es el número de $-1$, entonces $s-t$ es la \textbf{firma} de la métrica (la diferencia en el número de signos menos y más), y $s+t$ es el {\bf rango} de la métrica (el número de valores propios distintos de cero).
Si una métrica es continua, el rango y la firma del campo tensorial de la métrica son los mismos en todos los puntos, y si la métrica no es degenerada, el rango es igual a la dimensión $n$.
Siempre trataremos con métricas continuas y no degeneradas.
Si todos los signos son positivos ($t=0$) la métrica se llama {\bf euclidiana} o {\bf riemanniana} (o simplemente ``definida positiva''), mientras que si hay un solo signo negativo ($t=1$) se llama {\bf Lorentziano} o {\bf pseudo-riemanniano}, y cualquier métrica con algunos $+1$ y algunos $-1$ se llama ``indefinida''. ' a veces significa que el espacio es plano y otras no, pero siempre significa que la forma canónica es estrictamente positiva; la terminología es desafortunada pero estándar.)
Los espaciotiempos de interés en la Relatividad General tienen métricas lorentzianas.

Todavía no hemos demostrado que siempre es posible convertir la métrica en forma canónica.
De hecho, siempre es posible hacerlo en algún punto $p\in M$, pero en general sólo será posible en ese único punto, no en ninguna vecindad de $p$.
En realidad, podemos hacerlo un poco mejor que esto; resulta que en cualquier punto $p$ existe un sistema de coordenadas en el que $g_\mn$ toma su forma canónica y las primeras derivadas $\p\sigma g_\mn$ desaparecen todas (mientras que no se puede hacer que las segundas derivadas $\p\rho \p\sigma g_\mn$ desaparezcan por completo).
Dichas coordenadas se conocen como {\bf coordenadas normales de Riemann} y los vectores base asociados constituyen un {\bf marco de Lorentz local}.
Observe que en las coordenadas normales de Riemann (o RNC) la métrica en $p$ se parece a la del espacio plano ``de primer orden''. Esta es la noción rigurosa de la idea de que "regiones del Espacio-Tiempo suficientemente pequeñas parecen planas ( Minkowski) espacio.'' (Además, no hay dificultad en construir simultáneamente conjuntos de vectores base en cada punto de $M$ de manera que la métrica tome su forma canónica; el problema es que en general esto no será un {\it coordinar} base, y no habrá manera de convertirlo en uno.)

No consideraremos la prueba detallada de esta afirmación; se puede encontrar en Schutz, págs. 158-160, donde recibe el nombre de ``teorema de planitud local''. (También llama a los marcos locales de Lorentz ``marcos de referencia momentáneamente comoving'', o MCRF).
Sin embargo, es útil ver un esquema de la demostración para el caso específico de una métrica lorentziana en cuatro dimensiones.
La idea es considerar la ley de transformación de la métrica.
\begin{equation}
g_{\mu'\nu'} = {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}} g_\mn\ ,\label{2.35}
\end{equation}
y expanda ambos lados en la serie de Taylor en las coordenadas buscadas $x^{\mu'}$.
La expansión de las antiguas coordenadas $x^\mu$ parece
\begin{equation}
x^\mu = \left({{\partial x^\mu}\over{\partial x^{\mu'}}}\right)_p
x^{\mu'} + {1\over 2} \left({{\partial^2 x^\mu}\over
{\partial x^{\mu_1'}\partial x^{\mu_2'}}}\right)_p
x^{\mu_1'}x^{\mu_2'} + {1\over 6} \left({{\partial^3 x^\mu}\over
{\partial x^{\mu_1'}\partial x^{\mu_2'}\partial x^{\mu_3'}}}\right)_p
x^{\mu_1'}x^{\mu_2'}x^{\mu_3'} +\cdots\ ,\label{2.36}
\end{equation}
y las demás ampliaciones procedieron en la misma línea.
(Para simplificar, hemos configurado $x^\mu(p)=x^{\mu'}(p)=0$.)
Entonces, usando alguna notación extremadamente esquemática, la expansión de (2.35) a segundo orden es
\begin{align}
\lefteqn{\left(g'\right)_p + \left(\partial' g'\right)_p x' +
\left(\partial'\partial' g'\right)_p x' x'}\nonumber \\
&=
\left({{\partial x}\over{\partial x'}}{{\partial x}\over{\partial x'}}
g\right)_p + \left({{\partial x}\over{\partial x'}}
{{\partial^2 x}\over{\partial x'\partial x'}}g +
{{\partial x}\over{\partial x'}}{{\partial x}\over{\partial x'}}
\partial' g\right)_p x' \nonumber \\
&\quad + \left({{\partial x}\over{\partial x'}}
{{\partial^3 x}\over{\partial x'\partial x'\partial x'}}g +
{{\partial^2 x}\over{\partial x'\partial x'}}
{{\partial^2 x}\over{\partial x'\partial x'}}g +
{{\partial x}\over{\partial x'}}
{{\partial^2 x}\over{\partial x'\partial x'}}\partial' g +
{{\partial x}\over{\partial x'}}{{\partial x}\over{\partial x'}}
\partial'\partial' g\right)_p x' x'\,. \label{2.37}
\end{align}
Podemos establecer términos de igual orden en $x'$ en cada lado iguales entre sí.
Por lo tanto, los componentes $g_{\mu'\nu'}(p)$, 10 números en total (para describir un tensor simétrico de dos índices), están determinados por la matriz $(\partial x^\mu/\partial x^{\mu'})_p$.
Esta es una matriz $4\times 4$ sin restricciones; por lo tanto, somos libres de elegir 16 números.
Claramente, esta es suficiente libertad para poner los 10 números de $g_{\mu'\nu'}(p)$ en forma canónica, al menos en lo que respecta a tener suficientes grados de libertad.
(De hecho, existen algunas limitaciones: si sigue el procedimiento con atención, encontrará, por ejemplo, que no puede cambiar la firma y el rango).
Los seis grados de libertad restantes pueden interpretarse como exactamente los seis parámetros del grupo de Lorentz; sabemos que estos dejan la forma canónica sin cambios.
De primer orden tenemos las derivadas $\p{\sigma'}g_{\mu'\nu'}(p)$, cuatro derivadas de diez componentes para un total de 40 números.
Pero mirando el lado derecho de (2.37) vemos que ahora tenemos la libertad adicional de elegir $(\partial^2 x^\mu/\partial x^{\mu'_1} \partial x^{\mu_2'})_p$.
En este conjunto de números hay 10 opciones independientes de los índices $\mu_1'$ y $\mu_2'$ (es simétrico, ya que las derivadas parciales conmutan) y cuatro opciones de $\mu$, para un total de 40 grados de libertad.
Ésta es precisamente la cantidad de opciones que necesitamos para determinar todas las primeras derivadas de la métrica, que por lo tanto podemos establecer en cero.
Sin embargo, en el segundo orden nos ocupamos de $\p{\rho'}\p{\sigma'}g_{\mu'\nu'}(p)$; esto es simétrico en $\rho'$ y $\sigma'$ así como en $\mu'$ y $\nu'$, para un total de $10\times 10=100$ números.
Nuestra capacidad para tomar decisiones adicionales está contenida en $(\partial^3 x^\mu/\partial x^{\mu'_1}\partial x^{\mu'_2} \partial x^{\mu_3'})_p$.
Esto es simétrico en los tres índices inferiores, lo que da 20 posibilidades, multiplicado por cuatro para el índice superior nos da 80 grados de libertad, 20 menos de los que necesitamos para establecer las segundas derivadas de la métrica en cero.
De hecho, no podemos hacer que las segundas derivadas desaparezcan; Por lo tanto, la desviación de la planitud debe medirse mediante los 20 grados de libertad independientes de las coordenadas que representan las segundas derivadas del campo tensorial métrico.
Más adelante veremos cómo se produce esto, cuando caractericemos la curvatura utilizando el tensor de Riemann, que resultará tener 20 componentes independientes.

El último cambio que tenemos que hacer en nuestro conocimiento del tensor ahora que hemos abandonado el supuesto de espacio plano tiene que ver con el tensor de Levi-Civita, $\epsilon_{\mu_1\mu_2\cdots\mu_n}$.
Recuerde que la versión de espacio plano de este objeto, que ahora denotaremos como $\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n}$, se definió como
\begin{equation}
\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n}=\left\{\mqty{
+1 {\rm ~if~}\mu_1\mu_2\cdots\mu_n
{\rm ~is~an~even~permutation~of~}01\cdots (n-1)\ ,\hfill \\
-1 {\rm ~if~}\mu_1\mu_2\cdots\mu_n
{\rm ~is~an~odd~permutation~of~}01\cdots (n-1)\ ,\hfill \\
0{\rm ~otherwise}\,.\hfill \\ }\right.
\label{2.38}
\end{equation}
Ahora definiremos el {\bf símbolo de Levi-Civita} como exactamente este $\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n}$ --- es decir, un objeto con índices $n$ que tiene los componentes especificados anteriormente {\it en cualquier sistema de coordenadas}.
Esto se llama ``símbolo'', por supuesto, porque no es un tensor; está definido para no cambiar bajo transformaciones de coordenadas.
Podemos relacionar su comportamiento con el de un tensor ordinario observando primero que, dada alguna matriz $n\times n$ $M^\mu{}_{\mu'}$, el determinante $|M|$ obedece
\begin{equation}
\tilde\epsilon_{\mu_1'\mu_2'\cdots\mu_n'} |M| =
\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n} M^{\mu_1}{}_{\mu_1'}
M^{\mu_2}{}_{\mu_2'}\cdots M^{\mu_n}{}_{\mu_n'}\,.\label{2.39}
\end{equation}
Este es simplemente un hecho cierto sobre el determinante que puede encontrar en un libro de álgebra lineal suficientemente ilustrado.
Si sigue esto, configurando $M^\mu{}_{\mu'}=\partial x^\mu/\partial x^{\mu'}$, tenemos
\begin{equation}
\tilde\epsilon_{\mu_1'\mu_2'\cdots\mu_n'} =
\left|{{\partial x^{\mu'}}\over{\partial x^\mu}}\right|
\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n}
{{\partial x^{\mu_1}}\over{\partial x^{\mu_1'}}}
{{\partial x^{\mu_2}}\over{\partial x^{\mu_2'}}}\cdots
{{\partial x^{\mu_n}}\over{\partial x^{\mu_n'}}}\,.\label{2.40}
\end{equation}
Esto está cerca de la ley de transformación del tensor, excepto por el determinante anterior.
Los objetos que se transforman de esta manera se conocen como {\bf densidades tensoriales}.
Otro ejemplo lo da el determinante de la métrica, $g=|g_\mn|$.
Es fácil comprobar (tomando el determinante de ambos lados de (2.35)) que bajo una transformación de coordenadas obtenemos
\begin{equation}
g(x^{\mu'}) = \left|{{\partial x^{\mu'}}\over{\partial x^\mu}}
\right|^{-2} g(x^\mu)\,.\label{2.41}
\end{equation}
Por tanto, $g$ tampoco es un tensor; se transforma de forma similar al símbolo de Levi-Civita, excepto que el jacobiano se eleva a la potencia $-2$.
La potencia a la que se eleva el jacobiano se conoce como {\bf peso} de la densidad tensorial; el símbolo de Levi-Civita es una densidad de peso $1$, mientras que $g$ es una densidad (escalar) de peso $-2$.

Sin embargo, no nos gustan las densidades tensoriales, nos gustan los tensores.
Hay una forma sencilla de convertir una densidad en un tensor honesto: multiplicar por $|g|^{w/2}$, donde $w$ es el peso de la densidad (los signos de valor absoluto están ahí porque $g<0$ para las métricas de Lorentz).
El resultado se transformará según la ley de transformación del tensor.
Por tanto, por ejemplo, podemos definir el tensor de Levi-Civita como
\begin{equation}
\epsilon_{\mu_1\mu_2\cdots\mu_n}= \sqrt{|g|}\,
\tilde\epsilon_{\mu_1\mu_2\cdots\mu_n}\,.\label{2.42}
\end{equation}
Es este tensor el que se utiliza en la definición del dual de Hodge, (1.87), que por lo demás no cambia cuando se generaliza a variedades arbitrarias.
Como se trata de un tensor real, podemos elevar índices, etc.
A veces la gente define una versión del símbolo de Levi-Civita con índices superiores, $\tilde\epsilon^{\mu_1\mu_2\cdots\mu_n}$, cuyos componentes son numéricamente iguales al símbolo con índices inferiores.
Esto resulta ser una densidad de peso $-1$, y está relacionada con el tensor con índices superiores por
\begin{equation}
\epsilon^{\mu_1\mu_2\cdots\mu_n} = {\rm ~sgn}(g){1\over{\sqrt{|g|}}}
\,\tilde\epsilon^{\mu_1\mu_2\cdots\mu_n}\,.\label{2.43}
\end{equation}

Aparte, debemos ser claros y admitir que, incluso con el factor de $\sqrt{|g|}$, el tensor de Levi-Civita no es en cierto sentido un tensor verdadero, porque en algunas variedades no se puede definir globalmente.
Aquellas en las que se puede definir se denominan {\bf orientables}, y en este curso nos ocuparemos exclusivamente de variedades orientables.
Un ejemplo de variedad no orientable es la tira de Möbius; consulte {\sl Métodos geométricos en física matemática} de Schutz (o un texto similar) para una discusión.

Una última aparición de las densidades tensoriales es la integración en variedades.
No haremos justicia a este tema, pero es necesaria al menos una mirada casual.
Probablemente haya estado expuesto al hecho de que en el cálculo ordinario en $\R^n$ el elemento de volumen $d^nx$ toma un factor del jacobiano bajo el cambio de coordenadas:
\begin{equation}
d^nx' = \left|{{\partial x^{\mu'}}\over{\partial x^\mu}}
\right| d^nx\,.\label{2.44}
\end{equation}
En realidad, existe una hermosa explicación de esta fórmula desde el punto de vista de las formas diferenciales, que surge del siguiente hecho: {\it en una variedad $n$-dimensional, el integrando se entiende propiamente como una forma $n$} .
El ingenuo elemento de volumen $d^nx$ es en sí mismo una densidad más que una forma $n$, pero no hay dificultad en usarlo para construir una forma $n$ real.
Para ver cómo funciona esto, debemos hacer la identificación.
\begin{equation}
d^nx \leftrightarrow  d\,x^0\wedge \cdots \wedge\, d\,x^{n-1}
\,.\label{2.45}
\end{equation}
La expresión del lado derecho puede ser engañosa, porque parece un tensor (una forma $n$, en realidad) pero en realidad es una densidad.
Ciertamente, si tenemos dos funciones $f$ y $g$ en $M$, entonces $ d\,f$ y $ d\,g$ son de una forma, y $ d\,f\wedge \, d\,g$ es de dos formas.
Pero nos gustaría interpretar el lado derecho de (2.45) como un objeto dependiente de las coordenadas que, en el sistema de coordenadas $x^\mu$, actúa como $ d\,x^0\wedge \cdots \wedge\, d\,x^{n-1}$.
Esto suena complicado, pero en realidad es sólo una ambigüedad de notación y en la práctica usaremos la notación abreviada ``$d^nx$''.

Para justificar esta canción y baile, veamos cómo (2.45) cambia bajo transformaciones de coordenadas.
Primero observe que la definición del producto cuña nos permite escribir
\begin{equation}
 d\,x^0\wedge \cdots \wedge\, d\,x^{n-1} = {1\over {n!}}
\tilde\epsilon_{\mu_1\cdots\mu_n}
\, d\,x^{\mu_1}\wedge \cdots \wedge\, d\,x^{\mu_n}\ ,\label{2.46}
\end{equation}
ya que tanto el producto cuña como el símbolo Levi-Civita son completamente antisimétricos.
Bajo una transformación de coordenadas, $\tilde\epsilon_{\mu_1\cdots\mu_n}$ permanece igual mientras que las formas unitarias cambian según (2.16), lo que lleva a
\begin{align}
\tilde\epsilon_{\mu_1\cdots\mu_n}
\, d\,x^{\mu_1}\wedge \cdots \wedge \, d\,x^{\mu_n}
&=  \tilde\epsilon_{\mu_1\cdots\mu_n}
{{\partial x^{\mu_1}}\over{\partial x^{\mu_1'}}}\cdots
{{\partial x^{\mu_n}}\over{\partial x^{\mu_n'}}}
\, d\,x^{\mu_1'}\wedge \cdots \wedge \, d\,x^{\mu_n'}\nonumber \\
&=  \left|{{\partial x^{\mu}}\over{\partial x^{\mu'}}}\right|
\tilde\epsilon_{\mu_1'\cdots\mu_n'}
\, d\,x^{\mu_1'}\wedge \cdots \wedge \, d\,x^{\mu_n'}\,. \label{2.47}
\end{align}
Multiplicando por el jacobiano en ambos lados se recupera (2.44).

Está claro que el sencillo elemento de volumen $d^nx$ se transforma como una densidad, no como un tensor, pero es sencillo construir un elemento de volumen invariante multiplicando por $\sqrt{|g|}$:
\begin{equation}
\sqrt{|g'|}\, d\,x^{0'}\wedge \cdots \wedge\, d\,x^{(n-1)'}
= \sqrt{|g|}\, d\,x^0\wedge \cdots \wedge\, d\,x^{n-1}\ ,\label{2.48}
\end{equation}
que por supuesto es solo $(n!)^{-1}\epsilon_{\mu_1\cdots\mu_n} \, d\,x^{\mu_1}\wedge \cdots \wedge \, d\,x^{\mu_n}$.
En aras de la simplicidad, normalmente escribiremos el elemento de volumen como $\sqrt{|g|}\,d^nx$, en lugar de como el producto de cuña explícito $\sqrt{|g|}\, d\,x^0\wedge \cdots \wedge\, d\,x^{n-1}$; bastará con tener en cuenta que se supone que es un formulario $n$.

Como último comentario para finalizar esta sección, consideremos uno de los teoremas más elegantes y poderosos de la geometría diferencial: el teorema de Stokes.
Este teorema es la generalización del teorema fundamental del cálculo, $\int^a_b dx = a-b$.
Imagine que tenemos una variedad $n$ $M$ con límite $\partial M$ y una forma $(n-1)$ $\omega$ en $M$.
(No hemos discutido variedades con límites, pero la idea es obvia; $M$ podría ser, por ejemplo, el interior de una superficie cerrada de $(n-1)$-dimensional $\partial M$.)
Entonces, $ d\,\omega$ es un formulario $n$, que se puede integrar a través de $M$, mientras que $\omega$ se puede integrar a través de $\partial M$.
El teorema de Stokes es entonces
\begin{equation}
\int_M  d\,\omega = \int_{\partial M}\omega\,.\label{2.49}
\end{equation}
Puede convencerse de que entre los diferentes casos especiales de este teorema se incluyen no sólo el teorema fundamental del cálculo, sino también los teoremas de Green, Gauss y Stokes, conocidos por el cálculo vectorial en tres dimensiones.





\chapter{Curvatura}
%\addcontentsline{toc}{chapter}{Curvatura}


En nuestra discusión sobre las variedades, quedó claro que había varias nociones de las que podíamos hablar tan pronto como se definiera la variedad; podríamos definir funciones, tomar sus derivadas, considerar caminos parametrizados, configurar tensores, etc.
Otros conceptos, como el volumen de una región o la longitud de un camino, requerían alguna pieza adicional de estructura, concretamente la introducción de una métrica.
Sería natural pensar que la noción de ``curvatura'', que ya hemos utilizado informalmente, es algo que depende de la métrica.
En realidad, esto resulta no ser del todo cierto, o al menos incompleto.
De hecho, hay una estructura adicional que necesitamos introducir: una ``conexión'', que se caracteriza por la curvatura.
Mostraremos cómo la existencia de una métrica implica una determinada conexión, cuya curvatura puede considerarse como la de la métrica.

La conexión se vuelve necesaria cuando intentamos abordar el problema de que la derivada parcial no es un buen operador tensorial.
Lo que nos gustaría es una derivada covariante; es decir, un operador que se reduce a la derivada parcial en un espacio plano con coordenadas cartesianas, pero se transforma como un tensor en una variedad arbitraria.
Es convencional dedicar una cierta cantidad de tiempo a motivar la introducción de una derivada covariante, pero en realidad la necesidad es obvia; ecuaciones como $\p\mu T^{\mn}=0$ tendrán que generalizarse de alguna manera al espacio curvo.
Entonces, acordemos que sería bueno tener una derivada covariante y comencemos a configurarla.

\section{La Derivada Covariante}

En un espacio plano en coordenadas cartesianas, el operador de derivada parcial $\p\mu$ es un mapa de $(k,l)$ campos tensoriales a $(k,l+1)$ campos tensoriales, que actúa linealmente sobre sus argumentos y obedece la regla de Leibniz sobre productos tensoriales.
Todo esto sigue siendo cierto en la situación más general que nos gustaría considerar ahora, pero el mapa proporcionado por la derivada parcial depende del sistema de coordenadas utilizado.
Por lo tanto, nos gustaría definir un operador {\bf derivada covariante} $\nabla$ para realizar las funciones de la derivada parcial, pero de forma independiente de las coordenadas.
Por lo tanto, requerimos que $\nabla$ sea un mapa de $(k,l)$ campos tensoriales a $(k,l+1)$ campos tensoriales que tenga estas dos propiedades:

\begin{enumerate}
\item Linealidad: $\;\nabla(T+S) = \nabla T + \nabla S$;
\item Regla de Leibniz (producto): $\;\nabla(T\otimes S) = (\nabla T)\otimes S + T\otimes (\nabla S)$ .
\end{enumerate}

Si $\nabla$ va a obedecer la regla de Leibniz, siempre se puede escribir como la derivada parcial más alguna transformación lineal.
Es decir, para tomar la derivada covariante primero tomamos la derivada parcial y luego aplicamos una corrección para hacer que el resultado sea covariante.
(No vamos a probar esta afirmación que parece razonable, pero Wald entra en detalles si está interesado).
Consideremos lo que esto significa para la derivada covariante de un vector $V^\nu$.

\begin{CajaColor}
    Significa que, para cada dirección $\mu$, la derivada covariante $\nabla_\mu$ estará dada por la derivada parcial $\partial_\mu$ más una corrección especificada por una matriz $(\Gamma_\mu)^\rho{}_\sigma$ (una matriz $n\times n$, donde $n$ es la dimensionalidad del colector, para cada $\mu$).
\end{CajaColor}

De hecho, los paréntesis generalmente se eliminan y escribimos estas matrices, conocidas como {\bf coeficientes de conexión}, con una colocación aleatoria del índice como $\Gamma^\rho_{\mu\sigma}$.
Por lo tanto tenemos
\begin{equation}
\nabla_\mu V^\nu = \partial_\mu V^\nu + \Gamma^\nu_{\mu\lambda}
V^\lambda\,.\label{3.1}
\end{equation}
Observe que en el segundo término el índice originalmente en $V$ se ha movido al $\Gamma$ y se suma un nuevo índice.
Si esta es la expresión de la derivada covariante de un vector en términos de la derivada parcial, deberíamos poder determinar las propiedades de transformación de $\Gamma^\nu_{\mu\lambda}$ exigiendo que el lado izquierdo sea un tensor $(1,1)$.
Es decir, queremos que la ley de transformación sea
\begin{equation}
\nabla_{\mu'}V^{\nu'} = {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}\nabla_{\mu}V^{\nu}
\,.\label{3.2}
\end{equation}
Miremos primero el lado izquierdo; podemos expandirlo usando (3.1) y luego transformar las partes que entendemos:
\begin{align}
\nabla_{\mu'}V^{\nu'} &= \partial_{\mu'} V^{\nu'}
+ \Gamma^{\nu'}_{\mu'\lambda'}V^{\lambda'} \notag \\
&=  {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}\partial_{\mu} V^{\nu}
+ {{\partial x^\mu}\over{\partial x^{\mu'}}} V^\nu
{{\partial}\over{\partial x^{\mu}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}
+ \Gamma^{\nu'}_{\mu'\lambda'}{{\partial x^{\lambda'}}\over
{\partial x^{\lambda}}}V^{\lambda}\,. \label{3.3}
\end{align}
El lado derecho también se puede ampliar:
\begin{equation}
{{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}\nabla_{\mu}V^{\nu}
= {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}\partial_{\mu}V^{\nu}
+ {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}
\Gamma^\nu_{\mu\lambda}V^{\lambda}\,.\label{3.4}
\end{equation}
Estas dos últimas expresiones deben equipararse; los primeros términos de cada uno son idénticos y por lo tanto se cancelan, por lo que tenemos
\begin{equation}
\Gamma^{\nu'}_{\mu'\lambda'}{{\partial x^{\lambda'}}\over
{\partial x^{\lambda}}}V^{\lambda} +
{{\partial x^\mu}\over{\partial x^{\mu'}}} V^\lambda
{{\partial}\over{\partial x^{\mu}}}
{{\partial x^{\nu'}}\over{\partial x^{\lambda}}}
= {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}}
\Gamma^\nu_{\mu\lambda}V^{\lambda}\ ,\label{3.5}
\end{equation}
donde hemos cambiado un índice ficticio de $\nu$ a $\lambda$.
Esta ecuación debe ser cierta para cualquier vector $V^\lambda$, por lo que podemos eliminarla en ambos lados.
Luego, los coeficientes de conexión en las coordenadas primadas se pueden aislar multiplicando por $\partial x^{\lambda}/\partial x^{\lambda'}$.
El resultado es
\begin{equation}
\Gamma^{\nu'}_{\mu'\lambda'} = {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\lambda}\over{\partial x^{\lambda'}}}
{{\partial x^{\nu'}}\over{\partial x^{\nu}}} \Gamma^\nu_{\mu\lambda}
- {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\lambda}\over{\partial x^{\lambda'}}}
{{\partial^2 x^{\nu'}}\over{\partial x^{\mu}\partial x^{\lambda}}}\,.
\label{3.6}
\end{equation}
Ésta no es, por supuesto, la ley de transformación tensorial; el segundo término de la derecha lo estropea.
Está bien, porque \textit{los coeficientes de conexión no son los componentes de un tensor}.
Están construidos intencionalmente para que no sean tensoriales, pero de tal manera que la combinación (3.1) se transforme como un tensor --- los términos adicionales en la transformación de los parciales y los $\Gamma$ se cancelan exactamente.
Esta es la razón por la que no somos tan cuidadosos con la ubicación del índice en los coeficientes de conexión; no son tensores y, por lo tanto, debes intentar no subir ni bajar sus índices.

\subsection{¿Qué pasa con las derivadas covariantes de otros tipos de tensores?}
Mediante un razonamiento similar al utilizado para los vectores, la derivada covariante de una forma única también se puede expresar como una derivada parcial más alguna transformación lineal.
Pero todavía no hay ninguna razón para que las matrices que representan esta transformación deban estar relacionadas con los coeficientes $\Gamma^\nu_{\mu\lambda}$.
En general podríamos escribir algo como
\begin{equation}
\nabla_\mu \omega_\nu = \partial_\mu \omega_\nu +
\widetilde{\Gamma}^\lambda_{\mu\nu}
\omega_\lambda\ ,\label{3.7}
\end{equation}
donde $\widetilde{\Gamma}^\lambda_{\mu\nu}$ es un nuevo conjunto de matrices para cada $\mu$.
(Preste atención a dónde van los distintos índices).
Es sencillo deducir que las propiedades de transformación de $\widetilde{\Gamma}$ deben ser las mismas que las de $\Gamma$, pero por lo demás no se ha establecido ninguna relación.
Para hacerlo, necesitamos introducir dos nuevas propiedades que nos gustaría que tuviera nuestra derivada covariante (además de las dos anteriores):

\begin{enumerate}
\setcounter{enumi}{2}
\item Conmuta con contracciones: $\;\nabla_\mu(T^\lambda{}_{\lambda\rho}) =(\nabla T)_\mu{}^\lambda{}_{\lambda\rho}$,
\item Se reduce a la derivada parcial en escalares: $\;\nabla_\mu\phi =\p\mu\phi$ .
\end{enumerate}

\noindent
No hay manera de ``derivar'' estas propiedades; simplemente exigimos que sean verdaderas como parte de la definición de \textbf{Derivada Covariante}.

\subsection{Veamos qué implican estas nuevas propiedades.}
Dado un campo de una forma $\omega_\mu$ y un campo vectorial $V^\mu$, podemos tomar la derivada covariante del escalar definido por $\omega_\lambda V^\lambda$ para obtener
\begin{align}
\nabla_\mu(\omega_\lambda V^\lambda) &=
(\nabla_\mu \omega_\lambda)V^\lambda + \omega_\lambda
(\nabla_\mu V^\lambda) \notag \\
&=  (\p\mu\omega_\lambda)V^\lambda +
\widetilde{\Gamma}^\sigma_{\mu\lambda}\omega_\sigma V^\lambda
+\omega_\lambda(\p\mu V^\lambda) +
\omega_\lambda\Gamma^\lambda_{\mu\rho}V^\rho\,. \label{3.8}
\end{align}
Pero como $\omega_\lambda V^\lambda$ es un escalar, esto también debe venir dado por la derivada parcial:
\begin{align}
\nabla_\mu(\omega_\lambda V^\lambda) &=  \partial_\mu
(\omega_\lambda V^\lambda)  \notag \\  &=
(\partial_\mu \omega_\lambda)V^\lambda + \omega_\lambda
(\partial_\mu V^\lambda)\,. \label{3.9}
\end{align}
Esto sólo puede ser cierto si los términos de (3.8) con coeficientes de conexión se cancelan entre sí; es decir, reorganizando índices ficticios, debemos tener
\begin{equation}
0 = \widetilde{\Gamma}^\sigma_{\mu\lambda}\omega_\sigma V^\lambda
+ {\Gamma}^\sigma_{\mu\lambda}\omega_\sigma V^\lambda\,.\label{3.10}
\end{equation}
Pero tanto $\omega_\sigma$ como $V^\lambda$ son completamente arbitrarios, por lo que
\begin{equation}
\widetilde{\Gamma}^\sigma_{\mu\lambda} = - \Gamma^\sigma_{\mu\lambda}
\,.\label{3.11}
\end{equation}
Por lo tanto, las dos condiciones adicionales que hemos impuesto nos permiten expresar la derivada covariante de una forma única usando los mismos coeficientes de conexión que se usaron para el vector, pero ahora con un signo menos (y los índices coinciden de manera algo diferente):
\begin{equation}
\nabla_\mu\omega_\nu = \partial_\mu\omega_\nu
-\Gamma^\lambda_{\mu\nu}\omega_\lambda\,.\label{3.12}
\end{equation}

No debería sorprender que los coeficientes de conexión codifiquen toda la información necesaria para tomar la derivada covariante de un tensor de rango arbitrario.
La fórmula es bastante sencilla; para cada índice superior introduces un término con un solo $+\Gamma$, y para cada índice inferior un término con un solo $-\Gamma$:
\begin{align}
\nabla_\sigma T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\nu_2 \cdots \nu_l} &= \partial_\sigma T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}  \notag \\
& +\Gamma^{\mu_1}_{\sigma\lambda}\, T^{\lambda \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}
+\Gamma^{\mu_2}_{\sigma\lambda}\, T^{\mu_1 \lambda \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l} +\cdots \notag \\
& -\Gamma^\lambda_{\sigma\nu_1}T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\lambda \nu_2 \cdots \nu_l}
-\Gamma^\lambda_{\sigma\nu_2}T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\lambda \cdots \nu_l} - \cdots \,. \label{3.13}
\end{align}
Ésta es la expresión general de la derivada covariante.
Puedes comprobarlo tú mismo; proviene del conjunto de axiomas que hemos establecido y de los requisitos habituales de que los tensores de varios tipos sean entidades independientes de coordenadas.
A veces se utiliza una notación alternativa; Así como se usan comas para las derivadas parciales, el punto y coma se usan para las covariantes:
\begin{equation}
\nabla_\sigma T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\nu_2 \cdots \nu_l} \equiv T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\nu_2 \cdots \nu_l ;\sigma}\,.\label{3.14}
\end{equation}
Una vez más, no soy un gran admirador de esta notación.

\subsection{Necesitamos poner una ``conexión'' en nuestra variedad}

Entonces, para definir una derivada covariante, necesitamos poner una ``conexión'' en nuestra variedad, que se especifica en algún sistema de coordenadas mediante un conjunto de coeficientes $\Gamma^\lambda_\mn$ ($n^3=64$ componentes independientes en $n=4$ dimensiones) que transformar según (3.6).
(El nombre ``conexión'' proviene del hecho de que se utiliza para transportar vectores de un espacio tangente a otro, como veremos pronto.)
Evidentemente, hay una gran cantidad de conexiones que podríamos definir en cualquier variedad, y cada una de ellas implica una noción distinta de diferenciación covariante.
En la Relatividad General esta libertad no es una gran preocupación, porque resulta que cada métrica define una conexión única, que es la que se usa en Relatividad General.
Veamos cómo funciona.

Lo primero que hay que notar es que la diferencia de dos conexiones es un tensor $(1,2)$.
Si tenemos dos conjuntos de coeficientes de conexión, $\Gamma^\lambda_\mn$ y $\widehat\Gamma^\lambda_\mn$, su diferencia $S_{\mn}{}^\lambda = \Gamma^\lambda_\mn-\widehat\Gamma^\lambda_\mn$ (ubicación del índice de aviso) se transforma como
\begin{align}
S_{\mu'\nu'}{}^{\lambda'} &=  \Gamma^{\lambda'}_{\mu'\nu'}
-\widehat\Gamma^{\lambda'}_{\mu'\nu'} \notag \\
&= {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial x^{\lambda'}}\over{\partial x^{\lambda}}}
\Gamma^\lambda_{\mu\nu} - {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial^2 x^{\lambda'}}\over{\partial x^{\mu}\partial x^{\nu}}}
- {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial x^{\lambda'}}\over{\partial x^{\lambda}}}
\widehat\Gamma^\lambda_{\mu\nu}
+ {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial^2 x^{\lambda'}}\over{\partial x^{\mu}\partial x^{\nu}}} \notag \\
&=  {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial x^{\lambda'}}\over{\partial x^{\lambda}}}
(\Gamma^\lambda_{\mu\nu}-\widehat\Gamma^\lambda_{\mu\nu}) \notag \\
&=  {{\partial x^\mu}\over{\partial x^{\mu'}}}
{{\partial x^\nu}\over{\partial x^{\nu'}}}
{{\partial x^{\lambda'}}\over{\partial x^{\lambda}}}
S_{\mu\nu}{}^{\lambda}\,. \label{3.15}
\end{align}
Esta es solo la ley de transformación del tensor, por lo que $S_{\mn}{}^\lambda$ es de hecho un tensor.
Esto implica que cualquier conjunto de conexiones puede expresarse como alguna conexión fiduciaria más una corrección tensorial.

A continuación, observe que, dada una conexión especificada por $\Gamma^\lambda_\mn$, podemos formar inmediatamente otra conexión simplemente permutando los índices inferiores.
Es decir, el conjunto de coeficientes $\Gamma^\lambda_{\nu\mu}$ también se transformará según (3.6) (ya que las derivadas parciales que aparecen en el último término se pueden conmutar), por lo que determinan una conexión distinta.
Por tanto, existe un tensor que podemos asociar con cualquier conexión dada, conocido como {\bf tensor de torsión}, definido por
\begin{equation}
T_{\mu\nu}{}^{\lambda} = \Gamma^\lambda_\mn - \Gamma^\lambda_{\nu\mu}
= 2\Gamma^\lambda_{[\mu\nu]}\,.\label{3.16}
\end{equation}
Está claro que la torsión es antisimétrica en sus índices inferiores, y una conexión que es simétrica en sus índices inferiores se conoce como ``libre de torsión''.

Ahora podemos definir una conexión única en un colector con una métrica $g_\mn$ introduciendo dos propiedades adicionales:

\begin{itemize}
\item Sin torsión: $\;\Gamma^\lambda_{\mu\nu}= \Gamma^\lambda_{(\mu\nu)}$.
\item Compatibilidad de métricas: $\;\nabla_\rho g_\mn=0$.
\end{itemize}

\noindent
Una conexión es compatible con métricas si la derivada covariante de la métrica con respecto a esa conexión es cero en todas partes.
Esto implica un par de buenas propiedades.

\textit{En primer lugar},\; es fácil demostrar que la métrica inversa también tiene derivada covariante cero,
\begin{equation}
\nabla_\rho g^\mn = 0\,.\label{3.17}
\end{equation}

\textit{En segundo lugar},\; una derivada covariante compatible con métricas conmuta con la subida y bajada de los índices.
Por lo tanto, para algún campo vectorial $V^\lambda$,
\begin{equation}
g_{\mu\lambda}\nabla_\rho V^\lambda = \nabla_\rho
(g_{\mu\lambda} V^\lambda) = \nabla_\rho V_\mu\,.\label{3.18}
\end{equation}
Con conexiones no compatibles con métricas se debe tener mucho cuidado con la colocación del índice al tomar una derivada covariante.

Por lo tanto, nuestra afirmación es que hay exactamente una conexión libre de torsión en un colector dado que es compatible con alguna métrica determinada en ese colector.
No queremos que estos dos requisitos formen parte de la definición de derivada covariante; simplemente seleccionan uno de los muchos posibles.

\subsection{Podemos demostrar tanto la existencia como la unicidad}

Podemos demostrar tanto la existencia como la unicidad derivando una expresión manifiestamente única para los coeficientes de conexión en términos de la métrica.
Para lograr esto, ampliamos la ecuación de compatibilidad métrica para tres permutaciones diferentes de los índices:
\begin{align}
\nabla_\rho g_\mn &=  \p\rho g_\mn - \Gamma^\lambda_{\rho\mu}
g_{\lambda\nu} - \Gamma^\lambda_{\rho\nu}g_{\mu\lambda} = 0 \notag \\
\nabla_\mu g_{\nu\rho} &=  \p\mu g_{\nu\rho} -\Gamma^\lambda_{\mu\nu}
g_{\lambda\rho} - \Gamma^\lambda_{\mu\rho}g_{\nu\lambda} = 0 \notag \\
\nabla_\nu g_{\rho\mu} &=  \p\nu g_{\rho\mu} -\Gamma^\lambda_{\nu\rho}
g_{\lambda\mu} - \Gamma^\lambda_{\nu\mu} g_{\rho\lambda} = 0\,.
\label{3.19}
\end{align}
Restamos el segundo y el tercero del primero y usamos la simetría de la conexión para obtener
\begin{equation}
\p\rho g_\mn - \p\mu g_{\nu\rho} - \p\nu g_{\rho\mu}
+2\Gamma^\lambda_\mn g_{\lambda\rho} = 0\,.\label{3.20}
\end{equation}
Es sencillo resolver esto para la conexión multiplicando por $g^{\sigma\rho}$.
El resultado es
\begin{equation}
\Gamma^\sigma_\mn = {1\over 2} g^{\sigma\rho}(\p\mu g_{\nu\rho} +
\p\nu g_{\rho\mu} - \p\rho g_\mn)\,.\label{3.21}
\end{equation}
Esta es una de las fórmulas más importantes en este tema; memorícelo.
Por supuesto, sólo hemos demostrado que si existe una conexión compatible con el sistema métrico y libre de torsión, debe ser de la forma (3.21); Puedes comprobar por ti mismo (para aquellos de ustedes que no tienen suficientes cálculos tediosos en sus vidas) que el lado derecho de (3.21) se transforma como una conexión.

Esta conexión que hemos derivado de la métrica es en la que se basa la Relatividad General convencional (aunque mantendremos la mente abierta por un tiempo más).
Se le conoce con diferentes nombres: a veces la conexión {\bf Christoffel}, a veces la conexión {\bf Levi-Civita}, a veces la conexión {\bf Riemanniana}.
Los coeficientes de conexión asociados a veces se denominan {\bf símbolos de Christoffel} y se escriben como $\left\{{\sigma \atop \mu\;\nu} \right\}$; A veces los llamaremos símbolos de Christoffel, pero no usaremos esta notación divertida.
El estudio de variedades con métricas y sus conexiones asociadas se llama ``geometría riemanniana''. Hasta donde puedo decir, el estudio de conexiones más generales se remonta a Cartan, pero nunca escuché que se llamara ``geometría cartaniana''. ''

\subsection{Debemos mencionar algunas propiedades diversas}

Antes de poner a trabajar nuestras derivadas covariantes, debemos mencionar algunas propiedades diversas.

Primero, enfaticemos nuevamente que la conexión no tiene que construirse a partir de la métrica.
En el espacio plano ordinario hay una conexión implícita que usamos todo el tiempo: la \textit{conexión de Christoffel} construida a partir de la métrica plana.
Pero podríamos, si así lo deseamos, utilizar una conexión diferente, manteniendo la métrica plana.
Observe también que los coeficientes de la \textit{conexión de Christoffel} en el espacio plano desaparecerán en las coordenadas cartesianas, pero no en los sistemas de coordenadas curvilíneos.
Consideremos, por ejemplo, el plano en coordenadas polares, con sistema métrico.
\begin{equation}
ds^2 = d\,r^2 + r^2 d\,\theta^2\,.\label{3.22}
\end{equation}
Se encuentra fácilmente que los componentes distintos de cero de la métrica inversa son $g^{rr}=1$ y $g^{\theta\theta}=r^{-2}$.
(Observe que usamos $r$ y $\theta$ como índices en una notación obvia).
Podemos calcular un coeficiente de conexión típico:
\begin{align}
\Gamma^r_{rr} &=  {1\over 2} g^{r\rho}(\p{r} g_{r\rho} +
\p{r} g_{\rho r} - \p\rho g_{rr}) \notag \\
&=  {1\over 2} g^{rr}(\p{r} g_{rr} +
\p{r} g_{rr} - \p{r} g_{rr}) \notag \\
& + {1\over 2} g^{r\theta}(\p{r} g_{r\theta} +
\p{r} g_{\theta r} - \p\theta g_{rr}) \notag \\
&=  {1\over 2}(1)(0+0-0) + {1\over 2}(0)(0+0-0) \notag \\
&= 0\,. \label{3.23}
\end{align}
Lamentablemente, desaparece.
Pero no todos lo hacen:
\begin{align}
\Gamma^r_{\theta\theta} &=  {1\over 2} g^{r\rho}
(\p{\theta} g_{\theta\rho} + \p{\theta} g_{\rho \theta}
- \p\rho g_{\theta\theta}) \notag \\
&=  {1\over 2}g^{rr}
(\p{\theta} g_{\theta r} + \p{\theta} g_{r \theta}
- \p{r} g_{\theta\theta}) \notag \\
&=  {1\over 2}(1)(0+0-2r) \notag \\
&=  -r\,. \label{3.24}
\end{align}
Continuamos girando la manivela, finalmente encontramos
\begin{align}
\Gamma^r_{\theta r} &=  \Gamma^r_{r\theta} = 0 \notag \\
\Gamma^\theta_{rr} &=  0 \notag \\
\Gamma^\theta_{r\theta} &=  \Gamma^\theta_{\theta r} = {1\over r} \notag \\
\Gamma^\theta_{\theta\theta} &=  0\,. \label{3.25}
\end{align}
La existencia de coeficientes de conexión que no desaparecen en sistemas de coordenadas curvilíneos es la causa última de las fórmulas para la divergencia y demás que se encuentran en los libros sobre electricidad y magnetismo.

Por el contrario, incluso en un espacio curvo es posible hacer que los símbolos de Christoffel desaparezcan en cualquier punto.
Esto se debe simplemente a que, como vimos en la última sección, siempre podemos hacer que la primera derivada de la métrica desaparezca en un punto; entonces, según (3.21), los coeficientes de conexión derivados de esta métrica también desaparecerán.
Por supuesto, esto sólo puede establecerse en un punto, no en alguna vecindad del punto.

Otra propiedad útil es que la fórmula para la divergencia de un vector (con respecto a la conexión de Christoffel) tiene una forma simplificada.
La divergencia covariante de $V^\mu$ viene dada por
\begin{equation}
\nabla_\mu V^\mu = \p\mu V^\mu +\Gamma^\mu_{\mu\lambda}V^\lambda
\,.\label{3.26}
\end{equation}
Es fácil demostrar (véanse las páginas 106-108 de Weinberg) que la conexión de Christoffel satisface
\begin{equation}
\Gamma^\mu_{\mu\lambda}= {1\over{\sqrt{|g|}}}\p\lambda
\sqrt{|g|}\ ,\label{3.27}
\end{equation}
y por lo tanto obtenemos
\begin{equation}
\nabla_\mu V^\mu = {1\over{\sqrt{|g|}}}\p\mu(\sqrt{|g|}V^\mu)
\,.\label{3.28}
\end{equation}
También existen fórmulas para las divergencias de tensores de rango superior, pero generalmente no constituyen una simplificación tan grande.

\subsubsection{La Derivada Exterior es un tensor bien definido en ausencia de cualquier conexión.}

Como último dato que debemos mencionar sobre las conexiones, enfaticemos (una vez más) que la derivada exterior es un tensor bien definido en ausencia de cualquier conexión.
La razón por la que es necesario enfatizar esto es que, si se utiliza una conexión simétrica (libre de torsión), la derivada exterior (definida como la derivada parcial antisimetrizada) resulta ser igual a la derivada covariante antisimetrizada:
\begin{align}
\nabla_{[\mu}\omega_{\nu]} &=  \p{[\mu}\omega_{\nu]}
-\Gamma^\lambda_{[\mn]}\omega_\lambda  \notag \\
&=  \p{[\mu}\omega_{\nu]}\,. \label{3.29}
\end{align}
Esto ha llevado a algunas almas desafortunadas a preocuparse por la ``ambigüedad'' de la derivada exterior en espacios con torsión, donde la simplificación anterior no ocurre.
No hay ambigüedad: la derivada exterior no implica la conexión, sin importar qué conexión estés usando, y por lo tanto la torsión nunca entra en la fórmula para la derivada exterior de nada.

\subsubsection{El proceso de agregar estructuras a nuestras construcciones matemáticas.}

Antes de continuar, revisemos el proceso mediante el cual hemos estado agregando estructuras a nuestras construcciones matemáticas.
Comenzamos con la noción básica de conjunto, que se suponía que conocías (de manera informal, si no rigurosa).
Introdujimos el concepto de subconjuntos abiertos de nuestro conjunto; esto equivale a introducir una topología y promover el conjunto a un espacio topológico.
Luego, al exigir que cada conjunto abierto pareciera una región de $\R^n$ (con $n$ igual para cada conjunto) y que los gráficos de coordenadas se unieran suavemente, el espacio topológico se convirtió en una variedad.
Una variedad es a la vez una estructura muy flexible y poderosa, y viene equipada naturalmente con un paquete tangente, paquetes tensoriales de varios rangos, la capacidad de tomar derivadas exteriores, etc.
Luego procedimos a poner una métrica en la variedad, lo que resultó en una variedad con métrica (o, a veces, ``variedad de Riemann'').
Independientemente de la métrica que encontremos, podríamos introducir una conexión, lo que nos permitiría tomar derivadas covariantes.
Sin embargo, una vez que tenemos una métrica, automáticamente hay una conexión única compatible con métricas sin torsión.
(En principio, no hay nada que nos impida introducir más de una conexión, o más de una métrica, en una variedad determinada).
La situación es, pues, la que se muestra en el diagrama de la página siguiente.

\begin{figure}[h]
\centering
\includegraphics[width=0.35\linewidth]{imagenes/three01.pdf}
\end{figure}

\section{Hablemos del Transporte Paralelo.}

Una vez montada la maquinaria de conexiones, lo primero que haremos será hablar del transporte paralelo.
Recordemos que en el espacio plano no era necesario tener mucho cuidado con el hecho de que los vectores eran elementos de espacios tangentes definidos en puntos individuales; en realidad es muy natural comparar vectores en diferentes puntos (donde por ``comparar'' queremos decir sumar, restar, tomar el producto escalar, etc.).
La razón por la que es natural es porque tiene sentido, en un espacio plano, ``mover un vector de un punto a otro manteniéndolo constante''. Luego, una vez que obtengamos el vector de un punto a otro, podemos hacer lo habitual. Operaciones permitidas en un espacio vectorial.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/three02.pdf}
\end{figure}

El concepto de mover un vector a lo largo de una trayectoria, manteniéndolo constante a cada paso, se conoce como transporte paralelo.
Como veremos, el transporte paralelo se define siempre que tengamos una conexión; la manipulación intuitiva de vectores en un espacio plano hace un uso implícito de la conexión de Christoffel en este espacio.
La diferencia crucial entre espacios planos y curvos es que, en un espacio curvo, \textit{el resultado del transporte paralelo de un vector de un punto a otro dependerá del camino tomado entre los puntos}.
Sin aún ensamblar el mecanismo completo del transporte paralelo, podemos usar nuestra intuición sobre las dos esferas para ver que este es el caso.
Comience con un vector en el ecuador, apuntando a lo largo de una línea de longitud constante.
Transpórtelo en paralelo hasta el polo norte a lo largo de una línea de longitud de la manera obvia.
Luego tome el vector original, transpórtelo en paralelo a lo largo del ecuador en un ángulo $\theta$ y luego muévalo hacia el polo norte como antes.
Está claro que el vector, transportado en paralelo por dos caminos, llegó al mismo destino con dos valores diferentes (girado $\theta$).

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/three03.pdf}
\end{figure}

Por tanto, parece que no existe una forma natural de mover de forma única un vector de un espacio tangente a otro; Siempre podemos transportarlo en paralelo, pero el resultado depende del camino y no existe una elección natural sobre qué camino tomar.
A diferencia de algunos de los problemas que hemos encontrado, \textit{que este no tiene solución} --- simplemente debemos aprender a vivir con el hecho de que dos vectores sólo pueden compararse de forma natural si son elementos del mismo espacio tangente.
Por ejemplo, dos partículas que pasan entre sí tienen una velocidad relativa bien definida (que no puede ser mayor que la velocidad de la luz).
Pero dos partículas en diferentes puntos de una variedad curva no tienen una noción bien definida de velocidad relativa; el concepto simplemente no tiene sentido.
Por supuesto, en determinadas situaciones especiales sigue siendo útil hablar como si tuviera sentido, pero es necesario comprender que la utilidad ocasional no sustituye a una definición rigurosa.
En cosmología, por ejemplo, la luz de galaxias distantes está desplazada al rojo con respecto a las frecuencias que observaríamos desde una fuente estacionaria cercana.
Dado que este fenómeno guarda una gran semejanza con el efecto Doppler convencional debido al movimiento relativo, es muy tentador decir que las galaxias se están ``alejando de nosotros'' a una velocidad definida por su corrimiento al rojo.
En un nivel riguroso esto es una tontería, lo que Wittgenstein llamaría un ``error gramatical'': las galaxias no están retrocediendo, ya que la noción de su velocidad con respecto a nosotros no está bien definida.
Lo que realmente está sucediendo es que la métrica del Espacio-Tiempo entre nosotros y las galaxias ha cambiado (el universo se ha expandido) a lo largo del camino del fotón de aquí a allá, lo que lleva a un aumento en la longitud de onda de la luz.
Como ejemplo de cómo se puede equivocar, la aplicación ingenua de la fórmula Doppler al corrimiento al rojo de las galaxias implica que algunas de ellas se están alejando más rápido que la luz, en aparente contradicción con la relatividad.
La solución a esta aparente paradoja es simplemente que la noción misma de su recesión no debe tomarse literalmente.

Ya basta de lo que no podemos hacer; veamos qué podemos.
Se supone que el transporte paralelo es la generalización en el espacio curvo del concepto de ``mantener constante el vector'' a medida que lo movemos a lo largo de una trayectoria; de manera similar para un tensor de rango arbitrario.
Dada una curva $x^\mu(\lambda)$, el requisito de constancia de un tensor $T$ a lo largo de esta curva en un espacio plano es simplemente ${{dT}\over{d\lambda}} = {{dx^\mu}\over{d\lambda}}{{\partial T}\over{\partial x^\mu}}=0$.
Por lo tanto, definimos la derivada covariante a lo largo del camino que dará un operador
\begin{equation}
{{D}\over {d\lambda}} = {{dx^\mu}\over{d\lambda}}\nabla_\mu
\,.\label{3.30}
\end{equation}
Luego definimos {\bf transporte paralelo} del tensor $T$ a lo largo del camino $x^\mu(\lambda)$ como el requisito de que, a lo largo del camino,
\begin{equation}
\left({{D}\over{d\lambda}}T\right)^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l} \equiv {{dx^\sigma}\over
{d\lambda}}\nabla_\sigma T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l} = 0\,.\label{3.31}
\end{equation}
Esta es una ecuación tensorial bien definida, ya que tanto el vector tangente $dx^\mu/d\lambda$ como la derivada covariante $\nabla T$ son tensores.
Esto se conoce como {\bf ecuación de transporte paralelo}.
Para un vector toma la forma
\begin{equation}
{{d}\over{d\lambda}} V^\mu
+ \Gamma^\mu_{\sigma\rho}{{dx^\sigma}\over{d\lambda}}V^\rho = 0\,.
\label{3.32}
\end{equation}
Podemos considerar la ecuación de transporte paralelo como una ecuación diferencial de primer orden que define un problema de valor inicial: dado un tensor en algún punto a lo largo del camino, habrá una continuación única del tensor hacia otros puntos a lo largo del camino tal que el la continuación resuelve (3.31).
Decimos que dicho tensor se transporta en paralelo.

La noción de transporte paralelo depende obviamente de la conexión, y diferentes conexiones conducen a diferentes respuestas.
Si la conexión es compatible con la métrica, la métrica siempre se transporta en paralelo con respecto a ella:
\begin{equation}
{{D}\over{d\lambda}}g_\mn = {{dx^\sigma}\over{d\lambda}}
\nabla_\sigma g_\mn =0\,.\label{3.33}
\end{equation}
De ello se deduce que se conserva el producto interno de dos vectores transportados en paralelo.
Es decir, si $V^\mu$ y $W^\nu$ se transportan en paralelo a lo largo de una curva $x^\sigma(\lambda)$, tenemos
\begin{align}
{{D}\over{d\lambda}}(g_\mn V^\mu W^\nu) &=
\left({{D}\over{d\lambda}}g_\mn\right)V^\mu W^\nu +
g_\mn \left({{D}\over{d\lambda}} V^\mu\right)W^\nu +
g_\mn V^\mu\left({{D}\over{d\lambda}} W^\nu\right) \notag \\
&=  0\,. \label{3.34}
\end{align}
Esto significa que el transporte paralelo con respecto a una conexión compatible con métricas preserva la norma de los vectores, el sentido de ortogonalidad, etc.

Una cosa que no suelen decirte en los libros de Relatividad General es que puedes escribir una solución explícita y general a la ecuación de transporte paralelo, aunque es algo formal.
Primero, observe que para alguna ruta $\gamma :\lambda \rightarrow x^\sigma(\lambda)$, resolver la ecuación de transporte paralelo para un vector $V^\mu$ equivale a encontrar una matriz $P^\mu{}_\rho(\lambda,\lambda_0)$ que relaciona el vector en su valor inicial $V^\mu(\lambda_0)$ con su valor en algún lugar posterior de la ruta:
\begin{equation}
V^\mu(\lambda) = P^\mu{}_\rho(\lambda,\lambda_0)V^\rho(\lambda_0)
\,.\label{3.35}
\end{equation}
Por supuesto, la matriz $P^\mu{}_\rho(\lambda,\lambda_0)$, conocida como {\bf propagador paralelo}, depende de la ruta $\gamma$ (aunque es difícil encontrar una notación que indique esto sin que $\gamma$ parezca un índice).
si definimos
\begin{equation}
A^\mu{}_\rho(\lambda) = -\Gamma^\mu_{\sigma\rho}
{{dx^\sigma}\over{d \lambda}}\ ,\label{3.36}
\end{equation}
donde las cantidades del lado derecho se evalúan en $x^\nu(\lambda)$, entonces la ecuación de transporte paralelo se convierte en
\begin{equation}
{{d}\over{d\lambda}}V^\mu = A^\mu{}_\rho V^\rho\,.\label{3.37}
\end{equation}
Dado que el propagador paralelo debe funcionar para cualquier vector, sustituir (3.35) en (3.37) muestra que $P^\mu{}_\rho(\lambda,\lambda_0)$ también obedece a esta ecuación:
\begin{equation}
{{d}\over{d\lambda}}P^\mu{}_\rho(\lambda,\lambda_0) =
A^\mu{}_\sigma(\lambda) P^\sigma{}_\rho(\lambda,\lambda_0)
\,.\label{3.38}
\end{equation}
Para resolver esta ecuación, primero integra ambos lados:
\begin{equation}
P^\mu{}_\rho(\lambda,\lambda_0)=\delta^\mu_\rho
+\int^\lambda_{\lambda_0} A^\mu{}_\sigma(\eta)
P^\sigma{}_\rho(\eta,\lambda_0)\, d\eta\,.\label{3.39}
\end{equation}
Es fácil ver que el delta de Kronecker proporciona la normalización correcta para $\lambda=\lambda_0$.

Podemos resolver (3.39) por iteración, tomando el lado derecho y conectándolo a sí mismo repetidamente, dando
\begin{equation}
P^\mu{}_\rho(\lambda,\lambda_0)=\delta^\mu_\rho
+\int^\lambda_{\lambda_0} A^\mu{}_\rho(\eta) \, d\eta
+\int^\lambda_{\lambda_0} \int^\eta_{\lambda_0}
A^\mu{}_\sigma(\eta) A^\sigma{}_\rho(\eta')\, d\eta' d\eta
+\cdots\,.\label{3.40}
\end{equation}
El $n$-ésimo término de esta serie es una integral sobre un triángulo rectángulo de $n$ dimensión o $n$-símplex.
\begin{equation*}
\int^\lambda_{\lambda_0} A(\eta_1) \, d\eta_1 \qquad
\int^\lambda_{\lambda_0} \int^{\eta_2}_{\lambda_0}
A(\eta_2) A(\eta_1)\, d\eta_1 d\eta_2 \qquad
\int^\lambda_{\lambda_0} \int^{\eta_3}_{\lambda_0}\int^{\eta_2}_{\lambda_0}
A(\eta_3) A(\eta_2) A(\eta_1)\, d^3\eta
\end{equation*}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{imagenes/three04.pdf}
\end{figure}

Simplificaría las cosas si pudiéramos considerar que dicha integral está sobre un cubo $n$ en lugar de un símplex $n$; ¿Hay alguna manera de hacer esto? Hay $n!$ simples de este tipo en cada cubo, por lo que tendríamos que multiplicar por $1/n!$ para compensar este volumen adicional.
Pero también queremos acertar con el integrando; Usando notación matricial, el integrando de $n$-ésimo orden es $A(\eta_n)A(\eta_{n-1})\cdots A(\eta_1)$, pero con la propiedad especial de que $\eta_n\geq \eta_{n-1}\geq \cdots \geq \eta_1$.
Por lo tanto, definimos el {\bf símbolo de ordenación de rutas}, ${\cal P}$, para garantizar que esta condición se cumpla.
En otras palabras, la expresión
\begin{equation}
{\cal P}[A(\eta_n)A(\eta_{n-1})\cdots A(\eta_1)]\label{3.41}
\end{equation}
representa el producto de las matrices $n$ $A(\eta_i)$, ordenadas de tal manera que el valor mayor de $\eta_i$ está a la izquierda, y cada valor posterior de $\eta_i$ es menor o igual al anterior.
Entonces podemos expresar el término de orden $n$ en (3.40) como
\begin{align}
\lefteqn{\int^\lambda_{\lambda_0}\int^{\eta_n}_{\lambda_0}\cdots
\int^{\eta_2}_{\lambda_0} A(\eta_n) A(\eta_{n-1})\cdots
A(\eta_1)\, d^n\eta}  \notag \\
&=  {1\over{n!}}\int^\lambda_{\lambda_0}
\int^\lambda_{\lambda_0}\cdots\int^\lambda_{\lambda_0}
{\cal P}[A(\eta_n) A(\eta_{n-1})\cdots A(\eta_1)]\, d^n\eta\,.
\label{3.42}
\end{align}
Esta expresión no contiene ninguna declaración sustancial sobre las matrices $A(\eta_i)$; es solo notación.
Pero ahora podemos escribir (3.40) en forma matricial como
\begin{equation}
P(\lambda,\lambda_0) =  \bigone  + \sum^\infty_{n=1}{1\over {n!}}
\int^\lambda_{\lambda_0} {\cal P}[A(\eta_n) A(\eta_{n-1})\cdots
A(\eta_1)]\, d^n\eta\,.\label{3.43}
\end{equation}
Esta fórmula es sólo la expresión en serie de una exponencial; por lo tanto decimos que el propagador paralelo está dado por el exponencial ordenado por trayectoria
\begin{equation}
P(\lambda,\lambda_0) = {\cal P}\exp\left(\int^\lambda_{\lambda_0}
A(\eta)\, d\eta\right)\ ,\label{3.44}
\end{equation}
donde una vez más esto es sólo notación; la exponencial ordenada por trayectoria se define como el lado derecho de (3.43).
Podemos escribirlo más explícitamente como
\begin{equation}
P^\mu{}_\nu(\lambda,\lambda_0) ={\cal P}\exp\left(-
\int^\lambda_{\lambda_0}\Gamma^\mu_{\sigma\nu}{{dx^\sigma}\over
{d\eta}}\, d\eta\right)\,.\label{3.45}
\end{equation}
Es bueno tener una fórmula explícita, aunque sea bastante abstracta.
El mismo tipo de expresión aparece en la teoría cuántica de campos como ``fórmula de Dyson'', donde surge porque la ecuación de Schrödinger para el operador de evolución en el tiempo tiene la misma forma que (3.38).

Además, un ejemplo especialmente interesante del propagador paralelo ocurre cuando la ruta es un bucle, que comienza y termina en el mismo punto.
Entonces, si la conexión es compatible con métricas, la matriz resultante será simplemente una transformación de Lorentz en el espacio tangente en el punto.
Esta transformación se conoce como la ``holonomía'' del bucle.
Si conoce la holonomía de cada bucle posible, eso equivale a conocer la métrica.
Este hecho ha permitido a Ashtekar y sus colaboradores examinar la Relatividad General en la ``representación de bucle'', donde las variables fundamentales son holonomías en lugar de la métrica explícita.
Han logrado algunos avances hacia la cuantificación de la teoría en este enfoque, aunque aún no se sabe cuánto progreso se puede lograr.

Una vez entendido el transporte paralelo, el siguiente paso lógico es discutir las geodésicas.
Una geodésica es la generalización en el espacio curvo de la noción de ``línea recta'' en el espacio euclidiano.
Todos sabemos lo que es una línea recta: es el camino de menor distancia entre dos puntos.
Pero hay una definición igualmente buena: una línea recta es un camino que paralelamente transporta su propio vector tangente.
En una variedad con una conexión arbitraria (no necesariamente Christoffel), estos dos conceptos no coinciden del todo y deberíamos discutirlos por separado.

Tomaremos primero la segunda definición, ya que computacionalmente es mucho más sencilla.
El vector tangente a una ruta $x^\mu(\lambda)$ es $dx^\mu/d\lambda$.
La condición de que sea transportado en paralelo es, por tanto,
\begin{equation}
{{D}\over{d\lambda}}{{dx^\mu}\over{d\lambda}}=0\ ,\label{3.46}
\end{equation}
o alternativamente
\begin{equation}
{{d^2x^\mu}\over{d\lambda^2}}+\Gamma^\mu_{\rho\sigma}
{{dx^\rho}\over{d\lambda}}{{dx^\sigma}\over{d\lambda}}=0\,.
\label{3.47}
\end{equation}
Esta es la {\bf ecuación geodésica}, otra que debes memorizar.
Podemos ver fácilmente que reproduce la noción habitual de líneas rectas si los coeficientes de conexión son los símbolos de Christoffel en el espacio euclidiano; en ese caso podemos elegir coordenadas cartesianas en las que $\Gamma^\mu_{\rho\sigma}=0$, y la ecuación geodésica es simplemente $d^2x^\mu/d\lambda^2=0$, que es la ecuación de una línea recta.

Eso fue vergonzosamente simple; Pasemos al caso menos trivial de la definición de distancia más corta.
Como sabemos, existen varias sutilezas involucradas en la definición de distancia en un Espacio-Tiempo lorentziano; para caminos nulos la distancia es cero, para caminos temporales es más conveniente usar el tiempo adecuado, etc.
Entonces, en nombre de la simplicidad, hagamos el cálculo solo para una ruta temporal: la ecuación resultante resultará buena para cualquier ruta, por lo que no estamos perdiendo ninguna generalidad.
Por lo tanto consideramos funcional el tiempo adecuado,
\begin{equation}
\tau = \int \left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\right)^{1/2}\, d\lambda\ ,\label{3.48}
\end{equation}
donde la integral está sobre el camino.
Para buscar caminos de menor distancia, haremos el tratamiento habitual de cálculo de variaciones para buscar extremos de este funcional.
(De hecho, resultarán ser curvas de {\it máximo} tiempo propio.)

Queremos considerar el cambio en el tiempo adecuado bajo variaciones infinitesimales del camino,
\begin{align}
x^\mu &\rightarrow x^\mu+\delta x^\mu \notag \\
g_\mn &\rightarrow g_\mn + \delta x^\sigma\partial_\sigma g_\mn
\,. \label{3.49}
\end{align}
(La segunda línea proviene de la expansión de Taylor en el Espacio-Tiempo curvo, que, como puede ver, utiliza la derivada parcial, no la derivada covariante).
Al conectar esto a (3.48), obtenemos
\begin{align}
\tau + \delta\tau &=  \int\left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}} - \p\sigma g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\delta x^\sigma
-2 g_\mn {{dx^\mu}\over{d\lambda}}{{d(\delta x^\nu)}\over{d\lambda}}
\right)^{1/2}\, d\lambda \notag \\
&=  \int\left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\right)^{1/2}
\left[1+\left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\right)^{-1}\right. \notag \\
& \qquad\qquad\left.
\times\left(-\p\sigma g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\delta x^\sigma
-2 g_\mn {{dx^\mu}\over{d\lambda}}{{d(\delta x^\nu)}\over{d\lambda}}
\right)\right]^{1/2}\, d\lambda\,. \label{3.50}
\end{align}
Dado que se supone que $\delta x^\sigma$ es pequeño, podemos expandir la raíz cuadrada de la expresión entre corchetes para encontrar
\begin{equation}
\delta\tau = \int\left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\right)^{-1/2}
\left(-{1\over 2}\p\sigma g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\delta x^\sigma
- g_\mn {{dx^\mu}\over{d\lambda}}{{d(\delta x^\nu)}\over{d\lambda}}
\right)\, d\lambda\,.\label{3.51}
\end{equation}
Es útil en este punto cambiar la parametrización de nuestra curva de $\lambda$, que era arbitraria, al tiempo adecuado $\tau$, usando
\begin{equation}
d\lambda = \left(-g_\mn {{dx^\mu}\over{d\lambda}}
{{dx^\nu}\over{d\lambda}}\right)^{-1/2}\, d\tau\,.\label{3.52}
\end{equation}
Conectamos esto a (3.51) (nota: lo conectamos para cada aparición de $d\lambda$) para obtener
\begin{align}
\delta\tau &= \int \left[-{1\over 2}\p\sigma g_\mn
{{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}}\delta x^\sigma
- g_\mn {{dx^\mu}\over{d\tau}}{{d(\delta x^\nu)}\over{d\tau}}
\right]\, d\tau \notag \\
&=  \int \left[-{1\over 2}\p\sigma g_\mn
{{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}}
+{{d}\over{d\tau}}\left(g_{\mu\sigma} {{dx^\mu}\over{d\tau}}\right)
\right]\delta x^\sigma\, d\tau\ , \label{3.53}
\end{align}
donde en la última línea hemos integrado por partes, evitando posibles contribuciones de límites al exigir que la variación $\delta x^\sigma$ desaparezca en los puntos finales del camino.
Como estamos buscando puntos estacionarios, queremos que $\delta \tau$ desaparezca para cualquier variación; esto implica
\begin{equation}
-{1\over 2}\p\sigma g_\mn {{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}}
+ {{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}} \p\nu g_{\mu\sigma}
+g_{\mu\sigma}{{d^2x^\mu}\over{d\tau^2}} = 0\ ,\label{3.54}
\end{equation}
donde hemos utilizado $d g_{\mu\sigma}/d\tau=(dx^\nu/d\tau)\p\nu g_{\mu\sigma}$.
Un poco de mezcla de índices ficticios revela
\begin{equation}
g_{\mu\sigma}{{d^2x^\mu}\over{d\tau^2}} +{1\over 2}\left(
-\p\sigma g_{\mn} + \p\nu g_{\mu\sigma} + \p\mu g_{\nu\sigma}
\right){{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}} =0\ ,\label{3.55}
\end{equation}
y multiplicar por la métrica inversa finalmente conduce a
\begin{equation}
{{d^2x^\rho}\over{d\tau^2}} +{1\over 2}g^{\rho\sigma}\left(
\p\mu g_{\nu\sigma} + \p\nu g_{\sigma\mu}-\p\sigma g_{\mn}
\right){{dx^\mu}\over{d\tau}} {{dx^\nu}\over{d\tau}} =0\,.\label{3.56}
\end{equation}
Vemos que esta es precisamente la ecuación geodésica (3.32), pero con la elección específica de la conexión de Christoffel (3.21).
Así, en una variedad con métrica, los extremos de la longitud funcional son curvas que transportan paralelamente su vector tangente con respecto a la conexión de Christoffel asociada con esa métrica.
No importa si hay alguna otra conexión definida en el mismo colector.
Por supuesto, en Relatividad General la conexión Christoffel es la única que se utiliza, por lo que las dos nociones son las mismas.

La principal utilidad de las geodésicas en la Relatividad General es que son los caminos seguidos por partículas no aceleradas.
De hecho, se puede considerar la ecuación geodésica como la generalización de la ley de Newton ${\bf f}=m{\bf a}$ para el caso ${\bf f}=0$.
También es posible introducir fuerzas añadiendo términos al lado derecho; de hecho, volviendo a la expresión (1.103) para la fuerza de Lorentz en la Relatividad Especial, es tentador suponer que la ecuación de movimiento para una partícula de masa $m$ y carga $q$ en la Relatividad General debería ser
\begin{equation}
{{d^2x^\mu}\over{d\tau^2}}+\Gamma^\mu_{\rho\sigma}
{{dx^\rho}\over{d\tau}}{{dx^\sigma}\over{d\tau}}=
{q\over m}F^\mu{}_\nu{{dx^\nu}\over{d\tau}}\,.\label{3.57}
\end{equation}
Hablaremos de esto más adelante, pero en realidad tu suposición sería correcta.

Habiendo derivado audazmente estas expresiones, deberíamos decir algunas palabras más cuidadosas sobre la parametrización de una trayectoria geodésica.
Cuando presentamos la ecuación geodésica como el requisito de que el vector tangente sea transportado paralelo, (3.47), parametrizamos nuestra trayectoria con algún parámetro $\lambda$, mientras que cuando encontramos la fórmula (3.56) para el extremo del intervalo espacio-temporal terminamos con una parametrización muy específica, el momento adecuado.
Por supuesto, de la forma de (3.56) está claro que una transformación
\begin{equation}
\tau \rightarrow \lambda = a\tau +b \ ,\label{3.58}
\end{equation}
para algunas constantes $a$ y $b$, deja la ecuación invariante.
Cualquier parámetro relacionado con el tiempo adecuado de esta manera se denomina {\bf parámetro afín} y es tan bueno como el tiempo adecuado para parametrizar una geodésica.
Lo que estaba oculto en nuestra derivación de (3.47) era que \textit{la exigencia de que el vector tangente se transporte paralelo en realidad limita la parametrización de la curva}, específicamente una relacionada con el tiempo adecuado mediante (3.58).
En otras palabras, si comienzas en algún punto y con alguna dirección inicial, y luego construyes una curva comenzando a caminar en esa dirección y manteniendo tu vector tangente paralelo transportado, no solo definirás una ruta en la variedad sino también (hasta a transformaciones lineales) definen el parámetro a lo largo de la ruta.

Por supuesto, no hay nada que le impida utilizar cualquier otra parametrización que desee, pero entonces (3.47) no quedará satisfecho.
De manera más general, satisfarás una ecuación de la forma
\begin{equation}
{{d^2x^\mu}\over{d\alpha^2}}+\Gamma^\mu_{\rho\sigma}
{{dx^\rho}\over{d\alpha}}{{dx^\sigma}\over{d\alpha}}=
f(\alpha){{dx^\mu}\over{d\alpha}}\ ,\label{3.59}
\end{equation}
para algún parámetro $\alpha$ y alguna función $f(\alpha)$.
Por el contrario, si se satisface (3.59) a lo largo de una curva, siempre se puede encontrar un parámetro afín $\lambda(\alpha)$ para el cual se cumplirá la ecuación geodésica (3.47).

Una propiedad importante de las geodésicas en un Espacio-Tiempo con métrica de Lorentz es que el carácter (temporal/nulo/espacial) de la geodésica (en relación con una conexión compatible con métricas) nunca cambia.
Esto se debe simplemente a que el transporte paralelo conserva los productos internos y el carácter está determinado por el producto interno del vector tangente consigo mismo.
Esta es la razón por la que fuimos consistentes al considerar trayectorias puramente temporales cuando derivamos (3.56); para trayectorias espaciales habríamos obtenido la misma ecuación, ya que la única diferencia es un signo menos general en la respuesta final.
También hay geodésicas nulas, que satisfacen la misma ecuación, excepto que el tiempo adecuado no puede usarse como parámetro (existirá algún conjunto de parámetros permitidos, relacionados entre sí mediante transformaciones lineales).
Este hecho se puede derivar del simple requisito de que el vector tangente se transporte en paralelo, o extendiendo la variación de (3.48) para incluir todos los caminos no espaciales.

Expliquemos ahora la observación anterior de que las geodésicas temporales son máximos del tiempo propio.
La razón por la que sabemos que esto es cierto es que, dada cualquier curva temporal (geodésica o no), podemos aproximarla con precisión arbitraria mediante una curva nula.
Para hacer esto todo lo que tenemos que hacer es considerar curvas nulas ``irreguladas'' que siguen la curva temporal:

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/three05.pdf}
\end{figure}

\noindent
A medida que aumentamos el número de esquinas agudas, la curva nula se acerca cada vez más a la curva temporal sin dejar de tener una longitud de trayectoria cero.
Por lo tanto, las geodésicas temporales no pueden ser curvas de tiempo propio mínimo, ya que siempre están infinitamente cercanas a curvas de tiempo propio cero; de hecho, maximizan el tiempo adecuado.
(Así es como puedes recordar qué gemelo en la paradoja de los gemelos envejece más: el que se queda en casa está básicamente en una geodésica y, por lo tanto, experimenta un tiempo más adecuado).
Por supuesto, incluso esto es ser un poco arrogante; de hecho, cada vez que decimos ``maximizar'' o ``minimizar'' debemos agregar el modificador ``localmente''. A menudo ocurre que entre dos puntos de una variedad hay más de una geodésica.
Por ejemplo, en $S^2$ podemos dibujar un círculo máximo que pase por dos puntos cualesquiera e imaginar viajar entre ellos en el sentido corto o en el sentido más largo.
Uno de ellos es obviamente más largo que el otro, aunque ambos son puntos estacionarios de la longitud funcional.

El último hecho sobre las geodésicas antes de pasar a la curvatura propiamente dicha es su uso para mapear el espacio tangente en un punto $p$ a una vecindad local de $p$.
Para ello observamos que cualquier geodésica $x^\mu(\lambda)$ que pase por $p$ puede especificarse por su comportamiento en $p$; Elijamos el valor del parámetro como $\lambda(p)=0$ y el vector tangente en $p$ como
\begin{equation}
{{d x^\mu}\over{d\lambda}}(\lambda=0)=k^\mu\ ,\label{3.60}
\end{equation}
para $k^\mu$ algún vector en $p$ (algún elemento de $T_p$).
Entonces habrá un punto único en el colector $M$ que se encuentra en esta geodésica donde el parámetro tiene el valor $\lambda=1$.
Definimos el {\bf mapa exponencial} en $p$, $\exp_p :T_p\rightarrow M$, vía
\begin{equation}
\exp_p(k^\mu) = x^\nu(\lambda = 1)\ ,\label{3.61}
\end{equation}
donde $x^\nu(\lambda)$ resuelve la ecuación geodésica sujeta a (3.60).
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/three06.pdf}
\end{figure}
Para algún conjunto de vectores tangentes $k^\mu$ cerca del vector cero, este mapa estará bien definido y, de hecho, será invertible.
Por lo tanto, en la vecindad de $p$ dada por el rango del mapa en este conjunto de vectores tangentes, los propios vectores tangentes definen un sistema de coordenadas en la variedad.
En este sistema de coordenadas, cualquier geodésica que pase por $p$ se expresa trivialmente como
\begin{equation}
x^\mu(\lambda) = \lambda k^\mu\ ,\label{3.62}
\end{equation}
para algún vector apropiado $k^\mu$.

No entraremos en detalles sobre las propiedades del mapa exponencial, ya que de hecho no lo usaremos mucho, pero es importante enfatizar que el rango del mapa no es necesariamente toda la variedad, y el dominio no es necesariamente todo el espacio tangente.
El rango puede no ser todo $M$ simplemente porque puede haber dos puntos que no estén conectados por ninguna geodésica.
(En una métrica de firma euclidiana esto es imposible, pero no en un Espacio-Tiempo lorentziano).
El dominio puede no ser todo $T_p$ porque una geodésica puede toparse con una singularidad, que consideramos ``el borde de la variedad''. Las variedades que tienen tales singularidades se conocen como {\bf geodésicamente incompletas}.
Éste no es simplemente un problema para matemáticos cuidadosos; de hecho, los ``teoremas de singularidad'' de Hawking y Penrose establecen que, para un contenido de materia razonable (sin energías negativas), es casi seguro que los espacio-tiempos en la Relatividad General serán geodésicamente incompletos.
Como ejemplos, los dos espaciotiempos más útiles en Relatividad General (la solución de Schwarzschild que describe los agujeros negros y las soluciones de Friedmann-Robertson-Walker que describen cosmologías isotrópicas homogéneas) presentan singularidades importantes.

Habiendo establecido la maquinaria del transporte paralelo y las derivadas covariantes, por fin estamos preparados para discutir la curvatura propiamente dicha.
La curvatura se cuantifica mediante el tensor de Riemann, que se deriva de la conexión.
La idea detrás de esta medida de curvatura es que sabemos lo que queremos decir con ``planicidad'' de una conexión: la conexión de Christoffel convencional (y generalmente implícita) asociada con una métrica euclidiana o minkowskiana tiene una serie de propiedades que pueden ser considerados como diferentes manifestaciones de planitud.
Estos incluyen el hecho de que el transporte paralelo alrededor de un circuito cerrado deja un vector sin cambios, que las derivadas covariantes de tensores conmutan y que las geodésicas inicialmente paralelas permanecen paralelas.
Como veremos, el tensor de Riemann surge cuando estudiamos cómo se altera cualquiera de estas propiedades en contextos más generales.

Ya hemos argumentado, usando las dos esferas como ejemplo, que el transporte paralelo de un vector alrededor de un circuito cerrado en un espacio curvo conducirá a una transformación del vector.
La transformación resultante depende de la curvatura total encerrada por el bucle; Sería más útil tener una descripción local de la curvatura en cada punto, que es lo que se supone que proporciona el tensor de Riemann.
Por lo tanto, una forma convencional de introducir el tensor de Riemann es considerar el transporte paralelo alrededor de un bucle infinitesimal.
No vamos a hacer eso aquí, sino que tomaremos una ruta más directa.
(La mayoría de las presentaciones en la literatura son descuidadas o correctas, pero muy difíciles de seguir).
Sin embargo, incluso sin analizar los detalles, es posible ver qué forma debería adoptar la respuesta.
Imaginemos que transportamos en paralelo un vector $V^\sigma$ alrededor de un circuito cerrado definido por dos vectores $A^\nu$ y $B^\mu$:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/three07.pdf}
\end{figure}

\noindent
Las longitudes (infinitesimales) de los lados del bucle son $\delta a$ y $\delta b$, respectivamente.
Ahora, sabemos que la acción del transporte paralelo es independiente de las coordenadas, por lo que debería haber algún tensor que nos diga cómo cambia el vector cuando regresa a su punto inicial; será una transformación lineal en un vector y, por lo tanto, implicará un índice superior y uno inferior.
Pero también dependerá de los dos vectores $A$ y $B$ que definen el bucle; por lo tanto, debería haber dos índices inferiores adicionales para contratar con $A^\nu$ y $B^\mu$.
Además, el tensor debe ser antisimétrico en estos dos índices, ya que intercambiar los vectores corresponde a recorrer el bucle en la dirección opuesta, y debe dar la respuesta inversa a la original.
(Esto es consistente con el hecho de que la transformación debería desaparecer si $A$ y $B$ son el mismo vector).
Por lo tanto, esperamos que la expresión para el cambio $\delta V^\rho$ experimentado por este vector cuando se transporta en paralelo alrededor del bucle sea de la forma
\begin{equation}
\delta V^\rho = (\delta a) (\delta b) A^\nu B^\mu
R^\rho{}_{\sigma \mu\nu} V^\sigma\ ,\label{3.63}
\end{equation}
donde $R^\rho{}_{\sigma \mu\nu}$ es un tensor $(1,3)$ conocido como {\bf tensor de Riemann} (o simplemente ``tensor de curvatura'').
Es antisimétrico en los dos últimos índices:
\begin{equation}
R^\rho{}_{\sigma \mu\nu}=-R^\rho{}_{\sigma \nu\mu}\,.\label{3.64}
\end{equation}
(Por supuesto, si se toma (3.63) como definición del tensor de Riemann, es necesario elegir una convención para el orden de los índices.
No hay ningún acuerdo sobre cuál debería ser esta convención, así que tenga cuidado).

Sabiendo lo que hacemos con el transporte paralelo, podríamos realizar con mucho cuidado las manipulaciones necesarias para ver qué le sucede al vector bajo esta operación, y el resultado sería una fórmula para el tensor de curvatura en términos de los coeficientes de conexión.
Sin embargo, es mucho más rápido considerar una operación relacionada, el conmutador de dos derivadas covariantes.
La relación entre esto y el transporte paralelo alrededor de un circuito debería ser evidente; la derivada covariante de un tensor en una dirección determinada mide cuánto cambia el tensor en relación con lo que habría sido si se hubiera transportado en paralelo (ya que la derivada covariante de un tensor en una dirección a lo largo de la cual se transporta en paralelo es cero).
El conmutador de dos derivadas covariantes, entonces, mide la diferencia entre transportar en paralelo el tensor primero en una dirección y luego en la otra, versus el orden opuesto.

\begin{figure}[h]
\centering
\includegraphics[width=0.35\linewidth]{imagenes/three08.pdf}
\end{figure}

El cálculo real es muy sencillo.
Considerando un campo vectorial $V^\rho$, tomamos
\begin{align}
[\nabla_\mu,\nabla_\nu]V^\rho &=  \nabla_\mu\nabla_\nu
V^\rho - \nabla_\nu\nabla_\mu V^\rho  \notag \\
&= \p\mu(\nabla_\nu V^\rho) -\Gamma^\lambda_{\mn} \nabla_\lambda
V^\rho + \Gamma^\rho_{\mu\sigma} \nabla_\nu V^\sigma
- (\mu \leftrightarrow \nu) \notag \\
&=  \p\mu \p\nu V^\rho + (\p\mu \Gamma^\rho_{\nu\sigma})V^\sigma
+\Gamma^\rho_{\nu\sigma}\p\mu V^\sigma - \Gamma^\lambda_{\mn}
\p\lambda V^\rho - \Gamma^\lambda_\mn \Gamma^\rho_{\lambda\sigma}
V^\sigma  \notag \\
&\qquad +\Gamma^\rho_{\mu\sigma}\p\nu V^\sigma + \Gamma^\rho_{\mu\sigma}
\Gamma^\sigma_{\nu\lambda}V^\lambda - (\mu\leftrightarrow \nu ) \notag \\
&=  (\p\mu\Gamma^\rho_{\nu\sigma}-\p\nu\Gamma^\rho_{\mu\sigma}
+\Gamma^\rho_{\mu\lambda}\Gamma^\lambda_{\nu\sigma}
-\Gamma^\rho_{\nu\lambda}\Gamma^\lambda_{\mu\sigma})V^\sigma
- 2\Gamma^\lambda_{[\mn]}\nabla_\lambda V^\rho \,. \label{3.65}
\end{align}
En el último paso, hemos vuelto a etiquetar algunos índices ficticios y hemos eliminado algunos términos que se cancelan cuando se antisimetrizan.
Reconocemos que el último término es simplemente el tensor de torsión, y que el lado izquierdo es manifiestamente un tensor; por lo tanto, la expresión entre paréntesis debe ser un tensor en sí misma.
Nosotros escribimos
\begin{equation}
[\nabla_\mu,\nabla_\nu]V^\rho = R^\rho{}_{\sigma\mn}V^\sigma
- T_{\mn}{}^\lambda\nabla_\lambda V^\rho\ ,\label{3.66}
\end{equation}
donde el tensor de Riemann se identifica como
\begin{equation}
R^\rho{}_{\sigma\mn}=\p\mu\Gamma^\rho_{\nu\sigma}- \p\nu
\Gamma^\rho_{\mu\sigma}+\Gamma^\rho_{\mu\lambda}
\Gamma^\lambda_{\nu\sigma} -\Gamma^\rho_{\nu\lambda}
\Gamma^\lambda_{\mu\sigma}\,.\label{3.67}
\end{equation}
Hay una serie de cosas a tener en cuenta acerca de la derivación de esta expresión:

\begin{itemize}
\item Por supuesto, no hemos demostrado que (3.67) sea en realidad el mismo tensor que apareció en (3.63), pero de hecho es cierto (ver Wald para una demostración creíble aunque tortuosa).
\item Quizás resulte sorprendente que el conmutador $[\nabla_\mu,\nabla_\nu]$, que parece ser un operador diferencial, tenga una acción sobre campos vectoriales que (al menos en ausencia de torsión) es una simple transformación multiplicativa.
El tensor de Riemann mide la parte del conmutador de derivadas covariantes que es proporcional al campo vectorial, mientras que el tensor de torsión mide la parte que es proporcional a la derivada covariante del campo vectorial; la segunda derivada no entra en absoluto.
\item Observe que la expresión (3.67) se construye a partir de elementos no tensoriales; puede comprobar que todas las leyes de transformación funcionan para hacer de esta combinación particular un tensor legítimo.
\item La antisimetría de $R^\rho{}_{\sigma\mn}$ en sus dos últimos índices es inmediata a partir de esta fórmula y su derivación.
\item Construimos el tensor de curvatura completamente a partir de la conexión (no se mencionó la métrica).
Tuvimos suficiente cuidado de que la expresión anterior sea cierta para cualquier conexión, sea o no compatible métricamente o libre de torsión.
\item Usando lo que ahora son nuestros métodos habituales, la acción de $[\nabla_\rho,\nabla_\sigma]$ se puede calcular en un tensor de rango arbitrario.
La respuesta es
\begin{align}
[\nabla_\rho,\nabla_\sigma]X^{\mu_1\cdots
\mu_k}{}_{\nu_1\cdots\nu_l} &=
{} -T_{\rho\sigma}{}^\lambda\nabla_\lambda
X^{\mu_1\cdots \mu_k}{}_{\nu_1\cdots\nu_l}  \notag \\
&\quad +R^{\mu_1}{}_{\lambda\rho\sigma} X^{\lambda \mu_2\cdots \mu_k}{}_{\nu_1
\cdots \nu_l}+R^{\mu_2}{}_{\lambda\rho\sigma} X^{\mu_1\lambda\cdots
\mu_k}{}_{\nu_1 \cdots \nu_l} +\cdots  \notag \\
&\quad -R^{\lambda}{}_{\nu_1\rho\sigma} X^{\mu_1\cdots \mu_k}{}_{\lambda\nu_2
\cdots \nu_l} - R^{\lambda}{}_{\nu_2\rho\sigma} X^{\mu_1\cdots
\mu_k}{}_{\nu_1\lambda\cdots \nu_l} - \cdots \,. \label{3.68}
\end{align}
\end{itemize}

Una noción útil es la del conmutador de dos campos vectoriales $X$ y $Y$, que es un tercer campo vectorial con componentes
\begin{equation}
[X,Y]^\mu = X^\lambda\p\lambda Y^\mu - Y^\lambda\p\lambda X^\mu\,.
\label{3.69}
\end{equation}
Tanto el tensor de torsión como el tensor de Riemann, considerados aplicaciones multilineales, tienen expresiones elegantes en términos del conmutador.
Pensando en la torsión como un mapa de dos campos vectoriales a un tercer campo vectorial, tenemos
\begin{equation}
T(X,Y) = \nabla_X Y - \nabla_Y X - [X,Y]\ ,\label{3.70}
\end{equation}
y pensando en el tensor de Riemann como un mapa de tres campos vectoriales a un cuarto, tenemos
\begin{equation}
R(X,Y)Z = \nabla_X\nabla_Y Z-\nabla_Y\nabla_X Z
- \nabla_{[X,Y]}Z\,.\label{3.71}
\end{equation}
En estas expresiones, la notación $\nabla_X$ se refiere a la derivada covariante a lo largo del campo vectorial $X$; en componentes, $\nabla_X = X^\mu\nabla_\mu$.
Observe que los dos vectores $X$ y $Y$ en (3.71) corresponden a los dos índices antisimétricos en la forma componente del tensor de Riemann.
El último término en (3.71), que involucra al conmutador $[X,Y]$, desaparece cuando $X$ y $Y$ se toman como los campos vectoriales base de coordenadas (desde $[\p\mu ,\p\nu]=0$), razón por la cual este término no surgió cuando Originalmente tomó el conmutador de dos derivadas covariantes.
No usaremos esta notación extensamente, pero es posible que la vea en la literatura, por lo que debería poder decodificarla.

Habiendo definido el tensor de curvatura como algo que caracteriza la conexión, admitamos ahora que en Relatividad General lo que más nos preocupa es la conexión de Christoffel.
En este caso, la conexión se deriva de la métrica y la curvatura asociada puede considerarse como la de la propia métrica.
Esta identificación nos permite finalmente dar sentido a nuestra noción informal de que los espacios para los cuales la métrica parece euclidiana o minkowskiana son planos.
De hecho funciona en ambos sentidos: si las componentes de la métrica son constantes en algún sistema de coordenadas, el tensor de Riemann desaparecerá, mientras que si el tensor de Riemann desaparece siempre podemos construir un sistema de coordenadas en el que las componentes de la métrica sean constantes.

El primero de ellos es fácil de demostrar.
Si estamos en algún sistema de coordenadas tal que $\p\sigma g_\mn=0$ (en todas partes, no solo en un punto), entonces $\Gamma^\rho_\mn = 0$ y $\p\sigma\Gamma^\rho_\mn = 0$; por lo tanto $R^\rho{}_{\sigma\mn}=0$ por (3.67).
Pero ésta es una ecuación tensorial, y si es cierta en un sistema de coordenadas, debe serlo en cualquier sistema de coordenadas.
Por tanto, la afirmación de que el tensor de Riemann desaparece es una condición necesaria para que sea posible encontrar coordenadas en las que las componentes de $g_\mn$ sean constantes en todas partes.

También es una condición suficiente, aunque tengamos que trabajar más para demostrarlo.
Comience eligiendo las coordenadas normales de Riemann en algún punto $p$, de modo que $g_\mn = \eta_\mn$ en $p$.
(Aquí usamos $\eta_\mn$ en un sentido generalizado, como una matriz con $+1$ o $-1$ para cada elemento diagonal y ceros en otros lugares.
La disposición real de los $+1$ y $-1$ depende de la forma canónica de la métrica, pero es irrelevante para el presente argumento).
Denota los vectores base en $p$ por $\e\mu$, con componentes $\e\mu^\sigma$.
Entonces por construcción tenemos
\begin{equation}
g_{\sigma\rho}\e\mu^\sigma \e\nu^\rho (p) =\eta_\mn\,.\label{3.72}
\end{equation}
Ahora transportemos en paralelo todo el conjunto de vectores base desde $p$ a otro punto $q$; la desaparición del tensor de Riemann garantiza que el resultado será independiente del camino tomado entre $p$ y $q$.
Dado que el transporte paralelo con respecto a una conexión métrica compatible preserva los productos internos, debemos tener
\begin{equation}
g_{\sigma\rho}\e\mu^\sigma \e\nu^\rho (q) =\eta_\mn\,.\label{3.73}
\end{equation}
Por lo tanto, hemos especificado un conjunto de campos vectoriales que definen en todas partes una base en la que los componentes métricos son constantes.
Esto es completamente impresionante; se puede hacer en cualquier colector, independientemente de cuál sea la curvatura.
Lo que nos gustaría mostrar es que se trata de una base de coordenadas (que sólo puede ser cierta si la curvatura desaparece).
Sabemos que si los $\e\mu$ son una base de coordenadas, su conmutador desaparecerá:
\begin{equation}
[\e\mu,\e\nu] = 0\,.\label{3.74}
\end{equation}
Lo que realmente nos gustaría es lo contrario: que si el conmutador desaparece podamos encontrar coordenadas $y^\mu$ tales que $\e\mu = {{\partial} \over{\partial y^\mu}}$.
De hecho, este es un resultado verdadero, conocido como {\bf Teorema de Frobenius}.
Es una especie de lío demostrarlo, ya que implica mucho más aparato matemático del que nos hemos molestado en montar.
Démoslo por sentado (los escépticos pueden consultar el libro {\sl Geométrico Métodos} de Schutz).
Por tanto, nos gustaría demostrar (3.74) para los campos vectoriales que hemos configurado.
Usemos la expresión (3.70) para la torsión:
\begin{equation}
[\e\mu,\e\nu] = \nabla_{\e\mu} \e\nu - \nabla_{\e\nu}\e\mu
- T(\e\mu,\e\nu)\,.\label{3.75}
\end{equation}
La torsión desaparece por hipótesis.
Las derivadas covariantes también desaparecerán, dado el método mediante el cual construimos nuestros campos vectoriales; se hicieron mediante transporte paralelo a lo largo de caminos arbitrarios.
Si los campos se transportan en paralelo a lo largo de trayectorias arbitrarias, ciertamente se transportan en paralelo a lo largo de los vectores $\e\mu$ y, por lo tanto, sus derivadas covariantes en la dirección de estos vectores desaparecerán.
Por tanto, (3.70) implica que el conmutador desaparece y, por tanto, que podemos encontrar un sistema de coordenadas $y^\mu$ para el cual estos campos vectoriales son las derivadas parciales.
En este sistema de coordenadas la métrica tendrá componentes $\eta_\mn$, según se desee.

El tensor de Riemann, con cuatro índices, tiene ingenuamente $n^4$ componentes independientes en un espacio $n$ -dimensional.
De hecho, la propiedad antisimetría (3.64) significa que sólo hay $n(n-1)/2$ valores independientes que estos dos últimos índices pueden adoptar, dejándonos con $n^3(n-1)/2$ componentes independientes.
Sin embargo, cuando consideramos la conexión de Christoffel, hay otras simetrías que reducen aún más los componentes independientes.
Consideremos esto ahora.

La forma más sencilla de derivar estas simetrías adicionales es examinar el tensor de Riemann con todos los índices más bajos,
\begin{equation}
R_{\rho\sigma\mn} = g_{\rho\lambda}R^\lambda{}_{\sigma\mn}\,.
\label{3.76}
\end{equation}
Consideremos más a fondo las componentes de este tensor en coordenadas normales de Riemann establecidas en un punto $p$.
Entonces los propios símbolos de Christoffel desaparecerán, aunque sus derivados no.
Por lo tanto tenemos
\begin{align}
R_{\rho\sigma\mn} &=  g_{\rho\lambda}
(\p\mu\Gamma^\lambda_{\nu\sigma}- \p\nu
\Gamma^\lambda_{\mu\sigma}) \notag \\
&=  {1\over 2}g_{\rho\lambda}g^{\lambda\tau}(
\p\mu\p\nu g_{\sigma\tau} + \p\mu\p\sigma g_{\tau\nu}
-\p\mu\p\tau g_{\nu\sigma} - \p\nu\p\mu g_{\sigma\tau}
- \p\nu\p\sigma g_{\tau\mu}+\p\nu\p\tau g_{\mu\sigma}) \notag \\
&= {1\over 2}(\p\mu\p\sigma g_{\rho\nu}
-\p\mu\p\rho g_{\nu\sigma} - \p\nu\p\sigma g_{\rho\mu}
+\p\nu\p\rho g_{\mu\sigma})\,. \label{3.77}
\end{align}
En la segunda línea hemos usado $\partial_\mu g^{\lambda\tau}=0$ en los RNC, y en la tercera línea el hecho de que los parciales conmutan.
De esta expresión podemos notar inmediatamente dos propiedades de $R_{\rho\sigma\mn}$; es antisimétrico en sus dos primeros índices,
\begin{equation}
R_{\rho\sigma\mn}=-R_{\sigma\rho\mn}\ ,\label{3.78}
\end{equation}
y es invariante bajo intercambio del primer par de índices con el segundo:
\begin{equation}
R_{\rho\sigma\mn}= R_{\mn\rho\sigma}\,.\label{3.79}
\end{equation}
Con un poco más de trabajo, que dejamos a tu imaginación, podemos ver que la suma de permutaciones cíclicas de los últimos tres índices se desvanece:
\begin{equation}
R_{\rho\sigma\mn} + R_{\rho\mn\sigma} + R_{\rho\nu\sigma\mu}
=0 \,.\label{3.80}
\end{equation}
Esta última propiedad equivale a la desaparición de la parte antisimétrica de los últimos tres índices:
\begin{equation}
R_{\rho[\sigma\mn]} =0 \,.\label{3.81}
\end{equation}
Todas estas propiedades se han derivado en un sistema de coordenadas especial, pero todas son ecuaciones tensoriales; por lo tanto, serán verdaderas en cualquier coordenada.
No todos son independientes; Con un poco de esfuerzo, puedes demostrar que (3.64), (3.78) y (3.81) juntos implican (3.79).
La interdependencia lógica de las ecuaciones suele ser menos importante que el simple hecho de que sean verdaderas.

Dadas estas relaciones entre los diferentes componentes del tensor de Riemann, ¿cuántas cantidades independientes quedan? Comencemos con el hecho de que $R_{\rho\sigma\mn}$ es antisimétrico en los dos primeros índices, antisimétrico en los dos últimos índices y simétrico bajo el intercambio de estos dos pares.
Esto significa que podemos considerarla como una matriz simétrica $R_{[\rho\sigma][\mn]}$, donde los pares $\rho\sigma$ y $\mu\nu$ se consideran índices individuales.
Una matriz simétrica $m\times m$ tiene $m(m+1)/2$ componentes independientes, mientras que una matriz antisimétrica $n\times n$ tiene $n(n-1)/2$ componentes independientes.
Por lo tanto tenemos
\begin{equation}
{1\over 2}\left[{1\over 2}n(n-1)\right]\left[{1\over 2}n(n-1)
+1\right] = {1\over 8}(n^4-2n^3+3n^2-2n)\label{3.82}
\end{equation}
componentes independientes.
Todavía tenemos que ocuparnos de la simetría adicional (3.81).
Una consecuencia inmediata de (3.81) es que la parte totalmente antisimétrica del tensor de Riemann desaparece,
\begin{equation}
R_{[\rho\sigma\mn]} =0 \,.\label{3.83}
\end{equation}
De hecho, esta ecuación más las otras simetrías (3.64), (3.78) y (3.79) son suficientes para implicar (3.81), como se puede demostrar fácilmente expandiendo (3.83) y alterando los términos resultantes.
Por lo tanto, imponer la restricción adicional de (3.83) equivale a imponer (3.81), una vez que se han tenido en cuenta las otras simetrías.
¿Cuántas restricciones independientes representa esto?
Imaginemos que se descompone
\begin{equation}
R_{\rho\sigma\mn}=X_{\rho\sigma\mn}+R_{[\rho\sigma\mn]}\,.
\label{3.84}
\end{equation}
Es fácil ver que cualquier tensor de 4 índices totalmente antisimétrico es automáticamente antisimétrico en su primer y último índice, y simétrico bajo el intercambio de los dos pares.
Por lo tanto, estas propiedades son restricciones independientes sobre $X_{\rho\sigma\mn}$, sin relación con el requisito (3.83).
Ahora bien, un tensor de 4 índices totalmente antisimétrico tiene $n(n-1)(n-2)(n-3)/4!$ términos y, por lo tanto, (3.83) reduce el número de componentes independientes en esta cantidad.
nos quedamos con
\begin{equation}
{1\over 8}(n^4-2n^3+3n^2-2n)-{1\over {24}}n(n-1)(n-2)(n-3) =
{1\over{12}}n^2(n^2-1)\label{3.85}
\end{equation}
componentes independientes del tensor de Riemann.

Por tanto, en cuatro dimensiones, el tensor de Riemann tiene 20 componentes independientes.
(En una dimensión no tiene ninguno).
Estas veinte funciones son precisamente los 20 grados de libertad de las segundas derivadas de la métrica que no pudimos poner a cero mediante una inteligente elección de coordenadas.
Esto debería reforzar su confianza en que el tensor de Riemann es una medida apropiada de curvatura.

Además de las simetrías algebraicas del tensor de Riemann (que restringen el número de componentes independientes en cualquier punto), existe una identidad diferencial a la que obedece (que restringe sus valores relativos en diferentes puntos).
Considere la derivada covariante del tensor de Riemann, evaluada en coordenadas normales de Riemann:
\begin{align}
\nabla_\lambda R_{\rho\sigma\mn}&= \p\lambda
R_{\rho\sigma\mn} \notag \\
&=  {1\over 2}\p\lambda(\p\mu\p\sigma g_{\rho\nu}
-\p\mu\p\rho g_{\nu\sigma} - \p\nu\p\sigma g_{\rho\mu}
+\p\nu\p\rho g_{\mu\sigma})\,. \label{3.86}
\end{align}
Nos gustaría considerar la suma de permutaciones cíclicas de los primeros tres índices:
\begin{align}
\lefteqn{\nabla_\lambda R_{\rho\sigma\mn} +
\nabla_\rho R_{\sigma\lambda\mn}+\nabla_\sigma R_{\lambda\rho\mn}}  \notag \\
&=  {1\over 2}
(\p\lambda\p\mu\p\sigma g_{\rho\nu} -\p\lambda\p\mu\p\rho g_{\nu\sigma}
-\p\lambda\p\nu\p\sigma g_{\rho\mu}+\p\lambda\p\nu\p\rho g_{\mu\sigma} \notag \\
& +\p\rho\p\mu\p\lambda g_{\sigma\nu} -\p\rho\p\mu\p\sigma g_{\nu\lambda}
-\p\rho\p\nu\p\lambda g_{\sigma\mu}+\p\rho\p\nu\p\sigma g_{\mu\lambda} \notag \\
& +\p\sigma\p\mu\p\rho g_{\lambda\nu} -\p\sigma\p\mu\p\lambda g_{\nu\rho}
-\p\sigma\p\nu\p\rho g_{\lambda\mu}+\p\sigma\p\nu\p\lambda g_{\mu\rho})
\notag \\  &= 0\,.\label{3.87}
\end{align}
Una vez más, dado que se trata de una ecuación entre tensores, es cierta en cualquier sistema de coordenadas, aunque la hayamos obtenido en uno en particular.
Ahora reconocemos que la antisimetría $R_{\rho\sigma\mn}=- R_{\sigma\rho\mn}$ nos permite escribir este resultado como
\begin{equation}
\nabla_{[\lambda}R_{\rho\sigma]\mn}=0\,.\label{3.88}
\end{equation}
Esto se conoce como la {\bf identidad Bianchi}.
(Observe que para una conexión general habría términos adicionales que involucran al tensor de torsión).
Está estrechamente relacionado con la identidad Jacobi, ya que (como se puede demostrar) básicamente expresa
\begin{equation}
[[\nabla_\lambda,\nabla_\rho],\nabla_\sigma]
+[[\nabla_\rho,\nabla_\sigma],\nabla_\lambda]
+[[\nabla_\sigma,\nabla_\lambda],\nabla_\rho]=0\,.\label{3.89}
\end{equation}

Con frecuencia resulta útil considerar las contracciones del tensor de Riemann.
Incluso sin la métrica, podemos formar una contracción conocida como {\bf tensor de Ricci}:
\begin{equation}
R_{\mn} = R^\lambda{}_{\mu\lambda\nu}\,.\label{3.90}
\end{equation}
Observe que, para el tensor de curvatura formado a partir de una conexión arbitraria (no necesariamente de Christoffel), hay que tomar varias contracciones independientes.
Nuestra principal preocupación es la conexión de Christoffel, para la cual (3.90) es la única contracción independiente (convenciones de módulo para el signo, que por supuesto cambian de un lugar a otro).
El tensor de Ricci asociado con la conexión de Christoffel es simétrico,
\begin{equation}
R_{\mn} = R_{\nu\mu}\ ,\label{3.91}
\end{equation}
como consecuencia de las diversas simetrías del tensor de Riemann.
Usando la métrica, podemos tomar una contracción adicional para formar el {\bf escalar de Ricci}:
\begin{equation}
R = R^\mu{}_\mu = g^\mn R_\mn\,.\label{3.92}
\end{equation}

Una forma especialmente útil de la identidad de Bianchi proviene de contraerse dos veces en (3.87):
\begin{align}
0&=  g^{\nu\sigma}g^{\mu\lambda}(\nabla_\lambda R_{\rho\sigma\mn}
+\nabla_\rho R_{\sigma\lambda\mn}+\nabla_\sigma R_{\lambda\rho\mn}) \notag \\
&= \nabla^\mu R_{\rho\mu}-\nabla_\rho R + \nabla^\nu R_{\rho\nu}\ ,
\label{3.93}
\end{align}
o
\begin{equation}
\nabla^\mu R_{\rho\mu} = {1\over 2}\nabla_\rho R\,.\label{3.94}
\end{equation}
(Obsérvese que, a diferencia de la derivada parcial, tiene sentido aumentar un índice en la derivada covariante, debido a la compatibilidad métrica).
Si definimos el {\bf tensor de Einstein} como
\begin{equation}
G_{\mu\nu} = R_\mn -{1\over 2} R g_\mn\ ,\label{3.95}
\end{equation}
entonces vemos que la identidad de Bianchi dos veces contraída (3.94) es equivalente a
\begin{equation}
\nabla^\mu G_{\mn} = 0\,.\label{3.96}
\end{equation}
El tensor de Einstein, que es simétrico debido a la simetría del tensor de Ricci y la métrica, será de gran importancia en la Relatividad General.

El tensor de Ricci y el escalar de Ricci contienen información sobre las ``trazas'' del tensor de Riemann.
A veces resulta útil considerar por separado aquellas partes del tensor de Riemann de las que el tensor de Ricci no nos informa.
Por lo tanto, inventamos el {\bf tensor de Weyl}, que es básicamente el tensor de Riemann al que se le han eliminado todas sus contracciones.
Está dado en $n$ dimensiones por
\begin{equation}
C_{\rho\sigma\mn} = R_{\rho\sigma\mn} - {2\over{(n-2)}}
\left(g_{\rho[\mu}R_{\nu]\sigma} - g_{\sigma[\mu}R_{\nu]\rho}
\right) +{2\over{(n-1)(n-2)}}R g_{\rho[\mu}g_{\nu]\sigma}\,.\label{3.97}
\end{equation}
Esta desordenada fórmula está diseñada para que todas las posibles contracciones de $C_{\rho\sigma\mn}$ desaparezcan, mientras conserva las simetrías del tensor de Riemann:
\begin{align}
C_{\rho\sigma\mn} &=  C_{[\rho\sigma][\mn]}\ , \notag \\
C_{\rho\sigma\mn} &=  C_{\mn\rho\sigma}\ , \notag \\
C_{\rho[\sigma\mn]} &= 0\,. \label{3.98}
\end{align}
El tensor de Weyl sólo se define en tres o más dimensiones, y en tres dimensiones desaparece de forma idéntica.
Para $n\geq 4$ satisface una versión de la identidad de Bianchi,
\begin{equation}
\nabla^\rho C_{\rho\sigma\mn} = -2{{(n-3)}\over{(n-2)}}
\left(\nabla_{[\mu}R_{\nu]\sigma} + {1\over{2(n-1)}}
g_{\sigma[\nu}\nabla_{\mu]}R\right)\,.\label{3.99}
\end{equation}
Una de las propiedades más importantes del tensor de Weyl es que es invariante bajo {\bf transformaciones conformes}.
Esto significa que si calcula $C_{\rho\sigma\mn}$ para alguna métrica $g_{\mn}$ y luego la calcula nuevamente para una métrica dada por $\Omega^2 (x)g_{\mn}$, donde $\Omega(x)$ es una función arbitraria que no desaparece del Espacio-Tiempo, obtendrá la misma respuesta.
Por esta razón, a menudo se le conoce como ``tensor conforme''.

Después de esta gran cantidad de formalismo, tal vez sea hora de dar un paso atrás y pensar en lo que significa la curvatura para algunos ejemplos simples.
Primero observe que, según (3.85), en 1, 2, 3 y 4 dimensiones hay 0, 1, 6 y 20 componentes del tensor de curvatura, respectivamente.
(Todo lo que decimos sobre la curvatura en estos ejemplos se refiere a la curvatura asociada con la conexión de Christoffel y, por lo tanto, con la métrica).
Esto significa que las variedades unidimensionales (como $S^1$) nunca son curvas; la intuición que tienes que te dice que un círculo es curvo proviene de pensar que está incrustado en un cierto plano bidimensional.
(Existe algo llamado ``curvatura extrínseca'', que caracteriza la forma en que algo está incrustado en un espacio de dimensiones superiores.
Nuestra noción de curvatura es ``intrínseca'' y no tiene nada que ver con tales incrustaciones).

La distinción entre curvatura intrínseca y extrínseca también es importante en dos dimensiones, donde la curvatura tiene un componente independiente.
(De hecho, toda la información sobre la curvatura está contenida en el componente único del escalar de Ricci).
Considere un cilindro, $\R\times S^1$.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/three09.pdf}
\end{figure}
Aunque esto parece curvo desde nuestro punto de vista, debería quedar claro que podemos poner una métrica en el cilindro cuyos componentes son constantes en un sistema de coordenadas apropiado: simplemente desenróllelo y use la métrica inducida del plano.
En esta métrica, el cilindro es plano.
(Tampoco hay nada que nos impida introducir una métrica diferente en la que el cilindro no sea plano, pero el punto que intentamos enfatizar es que se puede hacer plano en alguna métrica).
La misma historia se aplica al toroide:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/three10.pdf}
\end{figure}

\noindent
Podemos pensar en el toroide como una región cuadrada del plano con lados opuestos identificados (en otras palabras, $S^1\times S^1$), de lo cual está claro que puede tener una métrica plana aunque parezca curva desde el punto de vista incrustado. .

Un cono es un ejemplo de una variedad bidimensional con curvatura distinta de cero exactamente en un punto.
Esto también lo podemos ver desenrollándolo; el cono es equivalente al plano al que se le ha eliminado un ``ángulo de déficit'' y se han identificado los lados opuestos:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/three11.pdf}
\end{figure}

\noindent
En la métrica heredada de esta descripción como parte del plano, el cono es plano en todas partes menos en su vértice.
Esto puede verse considerando el transporte paralelo de un vector alrededor de varios bucles; si un bucle no encierra el vértice, no habrá transformación general, mientras que un bucle que encierra el vértice (digamos, sólo una vez) dará lugar a una rotación en un ángulo que es simplemente el ángulo deficitario.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/three12.pdf}
\end{figure}

Nuestro ejemplo favorito es, por supuesto, el de dos esferas, con
\begin{equation}
ds^2 = a^2( d\,\theta^2 + \sin^2\theta ~  d\,\phi^2)\ ,\label{3.100}
\end{equation}
donde $a$ es el radio de la esfera (considerada incrustada en $\R^3$).
Sin entrar en detalles, los coeficientes de conexión distintos de cero son
\begin{align}
\Gamma^\theta_{\phi\phi} &=  -\sin\theta \cos\theta \notag \\
\Gamma^\phi_{\theta\phi} = \Gamma^\phi_{\phi\theta} &=
\cot\theta\,. \label{3.101}
\end{align}
Calculemos un componente prometedor del tensor de Riemann:
\begin{align}
R^\theta{}_{\phi\theta\phi} &=  \p\theta
\Gamma^\theta_{\phi\phi} - \p\phi \Gamma^\theta_{\theta\phi}
+\Gamma^\theta_{\theta\lambda}\Gamma^\lambda_{\phi\phi}
-\Gamma^\theta_{\phi\lambda}\Gamma^\lambda_{\theta\phi} \notag \\
&=  (\sin^2\theta - \cos^2\theta) -(0) + (0) - (-\sin\theta
\cos\theta)(\cot\theta) \notag \\
&=  \sin^2\theta\,. \label{3.102}
\end{align}
(La notación es obviamente imperfecta, ya que la letra griega $\lambda$ es un índice ficticio que se suma, mientras que las letras griegas $\theta$ y $\phi$ representan coordenadas específicas).
Bajando un índice, tenemos
\begin{align}
R_{\theta\phi\theta\phi} &=  g_{\theta\lambda}
R^\lambda{}_{\phi\theta\phi} \notag \\
&= g_{\theta\theta}R^\theta{}_{\phi\theta\phi} \notag \\
&=  a^2\sin^2\theta\,. \label{3.103}
\end{align}
Es fácil comprobar que todos los componentes del tensor de Riemann desaparecen o están relacionados con éste por simetría.
Podemos continuar para calcular el tensor de Ricci mediante $R_{\mn}=g^{\alpha\beta}R_{\alpha\mu \beta\nu}$.
Obtenemos
\begin{align}
R_{\theta\theta} &=  g^{\phi\phi}R_{\phi\theta\phi\theta}
= 1 \notag \\
R_{\theta\phi} &=  R_{\phi\theta} = 0 \notag \\
R_{\phi\phi} &=  g^{\theta\theta}R_{\theta\phi\theta\phi}
= \sin^2\theta\,. \label{3.104}
\end{align}
El escalar de Ricci es igualmente sencillo:
\begin{equation}
R = g^{\theta\theta}R_{\theta\theta}+ g^{\phi\phi}R_{\phi\phi}
= {2\over{a^2}}\,.\label{3.105}
\end{equation}
Por lo tanto, el escalar de Ricci, que para una variedad bidimensional caracteriza completamente la curvatura, es una constante en esta biesfera.
Esto es un reflejo del hecho de que la variedad es ``máximamente simétrica'', un concepto que definiremos con mayor precisión más adelante (aunque significa lo que usted cree que debería).
En cualquier número de dimensiones, la curvatura de un espacio máximamente simétrico satisface (para alguna constante $a$)
\begin{equation}
R_{\rho\sigma\mu\nu} = a^{-2}(g_{\rho\mu}g_{\sigma\nu}
- g_{\rho\nu}g_{\sigma\mu})\ ,\label{3.106}
\end{equation}
que puede comprobar se cumple con este ejemplo.

Observe que el escalar de Ricci no sólo es constante para las dos esferas, sino que es manifiestamente positivo.
Decimos que la esfera es ``curva positiva'' (por supuesto, entraron en juego una convención o dos, pero afortunadamente nuestras convenciones conspiraron para que los espacios que todos están de acuerdo en llamar curvados positivamente tengan en realidad un escalar de Ricci positivo).
Desde el punto de vista de alguien que vive en una variedad que está incrustada en un espacio euclidiano de dimensiones superiores, si está sentado en un punto de curvatura positiva, el espacio se curva alejándose de él de la misma manera en cualquier dirección, mientras que en un espacio de curvatura negativa, el espacio se curva en direcciones opuestas.
Por tanto, los espacios con curvatura negativa tienen forma de silla de montar.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/three13.pdf}
\end{figure}

Basta de diversión con ejemplos.
Hay un tema más que debemos cubrir antes de presentar la propia Relatividad General: la desviación geodésica.
Sin duda habrás oído que la propiedad definitoria de la geometría euclidiana (plana) es el postulado de las paralelas: las líneas inicialmente paralelas permanecen paralelas para siempre.
Por supuesto, en un espacio curvo esto no es cierto; en una esfera, ciertamente, las geodésicas inicialmente paralelas eventualmente se cruzarán.
Nos gustaría cuantificar este comportamiento para un espacio curvo arbitrario.

El problema es que la noción de ``paralelo'' no se extiende naturalmente desde los espacios planos hasta los curvos.
En lugar de eso, lo que haremos será construir una familia de geodésicas de un parámetro, $\gamma_s(t)$.
Es decir, para cada $s\in\R$, $\gamma_s$ es una geodésica parametrizada por el parámetro afín $t$.
La colección de estas curvas define una superficie bidimensional suave (incrustada en una variedad $M$ de dimensionalidad arbitraria).
Las coordenadas de esta superficie se podrán elegir como $s$ y $t$, siempre que hayamos elegido una familia de geodésicas que no se crucen.
Toda la superficie es el conjunto de puntos $x^\mu(s,t)\in M$.
Tenemos dos campos vectoriales naturales: los vectores tangentes a las geodésicas,
\begin{equation}
T^\mu = {{\partial x^\mu}\over{\partial t}}\ ,\label{3.107}
\end{equation}
y los ``vectores de desviación''
\begin{equation}
S^\mu = {{\partial x^\mu}\over{\partial s}}\,.\label{3.108}
\end{equation}
Este nombre deriva de la noción informal de que $S^\mu$ apunta desde una geodésica hacia las vecinas.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/three14.pdf}
\end{figure}

La idea de que $S^\mu$ apunta de una geodésica a la siguiente nos inspira a definir la ``velocidad relativa de las geodésicas''.
\begin{equation}
V^\mu = (\nabla_TS)^\mu = T^\rho\nabla_\rho S^\mu\ ,\label{3.109}
\end{equation}
y la ``aceleración relativa de las geodésicas'',
\begin{equation}
a^\mu = (\nabla_T V)^\mu =T^\rho\nabla_\rho V^\mu\,.\label{3.110}
\end{equation}
Deberías tomar los nombres con cautela, pero estos vectores ciertamente están bien definidos.

Dado que $S$ y $T$ son vectores base adaptados a un sistema de coordenadas, su conmutador desaparece:
\begin{equation*}
[S,T]=0\,.
\end{equation*}
Nos gustaría considerar el caso convencional donde la torsión desaparece, por lo que de (3.70) tenemos
\begin{equation}
S^\rho\nabla_\rho T^\mu = T^\rho\nabla_\rho S^\mu \,.\label{3.111}
\end{equation}
Teniendo esto en cuenta, calculemos la aceleración:
\begin{align}
a^\mu &=  T^\rho\nabla_\rho(T^\sigma\nabla_\sigma S^\mu) \notag \\
&=  T^\rho\nabla_\rho (S^\sigma\nabla_\sigma T^\mu) \notag \\
&= (T^\rho\nabla_\rho S^\sigma)(\nabla_\sigma T^\mu) +
T^\rho S^\sigma\nabla_\rho\nabla_\sigma T^\mu \notag \\
&= (S^\rho\nabla_\rho T^\sigma)(\nabla_\sigma T^\mu) +
T^\rho S^\sigma(\nabla_\sigma\nabla_\rho T^\mu
+R^\mu{}_{\nu\rho\sigma}T^\nu) \notag \\
&=  (S^\rho\nabla_\rho T^\sigma)(\nabla_\sigma T^\mu) +
S^\sigma\nabla_\sigma(T^\rho\nabla_\rho T^\mu)
-(S^\sigma\nabla_\sigma T^\rho)\nabla_\rho T^\mu
+R^\mu{}_{\nu\rho\sigma}T^\nu T^\rho S^\sigma \notag \\
&=  R^\mu{}_{\nu\rho\sigma}T^\nu T^\rho S^\sigma\,.
\label{3.112}
\end{align}
Pensemos en esto línea por línea.
La primera línea es la definición de $a^\mu$ y la segunda línea proviene directamente de (3.111).
La tercera línea es simplemente la regla de Leibniz.
La cuarta línea reemplaza una derivada covariante doble por las derivadas en orden opuesto más el tensor de Riemann.
En la quinta línea usamos Leibniz nuevamente (en el orden opuesto al habitual), y luego cancelamos dos términos idénticos y notamos que el término que involucra a $T^\rho\nabla_\rho T^\mu$ desaparece porque $T^\mu$ es el vector tangente a una geodésica.
El resultado,
\begin{equation}
a^\mu = {{D^2}\over{dt^2}}S^\mu = R^\mu{}_{\nu\rho\sigma}T^\nu
T^\rho S^\sigma\ ,\label{3.113}
\end{equation}
se conoce como {\bf ecuación de desviación geodésica}.
Expresa algo que podríamos haber esperado: la aceleración relativa entre dos geodésicas vecinas es proporcional a la curvatura.

Físicamente, por supuesto, la aceleración de las geodésicas vecinas se interpreta como una manifestación de fuerzas de marea gravitacionales.
Esto nos recuerda que ya estamos muy cerca de hacer física.

Hay un último aspecto de formalismo que sería bueno abordar antes de pasar a la gravitación propiamente dicha.
Lo que haremos será considerar una vez más (aunque mucho más concisamente) el formalismo de las conexiones y la curvatura, pero esta vez usaremos conjuntos de vectores base en el espacio tangente que \textit{no} se derivan de ningún sistema de coordenadas.
Resultará que este ligero cambio de énfasis revela un punto de vista diferente sobre la conexión y la curvatura, uno en el que la relación con las teorías de calibre en física de partículas es mucho más transparente.
De hecho, los conceptos que se van a introducir son muy sencillos, pero el tema es una pesadilla notacional, por lo que parece más difícil de lo que realmente es.

Hasta ahora hemos estado aprovechando el hecho de que una base natural para el espacio tangente $T_p$ en un punto $p$ viene dada por las derivadas parciales con respecto a las coordenadas en ese punto, $\e\mu = \p\mu$.
De manera similar, una base para el espacio cotangente $T^*_p$ viene dada por los gradientes de las funciones de coordenadas, $ \ztheta{\mu} =  d\,x^\mu$.
Pero nada nos impide establecer las bases que queramos.
Imaginemos, por tanto, que en cada punto de la variedad introducimos un conjunto de vectores base $\e{a}$ (indexados por una letra latina en lugar de griega, para recordarnos que no están relacionados con ningún sistema de coordenadas).
Elegiremos estos vectores base para que sean ``ortonormales'', en un sentido que sea apropiado para la firma de la variedad en la que estamos trabajando.
Es decir, si la forma canónica de la métrica se escribe $\eta_{ab}$, exigimos que el producto interno de nuestros vectores base sea
\begin{equation}
g(\e{a},\e{b}) = \eta_{ab}\ ,\label{3.114}
\end{equation}
donde $g(~,~)$ es el tensor métrico habitual.
Así, en un Espacio-Tiempo lorentziano $\eta_{ab}$ representa la métrica de Minkowski, mientras que en un espacio con métrica definida positiva representaría la métrica euclidiana.
El conjunto de vectores que comprende una base ortonormal se conoce a veces como {\bf tétrada} (del griego {\it tetras}, ``un grupo de cuatro'') o {\bf vielbein} (del alemán ``muchos patas'').
En diferentes números de dimensiones, ocasionalmente se convierte en {\it vierbein} (cuatro), {\it dreibein} (tres), {\it zweibein} (dos), y así sucesivamente.
(Así como en general no podemos encontrar gráficos de coordenadas que cubran toda la variedad, a menudo no podremos encontrar un único conjunto de campos vectoriales de base suave que estén definidos en todas partes.
Como de costumbre, podemos superar este problema trabajando en diferentes parches y asegurándonos de que todo se comporta bien en las superposiciones).

El objetivo de tener una base es que cualquier vector se puede expresar como una combinación lineal de vectores base.
Específicamente, podemos expresar nuestros antiguos vectores base $\e\mu = \p\mu$ en términos de los nuevos:
\begin{equation}
\e\mu = e^a_\mu\e{a}\,.\label{3.115}
\end{equation}
Los componentes $e^a_\mu$ forman una matriz invertible $n\times n$.
(De acuerdo con nuestra práctica habitual de difuminar la distinción entre objetos y sus componentes, nos referiremos a $e^a_\mu$ como la tétrada o vielbein, y a menudo en plural como ``vielbeins''.) Denotamos su inverso cambiando índices para obtener $e^\mu_a$, que satisfacen
\begin{equation}
e^\mu_a e^a_\nu=\delta^\mu_\nu\ ,\qquad
e^a_\mu e^\mu_b = \delta^a_b\,.\label{3.116}
\end{equation}
Estos sirven como componentes de los vectores $\e{a}$ en la base de coordenadas:
\begin{equation}
\e{a} = e^\mu_a \e\mu\,.\label{3.117}
\end{equation}
En términos de los vielbeins inversos, (3.114) se convierte en
\begin{equation}
g_{\mu\nu} e^\mu_a e^\nu_b = \eta_{ab}\ ,\label{3.118}
\end{equation}
o equivalente
\begin{equation}
g_{\mu\nu} = e_\mu^a e_\nu^b \eta_{ab}\,.\label{3.119}
\end{equation}
Esta última ecuación a veces lleva a la gente a decir que los vielbeins son la ``raíz cuadrada'' de la métrica.

De manera similar, podemos establecer una base ortonormal de formas unitarias en $T^*_p$, que denotamos $ \ztheta{a}$.
Pueden elegirse para que sean compatibles con los vectores base, en el sentido de que
\begin{equation}
 \ztheta{a}(\e{b}) = \delta^a_b\,.\label{3.120}
\end{equation}
Una consecuencia inmediata de esto es que las formas uniformes ortonormales están relacionadas con sus primos basados en coordenadas $ \ztheta{\mu} =  d\,x^\mu$ por
\begin{equation}
 \ztheta{\mu} = e^\mu_a  \ztheta{a}\label{3.121}
\end{equation}
y
\begin{equation}
 \ztheta{a} = e^a_\mu  \ztheta{\mu}\,.\label{3.122}
\end{equation}
Los vielbeins $e^a_\mu$ cumplen así una doble función como componentes de los vectores de la base de coordenadas en términos de los vectores de la base ortonormal, y como componentes de las formas únicas de la base ortonormal en términos de las formas únicas de la base de coordenadas; mientras que los vielbeins inversos sirven como componentes de los vectores de la base ortonormal en términos de la base de coordenadas, y como componentes de las formas únicas de la base de coordenadas en términos de la base ortonormal.

Cualquier otro vector se puede expresar en términos de sus componentes en base ortonormal.
Si un vector $V$ se escribe en base de coordenadas como $V^\mu\e\mu$ y en base ortonormal como $V^a\e{a}$, los conjuntos de componentes estarán relacionados por
\begin{equation}
V^a = e^a_\mu V^\mu\,.\label{3.123}
\end{equation}
Así que los vielbeins nos permiten ``cambiar de índices latinos a griegos y viceversa''. La agradable propiedad de los tensores, de que normalmente sólo hay una cosa sensata que hacer según la ubicación de los índices, es de gran ayuda en este caso.
Podemos pasar a referirnos a tensores de índices múltiples en cualquiera de las dos bases, o incluso en términos de componentes mixtos:
\begin{equation}
V^a{}_b = e^a_\mu V^\mu{}_b = e^\nu_b V^a{}_\nu =
e^a_\mu e^\nu_b V^\mu{}_\nu \,.\label{3.124}
\end{equation}
Volviendo a (3.118), vemos que los componentes del tensor métrico en base ortonormal son solo los de la métrica plana, $\eta_{ab}$.
(Por esta razón, a veces se hace referencia a los índices griegos como ``curvos'' y a los latinos como ``planos''.) De hecho, podemos llegar a subir y bajar los índices latinos utilizando la métrica plana y su inversa $\eta^{ab}$.
Puede comprobar usted mismo que todo funciona bien ({\it eg}, que la reducción de un índice con la métrica conmuta con el cambio de bases ortonormales a bases de coordenadas).

Al introducir un nuevo conjunto de vectores base y formas unitarias, es necesario volver a nuestro tema favorito de las propiedades de transformación.
Hemos tenido cuidado todo el tiempo de enfatizar que la ley de transformación del tensor fue sólo un resultado indirecto de una transformación de coordenadas; el verdadero problema era un cambio de base.
Ahora que tenemos bases sin coordenadas, estas bases se pueden cambiar independientemente de las coordenadas.
La única restricción es que se preserve la propiedad de ortonormalidad (3.114).
Pero sabemos qué tipo de transformaciones preservan la métrica plana: en una métrica de firma euclidiana son transformaciones ortogonales, mientras que en una métrica de firma lorentziana son transformaciones de Lorentz.
Por lo tanto consideramos cambios de base de la forma
\begin{equation}
\e{a}\rightarrow \e{a'} = \Lambda_{a'}{}^a(x) \e{a}\ ,\label{3.125}
\end{equation}
donde las matrices $\Lambda_{a'}{}^a(x)$ representan transformaciones dependientes de la posición que (en cada punto) dejan inalterada la forma canónica de la métrica:
\begin{equation}
\Lambda_{a'}{}^a\Lambda_{b'}{}^b\eta_{ab} = \eta_{a'b'}\,.
\label{3.126}
\end{equation}
De hecho, estas matrices corresponden a lo que en el espacio plano llamamos transformaciones inversas de Lorentz (que operan sobre vectores base); Como antes, también tenemos transformaciones de Lorentz ordinarias $\Lambda^{a'}{}_a$, que transforman las formas uniformes de la base.
En lo que respecta a los componentes, como antes transformamos los índices superiores con $\Lambda^{a'}{}_a$ y los índices inferiores con $\Lambda_{a'}{}^a$.

Así que ahora tenemos la libertad de realizar una transformación de Lorentz (o una rotación euclidiana ordinaria, según la firma) en cada punto del espacio.
Por lo tanto, estas transformaciones se denominan {\bf transformaciones locales de Lorentz} o LLT.
Todavía tenemos nuestra libertad habitual para realizar cambios en las coordenadas, que se denominan {\bf transformaciones de coordenadas generales}, o GCT.
Ambos pueden suceder al mismo tiempo, lo que resulta en una ley de transformación tensorial mixta:
\begin{equation}
T^{a'\mu'}{}_{b'\nu'} = \Lambda^{a'}{}_a {{\partial x^{\mu'}}\over
{\partial x^\mu}} \Lambda_{b'}{}^b {{\partial x^{\nu}}\over
{\partial x^{\nu'}}}T^{a\mu}{}_{b\nu}\,.\label{3.127}
\end{equation}

Traducir lo que sabemos sobre tensores a bases no coordinadas es, en su mayor parte, simplemente una cuestión de colocar vielbeins en los lugares correctos.
La excepción crucial llega cuando comenzamos a diferenciar las cosas.
En nuestro formalismo ordinario, la derivada covariante de un tensor viene dada por su derivada parcial más los términos de corrección, uno para cada índice, que involucran el tensor y los coeficientes de conexión.
El mismo procedimiento seguirá siendo válido para la base no coordinada, pero reemplazamos los coeficientes de conexión ordinarios $\Gamma^\lambda_\mn$ por la {\bf conexión de espín}, denotada como $\omega_\mu{}^a{}_b$.
Cada índice latino obtiene un factor de la conexión de espín de la forma habitual:
\begin{equation}
\nabla_\mu X^a{}_b=\partial_\mu X^a{}_b+\omega_\mu{}^a{}_cX^c{}_b
-\omega_\mu{}^c{}_b X^a{}_c\,.\label{3.128}
\end{equation}
(El nombre ``conexión de espín'' proviene del hecho de que puede usarse para tomar derivadas covariantes de espinores, lo que en realidad es imposible usando los coeficientes de conexión convencionales.)
En presencia de índices mixtos latinos y griegos obtenemos términos de ambos tipos.

La exigencia habitual de que un tensor sea independiente de la forma en que está escrito nos permite derivar una relación entre la conexión de espín, los vielbeins y los $\Gamma^\nu_{\mu\lambda}$.
Considere la derivada covariante de un vector $X$, primero en una base puramente coordinada:
\begin{align}
\nabla X &=  (\nabla_\mu X^\nu) d\,x^\mu\otimes
\partial_\nu \notag \\  &= (\partial_\mu X^\nu +\Gamma^\nu_{\mu\lambda}
X^\lambda) d\,x^\mu\otimes \partial_\nu\,. \label{3.129}
\end{align}
Ahora busque el mismo objeto en una base mixta y conviértalo a la base de coordenadas:
\begin{align}
\nabla X &=  (\nabla_\mu X^a) d\,x^\mu\otimes
\e{a} \notag \\   &= (\partial_\mu X^a +\omega_\mu{}^a{}_b X^b) d\,x^\mu
\otimes \e{a} \notag \\
&= (\partial_\mu(e^a_\nu X^\nu)+\omega_\mu{}^a{}_b e^b_\lambda
X^\lambda) d\,x^\mu \otimes(e^\sigma_a \partial_\sigma) \notag \\
&= e^\sigma_a(e^a_\nu \partial_\mu X^\nu +X^\nu \partial_\mu
e^a_\nu +\omega_\mu{}^a{}_b e^b_\lambda X^\lambda) d\,x^\mu \otimes
\partial_\sigma \notag \\
&= (\partial_\mu X^\nu+e^\nu_a \partial_\mu e^a_\lambda X^\lambda
+e^\nu_a e^b_\lambda \omega_\mu{}^a{}_b X^\lambda) d\,x^\mu \otimes
\partial_\nu \,. \label{3.130}
\end{align}
La comparación con (3.129) revela
\begin{equation}
\Gamma^\nu_{\mu\lambda}=e^\nu_a \partial_\mu e^a_\lambda
+e^\nu_a e^b_\lambda \omega_\mu{}^a{}_b\ ,\label{3.131}
\end{equation}
o equivalente
\begin{equation}
\omega_\mu{}^a{}_b = e^a_\nu e^\lambda_b \Gamma^\nu_{\mu\lambda}
- e^\lambda_b\p\mu e^a_\lambda\,.\label{3.132}
\end{equation}
Un poco de manipulación nos permite escribir esta relación como la desaparición de la derivada covariante del vielbein,
\begin{equation}
\nabla_\mu e_\nu^a=0\ ,\label{3.133}
\end{equation}
que a veces se conoce como el ``postulado de la tétrada''. Tenga en cuenta que esto siempre es cierto; no necesitábamos asumir nada sobre la conexión para derivarla.
Específicamente, no necesitábamos asumir que la conexión fuera compatible con el sistema métrico o que estuviera libre de torsión.

Dado que se puede considerar la conexión como algo que necesitamos para fijar la ley de transformación de la derivada covariante, no debería sorprender que la conexión de espín no obedezca en sí misma la ley de transformación del tensor.
En realidad, bajo el GCT, el índice griego inferior se transforma de la manera correcta, como una forma única.
Pero bajo LLT la conexión de espín se transforma de manera no homogénea, como
\begin{equation}
\omega_\mu{}^{a'}{}_{b'} = \Lambda^{a'}{}_a\Lambda_{b'}{}^b
\omega_\mu{}^a{}_b - \Lambda_{b'}{}^c\p\mu\Lambda^{a'}{}_c
\,.\label{3.134}
\end{equation}
Le recomendamos que compruebe usted mismo que esto da como resultado la transformación adecuada de la derivada covariante.

Hasta ahora no hemos hecho más que un formalismo vacío, traduciendo cosas que ya sabíamos a una nueva notación.
Pero el trabajo que estamos haciendo nos permite conseguir dos cosas.
La primera, a la que ya hemos aludido, es la capacidad de describir campos de espinores en el Espacio-Tiempo y tomar sus derivadas covariantes; en esta ocasión, no exploraremos esto más a fondo.
El segundo es un cambio de punto de vista, en el que podemos pensar en varios tensores como formas diferenciales con valores tensoriales.
Por ejemplo, un objeto como $X_\mu{}^a$, que consideramos un tensor $(1,1)$ escrito con índices mixtos, también puede considerarse como una ``forma única con valor vectorial''. Tiene un índice griego inferior, entonces lo consideramos como una forma única, pero para cada valor del índice inferior es un vector.
Análogamente, un tensor $A_{\mu\nu}{}^{a}{}_b$, antisimétrico en $\mu$ y $\nu$, puede considerarse como una ``dos-forma tensor-valorada $(1,1)$''. Así, cualquier tensor con cierto número de índices griegos inferiores antisimétricos y cierto número de índices latinos puede considerarse como una forma diferencial, pero tomando valores en el haz tensorial.
(Las formas diferenciales ordinarias son simplemente formas con valores escalares).
La utilidad de este punto de vista surge cuando consideramos los derivados exteriores.
Si queremos pensar en $X_\mu{}^a$ como una forma única con valor vectorial, estamos tentados a tomar su derivada exterior:
\begin{equation}
( d\,X)_{\mn}{}^a = \p\mu X_\nu{}^a - \p\nu X_\mu{}^a\,.\label{3.135}
\end{equation}
Es fácil comprobar que este objeto se transforma como una dos formas (es decir, según la ley de transformación para tensores $(0,2)$) bajo GCT, pero no como un vector bajo LLT (las transformaciones de Lorentz dependen de la posición, lo que introduce una término no homogéneo en la ley de transformación).
Pero podemos solucionar este problema mediante un uso sensato de la conexión de espín, que puede considerarse como una forma única.
(No es una forma única con valor tensorial, debido a la ley de transformación no tensorial (3.134).)
Así, el objeto
\begin{equation}
( d\,X)_{\mn}{}^a +(\omega\wedge X)_{\mn}{}^a
= \p\mu X_\nu{}^a - \p\nu X_\mu{}^a
+\omega_\mu{}^a{}_b X_\nu{}^b - \omega_\nu{}^a{}_b X_\mu{}^b
\ ,\label{3.136}
\end{equation}
como puedes comprobar en casa, se transforma como un tensor propio.

Una aplicación inmediata de este formalismo es a las expresiones de torsión y curvatura, los dos tensores que caracterizan cualquier conexión dada.
La torsión, con dos índices inferiores antisimétricos, se puede considerar como una forma de dos valores vectoriales $T_{\mn}{}^a$.
La curvatura, que siempre es antisimétrica en sus dos últimos índices, es una forma de dos valores tensoriales $(1,1)$, $R^a{}_{b\mn}$.
Usando nuestra libertad para suprimir índices en formas diferenciales, podemos escribir las relaciones que definen estos dos tensores como
\begin{equation}
T^a =  d\,e^a + \omega^a{}_b\wedge e^b\label{3.137}
\end{equation}
y
\begin{equation}
R^a{}_b =  d\, \omega^a{}_b + \omega^a{}_c\wedge\omega^c{}_b\,.
\label{3.138}
\end{equation}
Éstas se conocen como {\bf ecuaciones estructurales de Maurer-Cartan}.
Son equivalentes a las definiciones habituales; Hagamos el ejercicio de mostrar esto para la torsión y podrás comprobar la curvatura por ti mismo.
Tenemos
\begin{align}
T_\mn{}^\lambda  &=  e^\lambda_a T_\mn{}^a \notag \\
&=  e^\lambda_a(\p\mu e_\nu{}^a - \p\nu e_\mu{}^a
+\omega_\mu{}^a{}_b e_\nu{}^b - \omega_\nu{}^a{}_b e_\mu{}^b) \notag \\
&=  \Gamma^\lambda_{\mn} - \Gamma^\lambda_{\nu\mu}\ ,
\label{3.139}
\end{align}
que es solo la definición original que dimos.
Aquí hemos utilizado (3.131), la expresión para los $\Gamma^\lambda_{\mn}$ en términos de vielbeins y conexión de espín.
También podemos expresar identidades obedecidas por estos tensores como
\begin{equation}
 d\,T^a + \omega^a{}_b\wedge T^b = R^a{}_b\wedge e^b\label{3.140}
\end{equation}
y
\begin{equation}
 d\,R^a{}_b + \omega^a{}_c\wedge R^c{}_b - R^a{}_c\wedge
\omega^c{}_b=0\,.\label{3.141}
\end{equation}
La primera de ellas es la generalización de $R^\rho{}_{[\sigma\mn]}=0$, mientras que la segunda es la identidad de Bianchi $\nabla_{[\lambda|} R^\rho{}_{\sigma |\mn]}=0$.
(A veces ambas ecuaciones se denominan identidades de Bianchi).

La forma de estas expresiones conduce a una tentación casi irresistible de definir una ``derivada exterior covariante'', que actúa en una forma tensorial tomando la derivada exterior ordinaria y luego añadiendo los términos apropiados con la conexión de espín, uno para cada Índice latino.
Aunque no haremos eso aquí, está bien ceder a esta tentación y, de hecho, el lado derecho de (3.137) y el lado izquierdo de (3.140) y (3.141) pueden considerarse como tales. Derivadas covariantes exteriores.
Pero tengamos cuidado, ya que (3.138) no puede; no se puede tomar ningún tipo de derivada covariante de la conexión de espín, ya que no es un tensor.

Hasta ahora nuestras ecuaciones han sido válidas para conexiones generales; veamos qué obtenemos de la conexión Christoffel.
El requisito sin torsión es simplemente que (3.137) desaparece; Esto no conduce inmediatamente a ninguna afirmación sencilla sobre los coeficientes de la conexión de espín.
La compatibilidad de la métrica se expresa como la desaparición de la derivada covariante de la métrica: $\nabla g=0$.
Podemos ver a qué conduce esto cuando expresamos la métrica en base ortonormal, donde sus componentes son simplemente $\eta_{ab}$:
\begin{align}
\nabla_\mu \eta_{ab} &= \partial_\mu \eta_{ab}
-\omega_\mu{}^c{}_a \eta_{cb}-\omega_\mu{}^c{}_b \eta_{ac} \notag \\
&= -\omega_{\mu ab}-\omega_{\mu ba}\,. \label{3.142}
\end{align}
Entonces igualar esto a cero implica
\begin{equation}
\omega_{\mu ab}=-\omega_{\mu ba}\,.\label{3.143}
\end{equation}
Por tanto, la compatibilidad métrica es equivalente a la antisimetría de la conexión de espín en sus índices latinos.
(Como antes, tal afirmación sólo tiene sentido si ambos índices están arriba o abajo.)
Estas dos condiciones juntas nos permiten expresar la conexión de espín en términos de vielbeins.
Existe una fórmula explícita que expresa esta solución, pero en la práctica es más fácil resolver simplemente la condición libre de torsión.
\begin{equation}
\omega^{ab}\wedge e_b = - d\,e^a\ ,\label{3.144}
\end{equation}
utilizando la asimetría de la conexión de espín, para encontrar los componentes individuales.

Ahora tenemos los medios para comparar el formalismo de las conexiones y la curvatura en la geometría de Riemann con el de las teorías de calibre en la física de partículas.
(Este es un comentario aparte que, con suerte, será comprensible para todos, pero no es un ingrediente esencial del curso).
En ambas situaciones, los campos de interés viven en espacios vectoriales asignados a cada punto del Espacio-Tiempo.
En la geometría de Riemann, los espacios vectoriales incluyen el espacio tangente, el espacio cotangente y los espacios tensoriales superiores construidos a partir de estos.
Por otra parte, en las teorías de calibre nos ocupamos de los espacios vectoriales ``internos''.
La distinción es que el espacio tangente y sus parientes están íntimamente asociados con la variedad misma y se definieron naturalmente una vez que se estableció la variedad; un espacio vectorial interno puede ser de cualquier dimensión que queramos y debe definirse como una adición independiente a la variedad.
En la jerga matemática, la unión de la variedad base con los espacios vectoriales internos (definidos en cada punto) es un {\bf haz de fibras}, y cada copia del espacio vectorial se llama ``fibra'' (en perfecto acuerdo con nuestra definición del paquete tangente).

Además de la variedad base (para nosotros, el Espacio-Tiempo) y las fibras, el otro ingrediente importante en la definición de un haz de fibras es el ``grupo estructural'', un grupo de Lie que actúa sobre las fibras para describir cómo se cosen entre sí. parches de coordenadas superpuestas.
Sin entrar en detalles, el grupo estructural del paquete tangente en un Espacio-Tiempo de cuatro dimensiones es generalmente GL$(4,\R)$, el grupo de matrices reales invertibles $4\times 4$; si tenemos una métrica lorentziana, esta puede reducirse al grupo de Lorentz SO$(3,1)$.
Ahora imagine que introducimos un espacio vectorial tridimensional interno y cosimos las fibras con rotaciones ordinarias; el grupo de estructura de este nuevo paquete es entonces SO$(3)$.
Un campo que se encuentra en este paquete podría denominarse $\phi^A(x^\mu)$, donde $A$ va del uno al tres; es un vector de tres (uno interno, no relacionado con el Espacio-Tiempo) para cada punto de la variedad.
Tenemos libertad para elegir la base de las fibras de la forma que queramos; esto significa que las ``cantidades físicas'' deben dejarse invariantes bajo transformaciones locales SO(3) como
\begin{equation}
\phi^A(x^\mu)\rightarrow \phi^{A'}(x^\mu)=
O^{A'}{}_A(x^\mu)\phi^A(x^\mu)\ ,\label{3.145}
\end{equation}
donde $O^{A'}{}_A(x^\mu)$ es una matriz en SO(3) que depende del Espacio-Tiempo.
Estas transformaciones se conocen como {\bf transformaciones de calibre}, y las teorías invariantes bajo ellas se denominan ``teorías de calibre''.

En su mayor parte, no es difícil organizar las cosas de manera que las cantidades físicas sean invariantes bajo transformaciones de calibre.
La única dificultad surge cuando consideramos las derivadas parciales, $\p\mu \phi^A$.
Debido a que la matriz $O^{A'}{}_A(x^\mu)$ depende del Espacio-Tiempo, contribuirá con un término no deseado a la transformación de la derivada parcial.
A estas alturas ya deberías poder adivinar la solución: introduce una conexión para corregir el término no homogéneo en la ley de transformación.
Por lo tanto, definimos una conexión en el haz de fibras como un objeto $A_\mu{}^A{}_B$, con dos ``índices de grupo'' y un índice de Espacio-Tiempo.
Bajo GCT se transforma como una forma única, mientras que bajo transformaciones de calibre se transforma como
\begin{equation}
A_\mu{}^{A'}{}_{B'} = O^{A'}{}_A O_{B'}{}^B A_\mu{}^A{}_B
- O_{B'}{}^C\p\mu O^{A'}{}_C\,.\label{3.146}
\end{equation}
(Cuidado: nuestras convenciones son tan drásticamente diferentes de las de la literatura sobre física de partículas que ni siquiera intentaré aclararlas).
Con esta ley de transformación, la ``derivada covariante de calibre''
\begin{equation}
D_\mu \phi^A = \p\mu\phi^A + A_\mu{}^A{}_B\phi^B\label{3.147}
\end{equation}
se transforma ``tensorialmente'' bajo transformaciones de calibre, como puede comprobar.
(En el electromagnetismo ordinario, la conexión es simplemente el potencial vectorial convencional.
No son necesarios índices, porque el grupo de estructura U(1) es unidimensional.)

Está claro que esta noción de conexión en un haz de fibras internas está muy estrechamente relacionada con la conexión en el haz tangente, especialmente en la imagen del sistema ortonormal que hemos estado analizando.
La ley de transformación (3.146), por ejemplo, es exactamente la misma que la ley de transformación (3.134) para la conexión de espín.
También podemos definir un tensor de curvatura o ``intensidad de campo'' que tiene dos formas,
\begin{equation}
F^A{}_B =  d\,A^A{}_B + A^A{}_C\wedge A^C{}_B\ ,\label{3.148}
\end{equation}
en correspondencia exacta con (3.138).
Podemos transportar cosas en paralelo a lo largo de caminos, y existe una construcción análoga al propagador paralelo; la traza de la matriz obtenida transportando en paralelo un vector alrededor de una curva cerrada se llama ``bucle de Wilson''.

Podríamos continuar con el desarrollo de la relación entre el fibrado tangente y los fibrados vectoriales internos, pero el tiempo apremia y tenemos otros asuntos que resolver.
En lugar de ello, terminemos destacando la importante diferencia entre las dos construcciones.
La diferencia surge del hecho de que el haz tangente está estrechamente relacionado con la variedad base, mientras que otros haces de fibras se añaden después.
Tiene sentido decir que un vector en el espacio tangente en $p$ ``apunta a lo largo de un camino'' que pasa por $p$; pero esto no tiene sentido para un paquete de vectores interno.
Por lo tanto, no existe un análogo de la base de coordenadas para un espacio interno; las derivadas parciales a lo largo de las curvas no tienen nada que ver con los vectores internos.
De ello se deduce, a su vez, que no hay nada como los vielbeins, que relacionan bases ortonormales con bases coordinadas.
El tensor de torsión, en particular, sólo se define para una conexión en el haz tangente, no para ninguna conexión de teoría de calibre; se puede considerar como la derivada exterior covariante del vielbein, y tal construcción no está disponible en un paquete interno.
Se debe apreciar la relación entre los distintos usos de la noción de conexión, sin dejarse llevar.





\chapter{Gravitación}
%\addcontentsline{toc}{chapter}{Gravitación}


Habiendo pagado nuestras deudas matemáticas, ahora estamos preparados para examinar la física de la gravitación tal como la describe la Relatividad General.
Este tema se divide naturalmente en dos partes: cómo la curvatura del Espacio-Tiempo actúa sobre la materia para manifestarse como ``gravedad'', y cómo la energía y el momentum influyen en el Espacio-Tiempo para crear la curvatura.
En cualquier caso, sería legítimo empezar desde arriba, estableciendo claramente las leyes que gobiernan la física en el Espacio-Tiempo curvo y calculando sus consecuencias.
En lugar de ello, intentaremos ser un poco más motivadores, empezando por los principios físicos básicos e intentando argumentar que éstos conducen naturalmente a una teoría física casi única.

El más básico de estos principios físicos es el Principio de Equivalencia, que se presenta en diversas formas.
La forma más antigua data de Galileo y Newton y se conoce como {\bf Principio de Equivalencia Débil} o WEP.
El WEP establece que la ``masa inercial'' y la ``masa gravitacional'' de cualquier objeto son iguales.
Para ver lo que esto significa, pensemos en la Segunda Ley de Newton.
Este relaciona la fuerza ejercida sobre un objeto con la aceleración que sufre, poniéndolos proporcionales entre sí siendo la constante de proporcionalidad la masa inercial $m_i$:
\begin{equation}
{\bf f} = m_i {\bf a}\,.\label{4.1}
\end{equation}
La masa inercial claramente tiene un carácter universal, relacionado con la resistencia que sientes cuando intentas empujar el objeto; es la misma constante sin importar qué tipo de fuerza se ejerza.
También tenemos la ley de la gravitación, que establece que la fuerza gravitacional ejercida sobre un objeto es proporcional al gradiente de un campo escalar $\Phi$, conocido como potencial gravitacional.
La constante de proporcionalidad en este caso se llama masa gravitacional $m_g$:
\begin{equation}
{\bf f}_g = -m_g \nabla\Phi\,.\label{4.2}
\end{equation}
A primera vista, $m_g$ tiene un carácter muy diferente al de $m_i$; es una cantidad específica de la fuerza gravitacional.
Si se quiere, es la ``carga gravitacional'' del cuerpo.
Sin embargo, Galileo demostró hace mucho tiempo (apócrifamente arrojando pesas desde la Torre Inclinada de Pisa, en realidad haciendo rodar bolas por planos inclinados) que la respuesta de la materia a la gravitación era universal: todos los objetos caen a la misma velocidad en un campo gravitacional. , independiente de la composición del objeto.
En la mecánica newtoniana esto se traduce en el WEP, que es simplemente
\begin{equation}
m_i = m_g\label{4.3}
\end{equation}
para cualquier objeto.
Una consecuencia inmediata es que el comportamiento de las partículas de prueba en caída libre es universal, independientemente de su masa (o de cualquier otra cualidad que puedan tener); de hecho tenemos
\begin{equation}
{\bf a}= - \nabla\Phi\,.\label{4.4}
\end{equation}

La universalidad de la gravitación, tal como la implica el WEP, puede expresarse de otra forma, más popular.
Imaginemos que un físico está encerrado en una caja herméticamente cerrada, incapaz de observar el mundo exterior, y que está realizando experimentos con el movimiento de partículas de prueba, por ejemplo para medir el campo gravitacional local.
Por supuesto, obtendría respuestas diferentes si la caja estuviera en la Luna o en Júpiter que si estuviera en la Tierra.
Pero las respuestas también serían diferentes si la caja estuviera acelerando a una velocidad constante; esto cambiaría la aceleración de las partículas en caída libre con respecto a la caja.
El WEP implica que no hay manera de separar los efectos de un campo gravitacional de los de estar en un marco de aceleración uniforme, simplemente observando el comportamiento de las partículas en caída libre.
Esto se desprende de la universalidad de la gravitación; sería posible distinguir entre aceleración uniforme y campo electromagnético, observando el comportamiento de partículas con diferentes cargas.
Pero con la gravedad es imposible, ya que la ``carga'' es necesariamente proporcional a la masa (inercial).

Para ser prudentes, deberíamos limitar nuestras afirmaciones sobre la imposibilidad de distinguir la gravedad de la aceleración uniforme restringiendo nuestra atención a ``regiones suficientemente pequeñas del espaciotiempo''. Si la caja sellada fuera suficientemente grande, el campo gravitatorio cambiaría de un lugar a otro de forma observable, mientras que el efecto de la aceleración se produce siempre en la misma dirección.
En un cohete o en un ascensor, las partículas siempre caen hacia abajo:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/four01.pdf}
\end{figure}

\noindent
Sin embargo, en una caja muy grande en un campo gravitacional, las partículas se moverán hacia el centro de la Tierra (por ejemplo), lo que podría tener una dirección diferente en diferentes regiones:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/four02.pdf}
\end{figure}

\noindent
Por lo tanto, el WEP puede expresarse como ``las leyes de las partículas en caída libre son las mismas en un campo gravitacional y en un marco uniformemente acelerado, en regiones suficientemente pequeñas del Espacio-Tiempo''. campo, lo que conducirá a fuerzas de marea que pueden detectarse.

Después del advenimiento de la Relatividad Especial, el concepto de masa perdió parte de su singularidad, ya que quedó claro que la masa era simplemente una manifestación de energía y momentum ($E=mc^2$ y todo eso).
Por tanto, era natural que Einstein pensara en generalizar el WEP a algo más inclusivo.
Su idea era simplemente que el físico de la caja no debería tener forma alguna de distinguir entre aceleración uniforme y un campo gravitacional externo, sin importar qué experimentos hiciera (no sólo arrojando partículas de prueba).
Esta extrapolación razonable se convirtió en lo que ahora se conoce como el {\bf Principio de Equivalencia de Einstein}, o EEP: ``En regiones suficientemente pequeñas del Espacio-Tiempo, las leyes de la física se reducen a las de la Relatividad Especial; Es imposible detectar la existencia de un campo gravitacional.''

De hecho, es difícil imaginar teorías que respeten el WEP pero violen el EEP.
Consideremos un átomo de hidrógeno, un estado unido de un protón y un electrón.
Su masa es en realidad menor que la suma de las masas del protón y el electrón considerados individualmente, porque hay una energía de enlace negativa: hay que poner energía en el átomo para separar el protón y el electrón.
Según el WEP, la masa gravitacional del átomo de hidrógeno es, por tanto, menor que la suma de las masas de sus constituyentes; el campo gravitacional se acopla al electromagnetismo (que mantiene unido al átomo) exactamente de la manera correcta para hacer que la masa gravitacional salga bien.
Esto significa que no sólo la gravedad debe acoplarse universalmente a la masa en reposo, sino a todas las formas de energía y momentum, lo cual es prácticamente lo que afirma el EEP.
Sin embargo, es posible encontrar contraejemplos; por ejemplo, podríamos imaginar una teoría de la gravedad en la que las partículas que caen libremente comenzaran a girar a medida que se movían a través de un campo gravitacional.
Entonces podrían caer por las mismas trayectorias que lo harían en un marco acelerado (satisfaciendo así el WEP), pero aún así se podría detectar la existencia del campo gravitacional (en violación del EEP).
Tales teorías parecen artificiales, pero no existe ninguna ley de la naturaleza que las prohíba.

A veces se hace una distinción entre ``leyes gravitacionales de la física'' y ``leyes no gravitacionales de la física'', y se define que la EEP se aplica sólo a estas últimas.
Luego se define el ``Principio de Equivalencia Fuerte'' (SEP) para incluir todas las leyes de la física, gravitacionales y de otro tipo.
No encuentro que ésta sea una distinción particularmente útil y no la extenderé.
Para nuestros propósitos, el EEP (o simplemente ``el principio de equivalencia'') incluye todas las leyes de la física.

Es la EEP la que implica (o al menos sugiere) que debemos atribuir la acción de la gravedad a la curvatura del Espacio-Tiempo.
Recuerde que en la Relatividad Especial los marcos inerciales desempeñan un papel destacado: si bien no fue posible señalar algún marco de referencia como únicamente ``en reposo'', sí fue posible distinguir una familia de marcos que estaban ``en reposo''. `no acelerado'' (inercial).
Por lo tanto, la aceleración de una partícula cargada en un campo electromagnético se definió de forma única con respecto a estos marcos.
El EEP, por otra parte, implica que la gravedad es ineludible: no existe un ``objeto gravitacionalmente neutro'' con respecto al cual podamos medir la aceleración debida a la gravedad.
De ello se deduce que ``la aceleración debida a la gravedad'' no es algo que pueda definirse de manera confiable y, por lo tanto, es de poca utilidad.

En cambio, tiene más sentido definir ``sin aceleración'' como ``caída libre'', y eso es lo que haremos.
Este punto de vista es el origen de la idea de que la gravedad no es una ``fuerza''; una fuerza es algo que conduce a una aceleración, y nuestra definición de aceleración cero es ``moverse libremente en presencia de cualquier campo gravitacional''. resulta que está por aquí”.

Este paso aparentemente inofensivo tiene profundas implicaciones para la naturaleza del Espacio-Tiempo.
En Relatividad Especial, teníamos un procedimiento para comenzar en algún punto y construir un marco inercial que se extendía a lo largo del Espacio-Tiempo, uniendo varillas rígidas y uniéndoles relojes.
Pero, nuevamente, debido a la falta de homogeneidad en el campo gravitacional, esto ya no es posible.
Si comenzamos en algún estado de caída libre y construimos una estructura grande con varillas rígidas, a cierta distancia los objetos en caída libre parecerán como si estuvieran ``acelerando'' con respecto a este sistema de referencia, como se muestra en la figura de la página siguiente.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/four03.pdf}
\end{figure}

La solución es conservar la noción de marcos inerciales, pero descartar la esperanza de que puedan extenderse de manera única a lo largo del espacio y el tiempo.
En su lugar podemos definir {\bf marcos localmente inerciales}, aquellos que siguen el movimiento de partículas en caída libre en regiones suficientemente pequeñas del espaciotiempo.
(Cada vez que decimos ``regiones suficientemente pequeñas'', los puristas deberían imaginar un procedimiento limitante en el que llevemos el volumen espacio-temporal apropiado a cero.)
Esto es lo mejor que podemos hacer, pero nos obliga a renunciar a muchas cosas.
Por ejemplo, ya no podemos hablar con confianza sobre la velocidad relativa de objetos lejanos, puesto que los sistemas de referencia inerciales apropiados para esos objetos son independientes de los apropiados para nosotros.

Hasta ahora hemos estado hablando estrictamente de física, sin llegar a la conclusión de que el Espacio-Tiempo debería describirse como una variedad curva.
Sin embargo, debería quedar claro por qué tal conclusión es apropiada.
La idea de que las leyes de la Relatividad Especial deben obedecerse en regiones suficientemente pequeñas del Espacio-Tiempo, y además que se pueden establecer marcos inerciales locales en tales regiones, corresponde a nuestra capacidad de construir coordenadas normales de Riemann en cualquier punto de una variedad coordenadas, en las que la métrica toma su forma canónica y los símbolos de Christoffel se desvanecen.
La imposibilidad de comparar velocidades (vectores) en regiones muy separadas corresponde a la dependencia de la trayectoria del transporte paralelo en una variedad curva.
Estas consideraciones fueron suficientes para darle a Einstein la idea de que la gravedad era una manifestación de la curvatura del Espacio-Tiempo.
Pero, de hecho, podemos ser aún más persuasivos.
(Es imposible ``probar'' que la gravedad debe considerarse como una curvatura del Espacio-Tiempo, ya que las hipótesis científicas sólo pueden ser refutadas, nunca verificadas [y ni siquiera realmente refutadas, como ha argumentado Thomas Kuhn].
Pero no hay nada de qué estar insatisfecho con los argumentos de plausibilidad convincentes, si conducen a teorías empíricamente exitosas.)

Consideremos una de las célebres predicciones del EEP, el corrimiento al rojo gravitacional.
Considere dos cajas, separadas por una distancia $z$, que se mueven (lejos de cualquier materia, por lo que suponemos en ausencia de cualquier campo gravitacional) con una aceleración constante $a$.
En el instante $t_0$, el cuadro final emite un fotón de longitud de onda $\lambda_0$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/four04.pdf}
\end{figure}

Las cajas permanecen separadas por una distancia constante, por lo que el fotón llega a la caja principal después de un tiempo $\Delta t = z/c$ en el sistema de referencia de las cajas.
En este tiempo las cajas habrán cogido una velocidad adicional $\Delta v = a\Delta t = az/c$.
Por lo tanto, el fotón que llega a la caja principal será desplazado al rojo por el efecto Doppler convencional en una cantidad
\begin{equation}
{{\Delta \lambda}\over {\lambda_0}}={{\Delta v}\over c} =
{{az}\over{c^2}}\,.\label{4.5}
\end{equation}
(Asumimos que $\Delta v/c$ es pequeño, por lo que solo trabajamos en el primer orden).
Según la EEP, lo mismo debería ocurrir en un campo gravitacional uniforme.
Así que imaginamos una torre de altura $z$ asentada en la superficie de un planeta, con $a_g$ la fuerza del campo gravitacional (lo que Newton habría llamado la ``aceleración debida a la gravedad'').

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/four05.pdf}
\end{figure}

\noindent
Se supone que esta situación es indistinguible de la anterior, desde el punto de vista de un observador en una caja en lo alto de la torre (capaz de detectar el fotón emitido, pero por lo demás incapaz de mirar fuera de la caja).
Por lo tanto, un fotón emitido desde la Tierra con una longitud de onda $\lambda_0$ debe desplazarse al rojo una cantidad
\begin{equation}
{{\Delta \lambda}\over {\lambda_0}}={{a_gz}\over{c^2}}\,.\label{4.6}
\end{equation}
Este es el famoso corrimiento al rojo gravitacional.
Observe que es una consecuencia directa de la EEP, no de los detalles de la Relatividad General.
Ha sido verificado experimentalmente, por primera vez por Pound y Rebka en 1960.
Utilizaron el efecto Mössbauer para medir el cambio de frecuencia en los rayos $\gamma$ mientras viajaban desde el suelo hasta la cima de los Laboratorios Jefferson en Harvard.

La fórmula para el corrimiento al rojo suele expresarse en términos del potencial newtoniano $\Phi$, donde ${\bf a}_g = \nabla\Phi$.
(El signo cambia con respecto a la convención habitual, ya que pensamos en ${\bf a}_g$ como la aceleración del sistema de referencia, no de una partícula con respecto a este sistema de referencia).
Un gradiente no constante de $\Phi$ es como una aceleración que varía en el tiempo, y la velocidad neta equivalente se obtiene integrando el tiempo entre la emisión y la absorción del fotón.
entonces tenemos
\begin{align}
{{\Delta \lambda}\over {\lambda_0}}  &=
{1\over {c}}\int \nabla\Phi\ dt  \notag \notag \\
&=  {1\over {c^2}}\int \p{z}\Phi\ dz  \notag\notag \\
&=  \Delta\Phi\ , \label{4.7}
\end{align}
donde $\Delta\Phi$ es el cambio total en el potencial gravitacional y una vez más hemos establecido $c=1$.
Esta sencilla fórmula para el corrimiento al rojo gravitacional sigue siendo cierta en circunstancias más generales.
Por supuesto, al utilizar el potencial newtoniano, estamos restringiendo nuestro dominio de validez a campos gravitacionales débiles, pero eso suele estar completamente justificado para los efectos observables.

El corrimiento al rojo gravitacional conduce a otro argumento de que deberíamos considerar el Espacio-Tiempo como curvo.
Considere la misma configuración experimental que teníamos antes, ahora representada en el diagrama de Espacio-Tiempo de la página siguiente.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/four06.pdf}
\end{figure}

El físico en tierra emite un haz de luz con longitud de onda $\lambda_0$ desde una altura $z_0$, que viaja hasta la cima de la torre a la altura $z_1$.
El tiempo entre el instante en que se emite el comienzo de cualquier longitud de onda de la luz y el final de esa misma longitud de onda es $\Delta t_0 = \lambda_0/c$, y el mismo intervalo de tiempo para la absorción es $\Delta t_1=\lambda_1/c$.
Como imaginamos que el campo gravitacional no varía con el tiempo, los caminos a través del Espacio-Tiempo seguidos por el borde anterior y posterior de la onda única deben ser precisamente congruentes.
(Están representados por algunos caminos curvos genéricos, ya que no pretendemos saber exactamente cuáles serán los caminos).
La geometría simple nos dice que los tiempos $\Delta t_0$ y $\Delta t_1$ deben ser iguales.
Pero, por supuesto, no lo son; el corrimiento al rojo gravitacional implica que $\Delta t_1 > \Delta t_0$.
(Lo cual podemos interpretar como ``el reloj de la torre parece correr más rápido''.) La falla reside en la ``geometría simple''; una mejor descripción de lo que sucede es imaginar que el Espacio-Tiempo es curvo.

Todo esto debería constituir una motivación más que suficiente para nuestra afirmación de que, en presencia de la gravedad, el Espacio-Tiempo debería considerarse como una variedad curva.
Tomemos ahora esto como cierto y comencemos a establecer cómo funciona la física en un Espacio-Tiempo curvo.
El principio de equivalencia nos dice que las leyes de la física, en regiones suficientemente pequeñas del Espacio-Tiempo, se parecen a las de la Relatividad Especial.
Interpretamos esto en el lenguaje de las variedades como la afirmación de que estas leyes, cuando se escriben en coordenadas normales de Riemann $x^\mu$ con base en algún punto $p$, se describen mediante ecuaciones que toman la misma forma que lo harían en el espacio plano.
El ejemplo más simple es el de las partículas en caída libre (sin aceleración).
En el espacio plano, estas partículas se mueven en línea recta; en ecuaciones, esto se expresa como la desaparición de la segunda derivada de la ruta parametrizada $x^\mu(\lambda)$:
\begin{equation}
{{d^2 x^\mu}\over{d\lambda^2}} = 0\,.\label{4.8}
\end{equation}
Según el EEP, exactamente esta ecuación debería ser válida en el espacio curvo, siempre que las coordenadas $x^\mu$ sean RNC.
¿Qué pasa con algún otro sistema de coordenadas? Tal como está, (4.8) no es una ecuación entre tensores.
Sin embargo, existe una ecuación tensorial única que se reduce a (4.8) cuando los símbolos de Christoffel desaparecen; es
\begin{equation}
{{d^2 x^\mu}\over{d\lambda^2}}+\Gamma^\mu_{\rho\sigma}
{{d x^\rho}\over{d\lambda}}{{d x^\sigma}\over{d\lambda}} = 0\,.
\label{4.9}
\end{equation}
Por supuesto, esto es simplemente la ecuación geodésica.
Por tanto, en la Relatividad General, las partículas libres se mueven a lo largo de las geodésicas; Hemos mencionado esto antes, pero ahora sabes por qué es cierto.

En lo que respecta a las partículas libres, hemos argumentado que la curvatura del Espacio-Tiempo es necesaria para describir la gravedad; todavía no hemos demostrado que sea suficiente.
Para hacerlo, podemos mostrar cómo los resultados habituales de la gravedad newtoniana encajan en el panorama.
Definimos el ``límite newtoniano'' mediante tres requisitos: las partículas se mueven lentamente (con respecto a la velocidad de la luz), el campo gravitacional es débil (puede considerarse una perturbación del espacio plano) y el campo también es estático. (sin cambios con el tiempo).
Veamos qué efecto tienen estos supuestos en la ecuación geodésica, tomando el tiempo adecuado $\tau$ como parámetro afín.
``Moverse lentamente'' significa que
\begin{equation}
{{dx^i}\over{d\tau}}<<{{dt}\over{d\tau}}\ ,\label{4.10}
\end{equation}
entonces la ecuación geodésica se convierte en
\begin{equation}
{{d^2 x^\mu}\over{d\tau^2}}+\Gamma^\mu_{00}
\left({{d t}\over{d\tau}}\right)^2 = 0\,.
\label{4.11}
\end{equation}
Dado que el campo es estático, los símbolos de Christoffel relevantes $\Gamma^\mu_{00}$ simplifican:
\begin{align}
\Gamma^\mu_{00} &=  {1\over 2} g^{\mu\lambda}
(\p0 g_{\lambda 0} + \p0 g_{0 \lambda} - \p\lambda g_{00}) \notag \\
&=  - {1\over 2} g^{\mu\lambda}\p\lambda g_{00}\,. \label{4.12}
\end{align}
Finalmente, la debilidad del campo gravitacional nos permite descomponer la métrica en la forma de Minkowski más una pequeña perturbación:
\begin{equation}
g_\mn = \eta_\mn + h_\mn\ ,\qquad |h_\mn |<<1\,.\label{4.13}
\end{equation}
(Estamos trabajando en coordenadas cartesianas, por lo que $\eta_\mn$ es la forma canónica de la métrica.
La ``condición de pequeñez'' en la perturbación métrica $h_\mn$ realmente no tiene sentido en otras coordenadas).
De la definición de la métrica inversa, $g^\mn g_{\nu\sigma}=\delta^\mu_\sigma$, encontramos que en primer orden en $h$,
\begin{equation}
g^\mn = \eta^\mn - h^\mn\ ,\label{4.14}
\end{equation}
donde $h^\mn = \eta^{\mu\rho}\eta^{\nu\sigma}h_{\rho\sigma}$.
De hecho, podemos usar la métrica de Minkowski para subir y bajar índices en un objeto de cualquier orden definido en $h$, ya que las correcciones solo contribuirían en órdenes superiores.

Poniéndolo todo junto, encontramos
\begin{equation}
\Gamma^\mu_{00}= - {1\over 2} \eta^{\mu\lambda}\p\lambda h_{00}
\,.\label{4.15}
\end{equation}
Por tanto, la ecuación geodésica (4.11) es
\begin{equation}
{{d^2 x^\mu}\over{d\tau^2}}={1\over 2} \eta^{\mu\lambda}
\p\lambda h_{00} \left({{d t}\over{d\tau}}\right)^2\,.\label{4.16}
\end{equation}
Usando $\p0 h_{00}=0$, el componente $\mu=0$ de esto es solo
\begin{equation}
{{d^2 t}\over{d\tau^2}}=0\,.\label{4.17}
\end{equation}
Es decir, ${{dt}\over{d\tau}}$ es constante.
Para examinar los componentes espaciales de (4.16), recuerde que los componentes espaciales de $\eta^\mn$ son solo los de una matriz identidad $3\times 3$.
Por lo tanto tenemos
\begin{equation}
{{d^2 x^i}\over{d\tau^2}}={1\over 2}\left({{d t}\over{d\tau}}
\right)^2 \p{i} h_{00} \,.\label{4.18}
\end{equation}
Dividir ambos lados por $\left({{d t}\over{d\tau}}\right)^2$ tiene el efecto de convertir la derivada del lado izquierdo de $\tau$ a $t$, dejándonos con
\begin{equation}
{{d^2 x^i}\over{d t^2}}={1\over 2}\p{i} h_{00} \,.\label{4.19}
\end{equation}
Esto empieza a parecerse mucho a la teoría de la gravitación de Newton.
De hecho, si comparamos esta ecuación con (4.4), encontramos que son iguales una vez que identificamos
\begin{equation}
h_{00} = -2\Phi\ ,\label{4.20}
\end{equation}
o en otras palabras
\begin{equation}
g_{00} = -(1+2\Phi)\,.\label{4.21}
\end{equation}
Por lo tanto, hemos demostrado que la curvatura del Espacio-Tiempo es suficiente para describir la gravedad en el límite newtoniano, siempre que la métrica adopte la forma (4.21).
Por supuesto, queda por encontrar ecuaciones de campo para la métrica que impliquen que esta es la forma adoptada y que para un solo cuerpo gravitante recuperamos la fórmula newtoniana.
\begin{equation}
\Phi = -{{GM}\over r}\ ,\label{4.22}
\end{equation}
pero eso llegará muy pronto.

Nuestra próxima tarea es mostrar cómo las leyes restantes de la física, más allá de las que rigen las partículas en caída libre, se adaptan a la curvatura del Espacio-Tiempo.
El procedimiento sigue esencialmente el paradigma establecido al argumentar que las partículas libres se mueven a lo largo de las geodésicas.
Tomemos como ejemplo una ley de la física en un espacio plano, tradicionalmente escrita en términos de derivadas parciales y métrica plana.
Según el principio de equivalencia, esta ley se cumplirá en presencia de la gravedad, siempre que estemos en coordenadas normales de Riemann.
Traducir la ley a una relación entre tensores; por ejemplo, cambie las derivadas parciales a covariantes.
En RNC, esta versión de la ley se reducirá a la de espacio plano, pero los tensores son objetos independientes de las coordenadas, por lo que la versión tensorial debe cumplirse en cualquier sistema de coordenadas.

A este procedimiento a veces se le da un nombre: {\bf Principio de Covarianza}.
No estoy seguro de que merezca su propio nombre, ya que en realidad es una consecuencia del EEP más el requisito de que las leyes de la física sean independientes de las coordenadas.
(Es esencialmente imposible imaginar que el requisito de que las leyes de la física sean independientes de las coordenadas sea falso.
Dado algún experimento, si una persona usa un sistema de coordenadas para predecir un resultado y otra usa un sistema de coordenadas diferente, será mejor que estén de acuerdo).
Otro nombre es ``regla de la coma va al punto y coma'', ya que a nivel tipográfico lo que hay que hacer es sustituir las derivadas parciales (comas) por covariantes (punto y coma).

Ya hemos utilizado implícitamente el principio de covarianza (o como quiera llamarlo) para derivar la afirmación de que las partículas libres se mueven a lo largo de las geodésicas.
En su mayor parte, es muy sencillo aplicarlo a casos interesantes.
Consideremos, por ejemplo, la fórmula para la conservación de la energía en el Espacio-Tiempo plano, $\p\mu T^\mn =0$.
La adaptación al Espacio-Tiempo curvo es inmediata:
\begin{equation}
\nabla_\mu T^\mn =0\,.\label{4.23}
\end{equation}
Esta ecuación expresa la conservación de la energía en presencia de un campo gravitacional.

Desafortunadamente, la vida no siempre es tan fácil.
Consideremos las ecuaciones de Maxwell en Relatividad Especial, donde parecería que el principio de covarianza se puede aplicar de forma sencilla.
La ecuación no homogénea $\p\mu F^{\nu\mu} = 4\pi J^\nu$ se convierte en
\begin{equation}
\nabla_\mu F^{\nu\mu} = 4\pi J^\nu\ ,\label{4.24}
\end{equation}
y el homogéneo $\p{[\mu} F_{\nu\lambda]}= 0$ se convierte en
\begin{equation}
\nabla_{[\mu} F_{\nu\lambda]}= 0\,.\label{4.25}
\end{equation}
Por otro lado, también podríamos escribir las ecuaciones de Maxwell en el espacio plano en términos de formas diferenciales como
\begin{equation}
\d(*F) = 4\pi(* J)\ ,\label{4.26}
\end{equation}
y
\begin{equation}
 d\,F = 0\,.\label{4.27}
\end{equation}
Estos ya están en forma perfectamente tensorial, ya que hemos demostrado que la derivada exterior es un operador tensorial bien definido independientemente de cuál sea la conexión.
Por tanto, empezamos a preocuparnos un poco; ¿Cuál es la garantía de que el proceso de escribir una ley de la física en forma tensorial dé una respuesta única? De hecho, como mencionamos anteriormente, las versiones en formas diferenciales de las ecuaciones de Maxwell deben considerarse fundamentales.
Sin embargo, en este caso no hay diferencia, ya que en ausencia de torsión (4.26) es idéntico a (4.24) y (4.27) es idéntico a (4.25); la parte simétrica de la conexión no contribuye.
De manera similar, la definición del tensor de intensidad de campo en términos del potencial $A_\mu$ se puede escribir como
\begin{equation}
F_{\mn} = \nabla_\mu A_\nu - \nabla_\nu A_\mu\ ,\label{4.28}
\end{equation}
o igualmente bien como
\begin{equation}
F= d\,A\,.\label{4.29}
\end{equation}

Sin embargo, la preocupación por la unicidad es real.
Imagine que dos campos vectoriales $X^\mu$ y $Y^\nu$ obedecen una ley en un espacio plano dada por
\begin{equation}
Y^\mu\p\mu \p\nu X^\nu = 0\,.\label{4.30}
\end{equation}
El problema de escribir esto como una ecuación tensorial debería ser claro: las derivadas parciales se pueden conmutar, pero las derivadas covariantes no.
Si simplemente reemplazamos los parciales en (4.30) por derivadas covariantes, obtenemos una respuesta diferente a la que obtendríamos si primero hubiéramos intercambiado el orden de las derivadas (dejando invariante la ecuación en el espacio plano) y luego las reemplazamos.
La diferencia viene dada por
\begin{equation}
Y^\mu\nabla_\mu \nabla_\nu X^\nu-Y^\mu\nabla_\nu \nabla_\mu X^\nu
= -R_{\mn}Y^\mu X^\nu\,.\label{4.31}
\end{equation}
La prescripción para generalizar leyes desde el Espacio-Tiempo plano al curvo no nos guía a la hora de elegir el orden de las derivadas y, por tanto, es ambigua en cuanto a si un término como el de (4.31) debería aparecer en presencia de gravedad.
(El problema de ordenar derivadas covariantes es similar al problema de las ambigüedades de ordenamiento de operadores en mecánica cuántica).

En la literatura se pueden encontrar varias recetas para abordar ambigüedades como ésta, la mayoría de las cuales son consejos sensatos, como recordar preservar la invariancia del calibre para el electromagnetismo.
Pero en el fondo la verdadera respuesta es que no hay manera de resolver estos problemas sólo con el pensamiento puro; El hecho es que puede haber más de una forma de adaptar una ley de la física al espacio curvo y, en última instancia, sólo el experimento puede decidir entre las alternativas.

De hecho, seamos honestos acerca del principio de equivalencia: sirve como una guía útil, pero no merece ser tratado como un principio fundamental de la naturaleza.
Desde el punto de vista moderno, no esperamos que la AEA sea rigurosamente cierta.
Considere la siguiente versión alternativa de (4.24):
\begin{equation}
\nabla_\mu[(1+\alpha R)F^{\nu\mu}] = 4\pi J^\nu\ ,\label{4.32}
\end{equation}
donde $R$ es el escalar de Ricci y $\alpha$ es alguna constante de acoplamiento.
Si esta ecuación describiera correctamente la electrodinámica en el Espacio-Tiempo curvo, sería posible medir $R$ incluso en una región arbitrariamente pequeña, haciendo experimentos con partículas cargadas.
Por tanto, el principio de equivalencia exige que $\alpha=0$.
Pero por lo demás, ésta es una ecuación perfectamente respetable, consistente con la conservación de la carga y otras características deseables del electromagnetismo, que se reduce a la ecuación habitual en el espacio plano.
De hecho, en un mundo gobernado por la mecánica cuántica esperamos que todos los acoplamientos posibles entre diferentes campos (como la gravedad y el electromagnetismo) sean consistentes con las simetrías de la teoría (en este caso, la invariancia de calibre).
Entonces, ¿por qué es razonable establecer $\alpha=0$? La verdadera razón es de escalas.
Observe que el tensor de Ricci implica segundas derivadas de la métrica, que no tiene dimensiones, por lo que $R$ tiene dimensiones de (longitud)$^{-2}$ (con $c=1$).
Por lo tanto, $\alpha$ debe tener dimensiones de (longitud)$^{2}$.
Pero dado que el acoplamiento representado por $\alpha$ es de origen gravitacional, la única expectativa razonable para la escala de longitud relevante es
\begin{equation}
\alpha \sim l_P^2\ ,\label{4.33}
\end{equation}
donde $l_P$ es la longitud de Planck
\begin{equation}
l_P = \left({{G\hbar}\over{c^3}}\right)^{1/2}=1.6\times
10^{-33}{\rm ~cm}\ ,\label{4.34}
\end{equation}
donde $\hbar$ es, por supuesto, la constante de Planck.
Por tanto, la escala de longitud correspondiente a este acoplamiento es extremadamente pequeña, y para cualquier experimento concebible esperamos que la escala típica de variación del campo gravitacional sea mucho mayor.
Por lo tanto, la razón por la que este término que viola el principio de equivalencia puede ser ignorado con seguridad es simplemente porque $\alpha R$ es probablemente un número fantásticamente pequeño, lejos del alcance de cualquier experimento.
Por otro lado, también podríamos mantener la mente abierta, ya que nuestras expectativas no siempre se ven confirmadas por la observación.

Una vez establecido cómo las leyes físicas gobiernan el comportamiento de los campos y objetos en un espaciotiempo curvo, podemos completar el establecimiento de la Relatividad General propiamente dicha introduciendo las ecuaciones de campo de Einstein, que gobiernan cómo la métrica responde a la energía y al momentum.
En realidad, haremos esto de dos maneras: primero mediante un argumento informal cercano a lo que el propio Einstein estaba pensando, y luego comenzando con una acción y derivando las ecuaciones de movimiento correspondientes.

El argumento informal comienza con la comprensión de que nos gustaría encontrar una ecuación que reemplace la ecuación de Poisson para el potencial newtoniano:
\begin{equation}
\nabla^2\Phi = 4\pi G\rho\ ,\label{4.35}
\end{equation}
donde $\nabla^2 = \delta^{ij}\p{i}\p{j}$ es el laplaciano en el espacio y $\rho$ es la densidad de masa.
(La forma explícita de $\Phi$ dada en (4.22) es una solución de (4.35), para el caso de una distribución de masa puntual.)
¿Qué características debe poseer nuestra buscada ecuación? En el lado izquierdo de (4.35) tenemos un operador diferencial de segundo orden que actúa sobre el potencial gravitacional y en el lado derecho una medida de la distribución de masa.
Una generalización relativista debería tomar la forma de una ecuación entre tensores.
Sabemos cuál es la generalización tensorial de la densidad de masa; es el tensor de energía-momentum $T_\mn$.
Mientras tanto, el potencial gravitacional debería ser reemplazado por el tensor métrico.
Por lo tanto, podríamos suponer que nuestra nueva ecuación tendrá $T_\mn$ proporcional a algún tensor que sea de segundo orden en las derivadas de la métrica.
De hecho, usando (4.21) para la métrica en el límite newtoniano y $T_{00}=\rho$, vemos que en este límite estamos buscando una ecuación que prediga
\begin{equation}
\nabla^2 h_{00} = -8\pi G T_{00}\ ,\label{4.36}
\end{equation}
pero por supuesto queremos que sea completamente tensorial.

El lado izquierdo de (4.36) obviamente no se generaliza a un tensor.
La primera opción podría ser actuar como D'Alembertiano $\Box = \nabla^\mu \nabla_\mu$ en la métrica $g_\mn$, pero esto es automáticamente cero por compatibilidad de métricas.
Afortunadamente, hay una cantidad obvia que no es cero y se construye a partir de segundas derivadas (y primeras derivadas) de la métrica: el tensor de Riemann $R^\rho{}_{\sigma\mn}$.
No tiene el número correcto de índices, pero podemos contraerlo para formar el tensor de Ricci $R_{\mn}$, que los tiene (y además es simétrico).
Por tanto, es razonable suponer que las ecuaciones del campo gravitacional son
\begin{equation}
R_{\mn} = \kappa T_{\mn}\ ,\label{4.37}
\end{equation}
para alguna constante $\kappa$.
De hecho, Einstein sugirió esta ecuación en un instante dado.
Lamentablemente, existe un problema con la conservación de la energía.
Según el Principio de Equivalencia, la afirmación de la conservación del momentum de energía en el Espacio-Tiempo curvo debería ser
\begin{equation}
\nabla^\mu T_{\mn} =0\ ,\label{4.38}
\end{equation}
lo que entonces implicaría
\begin{equation}
\nabla^\mu R_{\mn} =0\,.\label{4.39}
\end{equation}
Ciertamente esto no es cierto en una geometría arbitraria; Hemos visto en la identidad de Bianchi (3.94) que
\begin{equation}
\nabla^\mu R_{\mn} ={1\over 2}\nabla_\nu R\,.\label{4.40}
\end{equation}
Pero nuestra ecuación de campo propuesta implica que $R=\kappa g^{\mn} T_{\mn} = \kappa T$, por lo que tomándolos juntos tenemos
\begin{equation}
\nabla_\mu T=0\,.\label{4.41}
\end{equation}
La derivada covariante de un escalar es simplemente la derivada parcial, por lo que (4.41) nos dice que $T$ es constante en todo el Espacio-Tiempo.
Esto es muy inverosímil, ya que $T=0$ está en el vacío mientras que $T>0$ está en la materia.
Tenemos que esforzarnos más.

(En realidad, estamos haciendo un poco de trampa al tomarnos la ecuación $\nabla^\mu T_{\mn} =0$ tan en serio.
Si, como dijimos, el principio de equivalencia es solo una guía aproximada, podríamos imaginar que hay términos distintos de cero en el lado derecho que involucran al tensor de curvatura.
Más adelante seremos más precisos y argumentaremos que son estrictamente cero).

Por supuesto, no tenemos que esforzarnos mucho, ya que ya conocemos un tensor simétrico $(0,2)$, construido a partir del tensor de Ricci, que se conserva automáticamente: el tensor de Einstein.
\begin{equation}
G_\mn = R_\mn - {1\over 2} R g_\mn\ ,\label{4.42}
\end{equation}
que siempre obedece a $\nabla^\mu G_\mn =0$.
Por lo tanto, nos vemos obligados a proponer
\begin{equation}
G_\mn = \kappa T_\mn\label{4.43}
\end{equation}
como ecuación de campo para la métrica.
Esta ecuación satisface todos los requisitos obvios; el lado derecho es una expresión covariante de la densidad de energía y momentum en forma de un tensor $(0,2)$ simétrico y conservado, mientras que el lado izquierdo es un tensor $(0,2)$ simétrico y conservado construido a partir de la métrica y su primera y segundas derivadas.
Sólo queda ver si realmente reproduce la gravedad tal como la conocemos.

Para responder a esto, observe que al contraer ambos lados de (4.43) se obtiene (en cuatro dimensiones)
\begin{equation}
R = -\kappa T\ ,\label{4.44}
\end{equation}
y usando esto podemos reescribir (4.43) como
\begin{equation}
R_\mn = \kappa(T_\mn -{1\over 2}T g_\mn)\,.\label{4.45}
\end{equation}
Esta es la misma ecuación, sólo que escrita de manera ligeramente diferente.
Nos gustaría ver si predice la gravedad newtoniana en el límite de partículas que se mueven lentamente, independientes del tiempo y de campo débil.
En este límite, la energía en reposo $\rho=T_{00}$ será mucho mayor que los otros términos en $T_\mn$, por lo que queremos centrarnos en el componente $\mu=0$, $\nu=0$ de (4.45).
En el límite del campo débil escribimos (de acuerdo con (4.13) y (4.14))
\begin{align}
g_{00}  &=  -1 +h_{00}\ , \notag\\
g^{00}  &=  -1 -h_{00}\,. \label{4.46}
\end{align}
La traza del tensor energía-momentum, al orden más bajo no trivial, es
\begin{equation}
T = g^{00}T_{00} = -T_{00}\,.\label{4.47}
\end{equation}
Al conectar esto a (4.45), obtenemos
\begin{equation}
R_{00} = {1\over 2} \kappa T_{00}\,.\label{4.48}
\end{equation}
Esta es una ecuación que relaciona las derivadas de la métrica con la densidad de energía.
Para encontrar la expresión explícita en términos de la métrica, necesitamos evaluar $R_{00} = R^\lambda{}_{0\lambda 0}$.
De hecho solo necesitamos $R^i{}_{0i0}$, ya que $R^0{}_{000}=0$.
Tenemos
\begin{equation}
R^i{}_{0j0} = \p{j}\Gamma^i_{00} - \p{0}\Gamma^i_{j0}
+\Gamma^i_{j\lambda}\Gamma^\lambda_{00}
-\Gamma^i_{0\lambda}\Gamma^\lambda_{j0}\,.\label{4.49}
\end{equation}
El segundo término aquí es una derivada del tiempo, que desaparece para campos estáticos.
Los términos tercero y cuarto son de la forma $(\Gamma)^2$, y dado que $\Gamma$ es de primer orden en la perturbación métrica, estos contribuyen sólo en segundo orden y pueden despreciarse.
Nos queda $R^i{}_{0j0} = \p{j}\Gamma^i_{00}$.
De esto obtenemos
\begin{align}
R_{00}  &=  R^i{}_{0i0} \notag \\
&=  \p{i}\left({1\over 2} g^{i\lambda}
(\p0 g_{ \lambda 0} + \p0 g_{0\lambda} - \p\lambda g_{00})\right) \notag \\
&=  -{1\over 2}\eta^{ij}\p{i}\p{j} h_{00} \notag \\
&=  -{1\over 2}\nabla^2h_{00}\,. \label{4.50}
\end{align}
Comparando con (4.48), vemos que la componente $00$ de (4.43) en el límite newtoniano predice
\begin{equation}
\nabla^2 h_{00} = -\kappa T_{00}\,.\label{4.51}
\end{equation}
Pero esto es exactamente (4.36), si configuramos $\kappa = 8\pi G$.

Entonces nuestra suposición parece haber funcionado.
Con la normalización fijada en comparación con el límite newtoniano, podemos presentar {\bf las ecuaciones de Einstein} para la Relatividad General:
\begin{equation}
R_{\mn} -{1\over 2}R g_\mn = 8\pi G T_{\mn}\,.\label{4.52}
\end{equation}
Estos nos dicen cómo reacciona la curvatura del Espacio-Tiempo ante la presencia de energía-momentum.
Como habrás oído, Einstein pensaba que el lado izquierdo era bonito y geométrico, mientras que el lado derecho era algo menos convincente.

Las ecuaciones de Einstein pueden considerarse como ecuaciones diferenciales de segundo orden para el campo tensorial métrico $g_\mn$.
Hay diez ecuaciones independientes (ya que ambos lados son tensores simétricos de dos índices), lo que parece ser exactamente correcto para las diez funciones desconocidas de los componentes métricos.
Sin embargo, la identidad de Bianchi $\nabla^\mu G_\mn=0$ representa cuatro restricciones sobre las funciones $R_{\mn}$, por lo que sólo hay seis ecuaciones verdaderamente independientes en (4.52).
De hecho, esto es apropiado, ya que si una métrica es una solución a la ecuación de Einstein en un sistema de coordenadas $x^\mu$, también debería ser una solución en cualquier otro sistema de coordenadas $x^{\mu'}$.
Esto significa que hay cuatro grados de libertad no físicos en $g_\mn$ (representados por las cuatro funciones $x^{\mu'}(x^\mu)$), y deberíamos esperar que las ecuaciones de Einstein solo restrinjan los seis grados de libertad independientes de las coordenadas.

Como ecuaciones diferenciales, son extremadamente complicadas; El escalar y tensor de Ricci son contracciones del tensor de Riemann, que involucra derivadas y productos de los símbolos de Christoffel, que a su vez involucran la métrica inversa y las derivadas de la métrica.
Además, el tensor de energía-momentum $T_\mn$ generalmente también involucrará a la métrica.
Las ecuaciones también son no lineales, de modo que dos soluciones conocidas no se pueden superponer para encontrar una tercera.
Por lo tanto, es muy difícil resolver las ecuaciones de Einstein con algún tipo de generalidad y normalmente es necesario hacer algunas suposiciones simplificadoras.
Incluso en el vacío, donde fijamos el tensor de energía-momentum a cero, las ecuaciones resultantes (de (4.45))
\begin{equation}
R_\mn=0\label{4.53}
\end{equation}
puede ser muy difícil de resolver.
El tipo de suposición simplificadora más popular es que la métrica tiene un grado significativo de simetría, y más adelante hablaremos de cómo las simetrías de la métrica hacen la vida más fácil.

Vale la pena destacar la no linealidad de la Relatividad General.
En la gravedad newtoniana, el potencial debido a dos masas puntuales es simplemente la suma de los potenciales de cada masa, pero claramente esto no se traslada a la Relatividad General (fuera del límite del campo débil).
Hay una razón física para esto: en Relatividad General el campo gravitacional se acopla consigo mismo.
Esto puede considerarse una consecuencia del principio de equivalencia: si la gravitación no se acoplara consigo misma, un ``átomo gravitacional'' (dos partículas unidas por su atracción gravitacional mutua) tendría una masa inercial diferente (debido a la energía de enlace negativa) que la masa gravitacional.
Desde el punto de vista de la física de partículas, esto se puede expresar en términos de diagramas de Feynman.
Se puede considerar que la interacción electromagnética entre dos electrones se debe al intercambio de un fotón virtual:

\begin{figure}[h]
\centering
\includegraphics[width=0.35\linewidth]{imagenes/four07.pdf}
\end{figure}

\noindent
Pero no existe ningún diagrama en el que dos fotones intercambien otro fotón entre sí; El electromagnetismo es lineal.
Mientras tanto, se puede considerar que la interacción gravitacional se debe al intercambio de un gravitón virtual (una perturbación cuantificada de la métrica).
La no linealidad se manifiesta como el hecho de que tanto los electrones como los gravitones (y cualquier otra cosa) pueden intercambiar gravitones virtuales y, por tanto, ejercer una fuerza gravitacional:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/four08.pdf}
\end{figure}

\noindent
No hay nada profundo en esta característica de la gravedad; es compartido por la mayoría de las teorías de calibre, como la cromodinámica cuántica, la teoría de las interacciones fuertes.
(El electromagnetismo es en realidad la excepción; la linealidad se puede atribuir al hecho de que el grupo de calibre relevante, U(1), es abeliano.)
Pero sí representa un alejamiento de la teoría newtoniana.
(Por supuesto, este lenguaje mecánico cuántico de los diagramas de Feynman es algo inapropiado para la Relatividad General, que [todavía] no ha sido cuantificado con éxito, pero los diagramas son sólo una abreviatura conveniente para recordar qué interacciones existen en la teoría).

Para aumentar su confianza en que las ecuaciones de Einstein tal como las hemos derivado son en realidad las ecuaciones de campo correctas para la métrica, veamos cómo se pueden derivar desde un punto de vista más moderno, partiendo de un principio de acción.
(De hecho, las ecuaciones fueron derivadas por primera vez por Hilbert, no por Einstein, y Hilbert lo hizo utilizando el principio de acción.
Pero se había inspirado en los artículos anteriores de Einstein sobre el tema, y el propio Einstein derivó las ecuaciones de forma independiente, por lo que con razón llevan el nombre de Einstein.
La acción, sin embargo, se llama con razón acción de Hilbert.)
La acción debe ser la integral sobre el Espacio-Tiempo de una densidad de Lagrange (``Lagrangiana'' para abreviar, aunque estrictamente hablando la Lagrangiana es la integral sobre el espacio de la densidad de Lagrange):
\begin{equation}
S_H=\int d^nx {\cal L}_H\,.\label{4.54}
\end{equation}
La densidad de Lagrange es una densidad tensorial, que se puede escribir como $\g$ multiplicado por un escalar.
¿Qué escalares podemos hacer con la métrica?
Como sabemos que la métrica se puede igualar a su forma canónica y sus primeras derivadas se pueden igualar a cero en cualquier punto, cualquier escalar no trivial debe involucrar al menos segundas derivadas de la métrica.
El tensor de Riemann, por supuesto, está hecho de segundas derivadas de la métrica, y anteriormente argumentamos que el único escalar independiente que podíamos construir a partir del tensor de Riemann era el escalar de Ricci $R$.
Lo que no mostramos, pero que es cierto, es que cualquier tensor no trivial formado a partir de la métrica y sus derivadas primera y segunda se puede expresar en términos de la métrica y el tensor de Riemann.
Por lo tanto, el único escalar independiente construido a partir de la métrica, que no es superior al segundo orden en sus derivadas, es el escalar de Ricci.
Hilbert pensó que ésta era, por tanto, la elección más sencilla posible para un lagrangiano y propuso
\begin{equation}
{\cal L}_H = \g R\,.\label{4.55}
\end{equation}
Las ecuaciones de movimiento deben surgir de variar la acción con respecto a la métrica.
De hecho, consideremos variaciones con respecto a la métrica inversa $g^\mn$, que son un poco más fáciles pero dan un conjunto de ecuaciones equivalente.
Usando $R=g^\mn R_\mn$, en general tendremos
\begin{align}
\delta S  &=  \int d^nx\left[\g g^\mn \delta R_\mn + \g R_\mn \delta
g^\mn+ R\delta\g\right] \notag \\
&=  (\delta S)_1 +(\delta S)_2 +(\delta S)_3 \,. \label{4.56}
\end{align}
El segundo término $(\delta S)_2$ ya tiene la forma de alguna expresión multiplicada por $\delta g^\mn$; Examinemos los demás más de cerca.

Recuerde que el tensor de Ricci es la contracción del tensor de Riemann, que viene dado por
\begin{equation}
R^\rho{}_{\mu\lambda\nu} = \p\lambda \Gamma^\lambda_{\nu\mu}
+\Gamma^\rho_{\lambda\sigma}\Gamma^\sigma_{\nu\mu}
- (\lambda \leftrightarrow \nu)\,.\label{4.57}
\end{equation}
La variación de esto con respecto a la métrica se puede encontrar primero variando la conexión con respecto a la métrica, y luego sustituyendo en esta expresión.
Sin embargo, consideremos variaciones arbitrarias de la conexión, reemplazando
\begin{equation}
\Gamma^\rho_{\nu\mu}\rightarrow \Gamma^\rho_{\nu\mu}+
\delta\Gamma^\rho_{\nu\mu}\,.\label{4.58}
\end{equation}
La variación $\delta\Gamma^\rho_{\nu\mu}$ es la diferencia de dos conexiones y, por tanto, es en sí misma un tensor.
Por tanto, podemos tomar su derivada covariante,
\begin{equation}
\nabla_\lambda(\delta\Gamma^\rho_{\nu\mu})=
\p\lambda(\delta\Gamma^\rho_{\nu\mu})
+\Gamma^\rho_{\lambda\sigma}\delta\Gamma^\sigma_{\nu\mu}
-\Gamma^\sigma_{\lambda\nu}\delta\Gamma^\rho_{\sigma\mu}
-\Gamma^\sigma_{\lambda\mu}\delta\Gamma^\rho_{\nu\sigma}\,.
\label{4.59}
\end{equation}
Dada esta expresión (y una pequeña cantidad de trabajo) es fácil demostrar que
\begin{equation}
\delta R^\rho{}_{\mu\lambda\nu}=
\nabla_\lambda(\delta\Gamma^\rho_{\nu\mu})
-\nabla_\nu(\delta\Gamma^\rho_{\lambda\mu})\,.\label{4.60}
\end{equation}
Puedes comprobarlo tú mismo.
Por lo tanto, la contribución del primer término en (4.56) a $\delta S$ se puede escribir
\begin{align}
(\delta S)_1  &=
\int d^nx \g ~g^\mn \left[\nabla_\lambda(
\delta\Gamma^\lambda_{\nu\mu})
-\nabla_\nu(\delta\Gamma^\lambda_{\lambda\mu})\right] \notag \\
&=  \int d^nx \g ~ {\nabla_\sigma}\left[g^{\mu\sigma}(\delta
\Gamma^\lambda_{\lambda\mu}) - g^{\mn}(\delta
\Gamma^\sigma_{\mu\nu})\right]\ , \label{4.61}
\end{align}
donde hemos utilizado la compatibilidad métrica y reetiquetado algunos índices ficticios.
Pero ahora tenemos la integral con respecto al elemento de volumen natural de la divergencia covariante de un vector; según el teorema de Stokes, esto es igual a una contribución límite en el infinito que podemos establecer en cero haciendo que la variación desaparezca en el infinito.
(En realidad, no hemos demostrado que el teorema de Stokes, como se mencionó anteriormente en términos de formas diferenciales, pueda considerarse de esta manera, pero usted puede convencerse fácilmente de que es cierto).
Por lo tanto este término no aporta nada a la variación total.

Para que el término $(\delta S)_3$ tenga sentido, debemos utilizar el siguiente hecho, válido para cualquier matriz $M$:
\begin{equation}
\tr(\ln M) = \ln(\det M)\,.\label{4.62}
\end{equation}
Aquí, $\ln M$ está definido por $\exp(\ln M)=M$.
(Para los números esto es obvio, para las matrices es un poco menos sencillo).
La variación de esta identidad produce
\begin{equation}
\tr(M^{-1} \delta M) = {1\over{\det M}}\delta(\det M)\,.\label{4.63}
\end{equation}
Aquí hemos utilizado la propiedad cíclica de la traza para permitirnos ignorar el hecho de que $M^{-1}$ y $\delta M$ pueden no conmutar.
Ahora nos gustaría aplicar esto a la métrica inversa, $M = g^\mn$.
Luego $\det M=g^{-1}$ (donde $g=\det g_{\mn}$), y
\begin{equation}
\delta(g^{-1})={1\over g}g_\mn \delta g^\mn\,.\label{4.64}
\end{equation}
Ahora podemos simplemente conectar:
\begin{align}
\delta\g  &=  \delta[(-g^{-1})^{-1/2}] \notag \\
&=  -{1\over 2}(-g^{-1})^{-3/2}\delta(-g^{-1}) \notag \\
&=  -{1\over 2}\g g_\mn \delta g^\mn\,. \label{4.65}
\end{align}
Volviendo a (4.56) y recordando que $(\delta S)_1$ no contribuye, encontramos
\begin{equation}
\delta S = \int d^nx \g ~\left[R_{\mn} -{1\over 2} Rg_\mn\right]
\delta g^\mn\,.\label{4.66}
\end{equation}
Esto debería desaparecer para variaciones arbitrarias, por lo que nos llevan a las ecuaciones de Einstein en el vacío:
\begin{equation}
{1\over{\g}}{{\delta S}\over{\delta g^\mn}}
=R_{\mn} -{1\over 2} Rg_\mn =0\,.\label{4.67}
\end{equation}

El hecho de que esta simple acción conduzca a las mismas ecuaciones de campo de vacío a las que habíamos llegado anteriormente mediante argumentos más informales ciertamente nos asegura que estamos haciendo algo bien.
Sin embargo, lo que realmente nos gustaría es obtener también las ecuaciones de campo sin vacío.
Eso significa que consideramos una acción de la forma
\begin{equation}
S={{1}\over{8\pi G}}S_H+S_M\ ,\label{4.68}
\end{equation}
donde $S_M$ es la acción de la materia, y hemos normalizado proféticamente la acción gravitacional (aunque la normalización adecuada depende en cierto modo de la convención).
Siguiendo el mismo procedimiento anterior se llega a
\begin{equation}
{1\over{\g}}{{\delta S}\over{\delta g^\mn}}
={{1}\over{8\pi G}}\left(R_{\mn} -{1\over 2} Rg_\mn\right)
+{1\over{\g}} {{\delta S_M}\over{\delta g^\mn}}=0\ ,\label{4.69}
\end{equation}
y recuperamos las ecuaciones de Einstein si podemos establecer
\begin{equation}
T_\mn = -{1\over{\g}}{{\delta S_M}\over{\delta g^\mn}}\,.\label{4.70}
\end{equation}
¿Qué nos hace pensar que podemos hacer tal identificación?
De hecho, (4.70) resulta ser la mejor manera de definir un tensor simétrico de energía-momentum.
La parte complicada es demostrar que se conserva, lo que de hecho es automáticamente cierto, pero que no justificaremos hasta la siguiente sección.

Decimos que (4.70) proporciona la ``mejor'' definición del tensor de energía-momentum porque no es la única que encontrará.
En el espacio plano de Minkowski, existe una definición alternativa que a veces se da en libros sobre electromagnetismo o teoría de campos.
En este contexto, la conservación de la energía-momentum surge como consecuencia de la simetría del Lagrangiano bajo traslaciones espacio-temporales.
El teorema de Noether establece que toda simetría de un lagrangiano implica la existencia de una ley de conservación; la invariancia bajo las cuatro traslaciones del Espacio-Tiempo conduce a un tensor $S^\mn$ que obedece a $\p\mu S^\mn=0$ (cuatro relaciones, una para cada valor de $\nu$).
Los detalles se pueden encontrar en Wald o en cualquier cantidad de libros de teoría de campo.
Aplicando el procedimiento de Noether a un lagrangiano que depende de unos campos $\psi^i$ y sus primeras derivadas $\p\mu\psi^i$, obtenemos
\begin{equation}
S^\mn={{\delta{\cal L}}\over{\delta(\p\mu\psi^i)}}\partial^\nu\psi^i
-\eta^\mn{\cal L}\ ,\label{4.71}
\end{equation}
donde está implícita una suma superior a $i$.
Puedes comprobar que este tensor se conserva en virtud de las ecuaciones de movimiento de los campos de materia.
$S^\mn$ a menudo recibe el nombre de ``tensor canónico de energía-momentum''; sin embargo, hay una serie de razones por las que nos resulta más conveniente utilizar (4.70).
En primer lugar, (4.70) es de hecho lo que aparece en el lado derecho de las ecuaciones de Einstein cuando se derivan de una acción, y no siempre es posible generalizar (4.71) al Espacio-Tiempo curvo.
Pero incluso en el espacio plano (4.70) tiene sus ventajas; es manifiestamente simétrico y también se garantiza que será invariante de calibre, lo cual no es cierto para (4.71).
Por lo tanto, nos atendremos a (4.70) como definición del tensor de energía-momentum.

A veces resulta útil pensar en las ecuaciones de Einstein sin especificar la teoría de la materia de la que se deriva $T_\mn$.
Esto nos deja con mucha arbitrariedad; Consideremos, por ejemplo, la pregunta ``¿Qué métricas obedecen a las ecuaciones de Einstein?''. En ausencia de algunas restricciones sobre $T_\mn$, la respuesta es ``cualquier métrica''; simplemente tome la métrica de su elección, calcule el tensor de Einstein $G_\mn$ para esta métrica y luego exija que $T_\mn$ sea igual a $G_\mn$.
(Será conservado automáticamente por la identidad de Bianchi).
Nuestra verdadera preocupación es la existencia de soluciones a las ecuaciones de Einstein en presencia de fuentes ``realistas'' de energía y momentum, sea lo que sea que eso signifique.
La propiedad más común que se exige de $T_\mn$ es que represente densidades de energía positivas; no se permiten masas negativas.
En un marco localmente inercial, este requisito se puede expresar como $\rho = T_{00} \geq 0$.
Para convertir esto en una declaración independiente de las coordenadas, pedimos que
\begin{equation}
T_\mn V^\mu V^\nu \geq 0\ ,\qquad{\rm ~for~all~timelike~vectors~}
V^\mu\,.\label{4.72}
\end{equation}
Esto se conoce como {\bf Condición de energía débil} o WEC.
Parece un requisito bastante razonable, y muchos de los teoremas importantes sobre soluciones a la Relatividad General (como los teoremas de singularidad de Hawking y Penrose) se basan en esta condición o en algo muy cercano a ella.
Desafortunadamente no está escrito en piedra; de hecho, es sencillo inventar teorías de campos clásicas, por lo demás respetables, que violen el WEC, y casi imposible inventar una teoría cuántica de campos que lo obedezca.
Sin embargo, es legítimo suponer que el WEC se mantiene en todas las condiciones excepto en las más extremas.
(También hay condiciones energéticas más fuertes, pero son incluso menos ciertas que las del WEC, y no nos detendremos en ellas).

Ahora hemos justificado las ecuaciones de Einstein de dos maneras diferentes: como la generalización covariante natural de la ecuación de Poisson para el potencial gravitacional newtoniano, y como resultado de variar la acción más simple posible que pudimos inventar para la métrica.
El resto del curso será una exploración de las consecuencias de estas ecuaciones, pero antes de comenzar ese camino, exploremos brevemente las formas en que se podrían modificar las ecuaciones.
Hay un número incontable de tales formas, pero consideraremos cuatro posibilidades diferentes: la introducción de una constante cosmológica, términos de orden superior en la acción, campos escalares gravitacionales y un tensor de torsión que no desaparece.

La primera posibilidad es la constante cosmológica; George Gamow ha citado a Einstein diciendo que esto fue el mayor error de su vida.
Recordemos que en nuestra búsqueda de la acción más simple posible para la gravedad observamos que cualquier escalar no trivial tenía que ser al menos de segundo orden en derivadas de la métrica; en el orden inferior, todo lo que podemos crear es una constante.
Aunque una constante no conduce por sí sola a una dinámica muy interesante, tiene un efecto importante si la sumamos a la acción convencional de Hilbert.
Por lo tanto, consideramos una acción dada por
\begin{equation}
S=\int d^nx \g(R-2\Lambda)\ ,\label{4.73}
\end{equation}
donde $\Lambda$ es una constante.
Las ecuaciones de campo resultantes son
\begin{equation}
R_\mn -{1\over 2}R g_\mn +\Lambda g_\mn =0\ ,\label{4.74}
\end{equation}
y, por supuesto, habría un tensor de energía-momentum en el lado derecho si hubiéramos incluido una acción para la materia.
$\Lambda$ es la constante cosmológica; Fue introducido originalmente por Einstein después de que quedó claro que no había soluciones para sus ecuaciones que representan una cosmología estática (un universo que no cambia con el tiempo a gran escala) con un contenido de materia distinto de cero.
Si la constante cosmológica está correctamente ajustada, es posible encontrar una solución estática, pero es inestable ante pequeñas perturbaciones.
Además, una vez que Hubble demostró que el universo se está expandiendo, se volvió menos importante encontrar soluciones estáticas y Einstein rechazó su sugerencia.
Sin embargo, al igual que Rasputín, la constante cosmológica ha resultado difícil de eliminar.
Si queremos, podemos mover el término adicional en (4.74) al lado derecho y pensar en él como una especie de tensor de energía-momentum, con $T_\mn = - \Lambda g_\mn$ (se conserva automáticamente por compatibilidad métrica).
Entonces $\Lambda$ puede interpretarse como la ``densidad de energía del vacío'', una fuente de energía y momentum que está presente incluso en ausencia de campos de materia.
Esta interpretación es importante porque la teoría cuántica de campos predice que el vacío debería tener algún tipo de energía y momentum.
En la mecánica cuántica ordinaria, un oscilador armónico con frecuencia $\omega$ y energía clásica mínima $E_0=0$ tras la cuantificación tiene un estado fundamental con energía $E_0={1\over 2}\hbar\omega$.
Se puede considerar un campo cuantificado como una colección de un número infinito de osciladores armónicos, y cada modo contribuye a la energía del estado fundamental.
El resultado es, por supuesto, infinito y debe regularizarse adecuadamente, por ejemplo introduciendo un límite en las frecuencias altas.
La energía del vacío final, que es la suma regularizada de las energías de las oscilaciones del estado fundamental de todos los campos de la teoría, no tiene ninguna buena razón para ser cero y, de hecho, se esperaría que tuviera una escala natural.
\begin{equation}
\Lambda \sim m_P^4\ ,\label{4.75}
\end{equation}
donde la masa de Planck $m_P$ es aproximadamente $10^{19}$ GeV o $10^{-5}$ gramos.
Las observaciones del universo a gran escala nos permiten limitar el valor real de $\Lambda$, que resulta ser menor que (4,75) en al menos un factor de $10^{120}$.
Ésta es la mayor discrepancia conocida entre la estimación teórica y la restricción observacional en física, y convence a muchas personas de que el ``problema de la constante cosmológica'' es uno de los problemas sin resolver más importantes en la actualidad.
Por otro lado las observaciones no nos dicen que $\Lambda$ sea estrictamente cero, y de hecho permiten valores que pueden tener importantes consecuencias para la evolución del universo.
Por lo tanto, este error de Einstein sigue atormentando tanto a los físicos, que quisieran entender por qué es tan pequeño, como a los astrónomos, que quisieran determinar si realmente es lo suficientemente pequeño como para ignorarlo.

Una generalización algo menos intrigante de la acción de Hilbert sería incluir escalares de más de segundo orden en las derivadas de la métrica.
Podríamos imaginar una acción de la forma
\begin{equation}
S = \int d^nx \g (R + \alpha_1 R^2 + \alpha_2 R_\mn R^\mn
+\alpha_3 g^\mn \nabla_\mu R \nabla_\nu R +\cdots)\ ,\label{4.76}
\end{equation}
donde los $\alpha$ son constantes de acoplamiento y los puntos representan cualquier otro escalar que podamos hacer a partir del tensor de curvatura, sus contracciones y sus derivadas.
Tradicionalmente, tales términos han sido descuidados por el argumento razonable de que simplemente complican una teoría que ya es estéticamente agradable y empíricamente exitosa.
Sin embargo, hay al menos tres razones más importantes para este abandono.
En primer lugar, como veremos más adelante, las ecuaciones de Einstein conducen a un problema de valor inicial bien planteado para la métrica, en el que las ``coordenadas'' y los ``momentos'' especificados en un momento inicial pueden utilizarse para predecir la evolución futura.
Con términos de derivadas más altas, necesitaríamos no sólo esos datos, sino también un cierto número de derivadas de los momentos.
En segundo lugar, la principal fuente de insatisfacción con la Relatividad General por parte de los físicos de partículas es que no puede renormalizarse (hasta donde sabemos), y los lagrangianos con derivadas superiores tienden generalmente a hacer que las teorías sean menos renormalizables en lugar de más.
En tercer lugar, por los mismos argumentos que utilizamos anteriormente al hablar de las limitaciones del principio de equivalencia, los términos extra en (4.76) deberían estar suprimidos (por potencias de la masa de Planck a alguna potencia) en relación con el término habitual de Hilbert, y por lo tanto no se esperaría que tuvieran ninguna importancia práctica para el mundo de baja energía.
Ninguna de estas razones es completamente convincente y, de hecho, la gente sigue considerando este tipo de teorías, pero en su mayor parte estos modelos no atraen mucha atención.

Un conjunto de modelos que sí atraen la atención se conocen como {\bf teorías del tensor escalar} de la gravedad, ya que involucran tanto el tensor métrico $g_\mn$ como un campo escalar fundamental, $\lambda$.
La acción se puede escribir.
\begin{equation}
S = \int d^nx \g \left[f(\lambda)R+{1\over 2}g^\mn(\p\mu\lambda)
(\p\nu\lambda) -V(\lambda)\right]\ ,\label{4.77}
\end{equation}
donde $f(\lambda)$ y $V(\lambda)$ son funciones que definen la teoría.
Recuerde de (4.68) que el coeficiente del escalar de Ricci en Relatividad General convencional es proporcional a la inversa de la constante de Newton $G$.
Entonces, en las teorías del tensor escalar, donde este coeficiente se reemplaza por alguna función de un campo que puede variar a lo largo del Espacio-Tiempo, la ``fuerza'' de la gravedad (medida por el valor local de la constante de Newton) será diferente de un lugar a otro lugar y de un instante a otro instante.
De hecho, la teoría del tensor escalar más famosa, inventada por Brans y Dicke y que ahora lleva su nombre, se inspiró en una sugerencia de Dirac de que la constante gravitacional varía con el tiempo.
Dirac había notado que había algunas coincidencias numéricas interesantes que se podían descubrir tomando combinaciones de números cosmológicos como la constante de Hubble $H_0$ (una medida de la tasa de expansión del universo) y parámetros típicos de la física de partículas como la masa del pión, $m_\pi$.
Por ejemplo,
\begin{equation}
{{m_\pi^3}\over{H_0}}\sim {{cG}\over{\hbar^2}}\,.\label{4.78}
\end{equation}
Si asumimos por el momento que esta relación no es simplemente un accidente, nos enfrentamos al problema de que la ``constante'' de Hubble en realidad cambia con el tiempo (en la mayoría de los modelos cosmológicos), mientras que las otras cantidades convencionalmente no lo hacen.
Por tanto, Dirac propuso que, de hecho, $G$ variaba con el tiempo, de tal manera que se mantuviera (4.78); Satisfacer esta propuesta fue la motivación de Brans y Dicke.
Hoy en día, las pruebas experimentales de la Relatividad General son lo suficientemente precisas como para que podamos afirmar con confianza que, si la teoría de Brans-Dicke es correcta, el cambio predicho en $G$ a lo largo del espacio y del tiempo debe ser muy pequeño, mucho más lento que el necesario para satisfacer la hipótesis de Dirac.
(Consulte Weinberg para obtener detalles sobre la teoría de Brans-Dicke y las pruebas experimentales).
Sin embargo, todavía se está trabajando mucho en otros tipos de teorías de tensores escalares, que resultan vitales en la teoría de supercuerdas y pueden tener importantes consecuencias en el universo primitivo.

Como última alternativa a la Relatividad General, debemos mencionar la posibilidad de que la conexión realmente no se derive de la métrica, sino que de hecho tenga una existencia independiente como campo fundamental.
Lo dejaremos como ejercicio para que usted demuestre que es posible considerar la acción convencional para la Relatividad General pero trátela como una función tanto de la métrica $g_\mn$ como de una conexión libre de torsión $\Gamma^\lambda_{\rho\sigma}$, y las ecuaciones de El movimiento derivado de variar dicha acción con respecto a la conexión implica que $\Gamma^\lambda_{\rho\sigma}$ es en realidad la conexión Christoffel asociada con $g_\mn$.
Podríamos eliminar la exigencia de que la conexión esté libre de torsión, en cuyo caso el tensor de torsión podría conducir a grados de libertad de propagación adicionales.
Sin entrar en detalles, la razón básica por la que tales teorías no reciben mucha atención es simplemente porque la torsión es en sí misma un tensor; no hay nada que lo distinga de otros campos tensoriales ``no gravitacionales''.
Por lo tanto, realmente no perdemos ninguna generalidad al considerar teorías de conexiones libres de torsión (que conducen a la Relatividad General) más cualquier número de campos tensoriales, que podemos nombrar como queramos.

Teniendo en cuenta la posibilidad de que una de estas alternativas (o, más probablemente, algo en lo que aún no hayamos pensado) se realice realmente en la naturaleza, durante el resto del curso trabajaremos bajo el supuesto de que la Relatividad General, que se basa en las ecuaciones de Einstein o la acción de Hilbert, es la teoría correcta y determine sus consecuencias.
Estas consecuencias, por supuesto, están constituidas por las soluciones de las ecuaciones de Einstein para diversas fuentes de energía y momento, y el comportamiento de las partículas de prueba en estas soluciones.
Antes de considerar soluciones específicas en detalle, veamos de manera más abstracta el problema del valor inicial en la Relatividad General.

En la mecánica newtoniana clásica, el comportamiento de una sola partícula está, por supuesto, gobernado por ${\bf f} = m{\bf a}$.
Si la partícula se mueve bajo la influencia de algún campo de energía potencial $\Phi(x)$, entonces la fuerza es ${\bf f} = -\nabla \Phi$ y la partícula obedece
\begin{equation}
m{{d^2x^i}\over{dt^2}}=-\p{i}\Phi\,.\label{4.79}
\end{equation}
Ésta es una ecuación diferencial de segundo orden para $x^i(t)$, que podemos reformular como un sistema de dos ecuaciones acopladas de primer orden introduciendo el momentum ${\bf p}$:
\begin{align}
{{dp^i}\over{dt}} &=  -\p{i}\Phi \notag \\
{{dx^i}\over{dt}}  &=  {1\over m}p^i\,. \label{4.80}
\end{align}
El problema de valores iniciales es simplemente el procedimiento de especificar un ``estado'' $(x^i,p^i)$ que sirve como condición de frontera con la cual (4.80) puede resolverse de manera única.
Puede pensar que (4.80) le permite, una vez que se le dan las coordenadas y los momentos en algún momento $t$, evolucionarlos hacia adelante una cantidad infinitesimal hasta un tiempo $t+\delta t$ e iterar este procedimiento para obtener la solución completa.

Nos gustaría formular el problema análogo en la Relatividad General.
Las ecuaciones de Einstein $G_\mn = 8\pi G T_\mn$ son, por supuesto, covariantes; no señalan una noción preferida de ``tiempo'' a través del cual un Estado puede evolucionar.
Sin embargo, podemos elegir manualmente una hipersuperficie espacial (o ``corte'') $\Sigma$, especificar datos iniciales sobre esa hipersuperficie y ver si podemos evolucionar de forma única desde ella hasta una hipersuperficie en el futuro.
(``Hyper'' porque una porción de tiempo constante en cuatro dimensiones será tridimensional, mientras que las ``superficies'' son convencionalmente bidimensionales.)
Este proceso viola la covarianza manifiesta de la teoría, pero si tenemos cuidado deberíamos terminar con una formulación que sea equivalente a resolver todas las ecuaciones de Einstein de una vez en todo el Espacio-Tiempo.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/four09.pdf}
\end{figure}

Dado que la métrica es la variable fundamental, nuestra primera suposición es que debemos considerar los valores $g_\mn |_\Sigma$ de la métrica en nuestra hipersuperficie como las ``coordenadas'' y las derivadas del tiempo $\p{t}g_\mn |_\Sigma$ (con respecto a alguna coordenada de tiempo especificada). ) son los ``momentos'', que juntos especifican el estado.
(También habrá coordenadas y momentos para los campos de materia, que no consideraremos explícitamente).
De hecho, las ecuaciones $G_\mn = 8\pi G T_\mn$ involucran segundas derivadas de la métrica con respecto al tiempo (dado que la conexión involucra primeras derivadas de la métrica y el tensor de Einstein involucra primeras derivadas de la conexión), por lo que parece que estamos en el camino correcto.
Sin embargo, la identidad de Bianchi nos dice que $\nabla_\mu G^\mn=0$.
Podemos reescribir esta ecuación como
\begin{equation}
\p0 G^{0\nu}=-\p{i}G^{i\nu}-\Gamma^\mu_{\mu\lambda}G^{\lambda\nu}
-\Gamma^\nu_{\mu\lambda}G^{\mu\lambda}\,.\label{4.81}
\end{equation}
Una mirada atenta al lado derecho revela que no existen derivadas temporales de tercer orden; por lo tanto no puede haber ninguno en el lado izquierdo.
Por lo tanto, aunque $G^\mn$ en su conjunto implica derivadas temporales de segundo orden de la métrica, los componentes específicos $G^{0\nu}$ no.
De los diez componentes independientes en las ecuaciones de Einstein, los cuatro representados por
\begin{equation}
G^{0\nu}=8\pi GT^{0\nu}\label{4.82}
\end{equation}
no se puede utilizar para evolucionar los datos iniciales $(g_\mn,\p{t}g_\mn)_\Sigma$.
Más bien, sirven como {\bf restricciones} sobre estos datos iniciales; No somos libres de especificar cualquier combinación de la métrica y sus derivadas temporales en la hipersuperficie $\Sigma$, ya que deben obedecer las relaciones (4.82).
Las ecuaciones restantes,
\begin{equation}
G^{ij}=8\pi GT^{ij}\label{4.83}
\end{equation}
son las ecuaciones de evolución dinámica de la métrica.
Por supuesto, estas son sólo seis ecuaciones para las diez funciones desconocidas $g_\mn(x^\sigma)$, por lo que la solución implicará inevitablemente una ambigüedad cuádruple.
Esta es simplemente la libertad que ya hemos mencionado, de elegir las cuatro funciones de coordenadas a lo largo del espaciotiempo.

Es un ejercicio sencillo pero poco esclarecedor examinar (4.83) para encontrar que no aparecen todas las derivadas segundas de la métrica.
De hecho encontramos que $\p{t}^2g_{ij}$ aparece en (4.83), pero no $\p{t}^2g_{0\nu}$.
Por lo tanto, un ``estado'' en la Relatividad General consistirá en una especificación de los componentes espaciales de la métrica $g_{ij}|_\Sigma$ y sus primeras derivadas $\p{t}g_{ij}|_\Sigma$ en la hipersuperficie $\Sigma$, a partir de la cual podemos determinar la evolución futura usando ( 4.83), hasta una ambigüedad inevitable en la fijación del resto de componentes $g_{0\nu}$.
La situación es precisamente análoga a la del electromagnetismo, donde sabemos que ninguna cantidad de datos iniciales puede ser suficiente para determinar la evolución de forma única, ya que siempre habrá libertad para realizar una transformación de calibre $A_\mu \rightarrow A_\mu +\p\mu\lambda$.
En Relatividad General, pues, las transformaciones de coordenadas desempeñan un papel que recuerda al de las transformaciones gauge en electromagnetismo, en el sentido de que introducen ambigüedad en la evolución temporal.

Una forma de afrontar este problema es simplemente ``elegir un calibre''. En electromagnetismo, esto significa imponer una condición al potencial vectorial $A_\mu$, lo que restringirá nuestra libertad para realizar transformaciones de calibre.
Por ejemplo podemos elegir el calibre de Lorentz, en el que $\nabla_\mu A^\mu=0$, o el calibre temporal, en el que $A_0=0$.
Podemos hacer algo similar en la Relatividad General, arreglando nuestro sistema de coordenadas.
Una opción popular es {\bf calibre armónico} (también conocido como calibre de Lorentz y muchos otros nombres), en el que
\begin{equation}
\Box x^\mu =0\,.\label{4.84}
\end{equation}
Aquí $\Box = \nabla^\mu\nabla_\mu$ es la covariante D'Alembertiana, y es crucial darse cuenta cuando tomamos la derivada covariante de que las cuatro funciones $x^\mu$ son solo funciones, no componentes de un vector.
Por lo tanto, esta condición es simplemente
\begin{align}
0 &=  \Box x^\mu \notag \\
&=  g^{\rho\sigma}\p\rho\p\sigma x^\mu - g^{\rho\sigma}
\Gamma^\lambda_{\rho\sigma}\p\lambda x^\mu \notag \\
&=  -g^{\rho\sigma}\Gamma^\lambda_{\rho\sigma}\,. \label{4.85}
\end{align}
En el espacio plano, por supuesto, las coordenadas cartesianas (en las que $\Gamma^\lambda_{\rho\sigma}=0$) son coordenadas armónicas.
(Como principio general, cualquier función $f$ que satisfaga $\Box f=0$ se denomina ``función armónica''.)

Para ver que esta elección de coordenadas soluciona con éxito nuestra libertad de calibre, reescribamos la condición (4.84) en una forma algo más simple.
Tenemos
\begin{equation}
g^{\rho\sigma}\Gamma^\mu_{\rho\sigma}={1\over 2}g^{\rho\sigma}
g^\mn(\p\rho g_{\sigma\nu}+\p\sigma g_{\nu\rho} -\p\nu
g_{\rho\sigma})\ ,\label{4.86}
\end{equation}
de la definición de los símbolos de Christoffel.
Mientras tanto, de $\p\rho(g^\mn g_{\sigma\nu})=\p\rho\delta^\mu_\sigma=0$ tenemos
\begin{equation}
g^\mn\p\rho g_{\sigma\nu} = - g_{\sigma\nu}\p\rho g^\mn\,.\label{4.87}
\end{equation}
Además, de nuestra exploración anterior de la variación del determinante de la métrica (4.65), tenemos
\begin{equation}
{1\over 2}g_{\rho\sigma}\p\nu g^{\rho\sigma} =
-{1\over{\g}}\,\p\nu \g\,.\label{4.88}
\end{equation}
Poniéndolo todo junto, encontramos que (en general),
\begin{equation}
g^{\rho\sigma}\Gamma^\mu_{\rho\sigma}={1\over{\g}}\,\p\lambda
(\g g^{\lambda\mu})\,.\label{4.89}
\end{equation}
Por lo tanto, la condición de calibre armónico (4.85) es equivalente a
\begin{equation}
\p\lambda(\g g^{\lambda\mu})=0\,.\label{4.90}
\end{equation}
Tomando la derivada parcial de esto con respecto a $t=x^0$ se obtiene
\begin{equation}
{{\partial^2}\over{\partial t^2}}(\g g^{0\nu})=
-{{\partial}\over{\partial x^i}}\left[{{\partial}\over
{\partial t}}(\g g^{i\nu})\right]\,.\label{4.91}
\end{equation}
Esta condición representa una ecuación diferencial de segundo orden para los componentes métricos previamente no restringidos $g^{0\nu}$, en términos de los datos iniciales dados.
Por lo tanto, hemos logrado fijar nuestra libertad de calibre, ya que ahora podemos resolver la evolución de toda la métrica en coordenadas armónicas.
(Al menos a nivel local; hemos estado pasando por alto el hecho de que nuestra elección de calibre puede no estar bien definida a nivel global y tendríamos que recurrir a trabajar en parches como de costumbre.
El mismo problema aparece en las teorías de calibre en física de partículas.)
Tenga en cuenta que todavía nos queda algo de libertad; nuestra condición de calibre (4.84) restringe cómo se extienden las coordenadas desde nuestra hipersuperficie inicial $\Sigma$ a lo largo del Espacio-Tiempo, pero aún podemos elegir las coordenadas $x^i$ en $\Sigma$ como queramos.
Esto corresponde a que realizar una transformación de coordenadas $x^\mu \rightarrow x^\mu +\delta^\mu$, con $\Box \delta^\mu=0$, no viola la condición de calibre armónico.

Por lo tanto, tenemos un problema de valor inicial bien definido para la Relatividad General; un estado se especifica mediante los componentes espaciales de la métrica y sus derivadas temporales en una hipersuperficie espacial $\Sigma$; Dadas éstas, las componentes espaciales (4.83) de las ecuaciones de Einstein nos permiten evolucionar la métrica hacia adelante en el tiempo, hasta una ambigüedad en la elección de coordenadas que puede resolverse mediante la elección del gauge.
Debemos tener en cuenta que los datos iniciales no son arbitrarios, sino que deben obedecer a las restricciones (4.82).
(Una vez que imponemos las restricciones a alguna hipersuperficie espacial, las ecuaciones de movimiento garantizan que sigan satisfechas, como puede comprobar).
Las restricciones sirven para garantizar que el resultado sigue siendo covariante del espaciotiempo después de haber dividido nuestra variedad en ``espacio'' y ``tiempo''. En concreto, la restricción $G^{i0}=8\pi GT^{i0}$ implica que la evolución es independiente de nuestra elección de coordenadas en $\Sigma$, mientras que $G^{00}=8\pi GT^{00}$ refuerza la invariancia bajo diferentes formas de dividir el espaciotiempo en hipersuperficies espaciales.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/four10.pdf}
\end{figure}

Una vez que hemos visto cómo plantear las ecuaciones de Einstein como un problema de valor inicial, una cuestión de crucial importancia es la existencia de soluciones al problema.
Es decir, una vez que hemos especificado una hipersuperficie espacial con datos iniciales, ¿hasta qué punto podemos tener la garantía de que se determinará un Espacio-Tiempo único? Aunque se puede trabajar mucho para responder esta pregunta con cierta precisión, es bastante sencillo entender las formas en que una solución bien definida puede dejar de existir, que ahora consideramos.

Lo más sencillo es considerar primero el problema de la evolución de campos de materia en un Espacio-Tiempo de fondo fijo, en lugar de la evolución de la métrica en sí.
Por lo tanto, consideramos una hipersuperficie espacial $\Sigma$ en alguna variedad $M$ con métrica fija $g_\mn$ y, además, analizamos algún subconjunto conectado $S$ en $\Sigma$.
Nuestro principio rector será que ninguna señal puede viajar más rápido que la velocidad de la luz; por lo tanto, la ``información'' sólo fluirá a lo largo de trayectorias nulas o temporales (no necesariamente geodésicas).
Definimos el {\bf dominio futuro de dependencia} de $S$, denotado $D^+(S)$, como el conjunto de todos los puntos $p$ tales que {\it cada} curva inextensible, temporal o nula que se mueve en el pasado y que pasa por $p$ debe cruzarse con $S$.
(``Inextensible'' simplemente significa que la curva continúa para siempre, sin terminar en algún punto finito.)
Interpretamos esta definición de tal manera que $S$ en sí es un subconjunto de $D^+(S)$.
(Por supuesto, una formulación rigurosa no requiere interpretación adicional además de las definiciones, pero no estamos siendo tan rigurosos como podríamos ser ahora).
De manera similar, definimos el dominio pasado de dependencia $D^-(S)$ de la misma manera, pero con ``movimiento pasado'' reemplazado por ``movimiento futuro''. En términos generales, algunos puntos en $M$ estarán en uno de los dominios de la dependencia, y algunos quedarán fuera; definimos el límite de $D^+(S)$ como el {\bf futuro horizonte de Cauchy} $H^+(S)$, y de la misma manera el límite de $D^-(S)$ es el horizonte de Cauchy pasado $H^-(S)$.
Puedes convencerte de que ambas son superficies nulas.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/four11.pdf}
\end{figure}

La utilidad de estas definiciones debería ser evidente; Si nada se mueve más rápido que la luz, las señales no pueden propagarse fuera del cono de luz de ningún punto $p$.
Por lo tanto, si cada curva que queda dentro de este cono de luz debe cruzar $S$, entonces la información especificada en $S$ debería ser suficiente para predecir cuál es la situación en $p$.
(Es decir, los datos iniciales de los campos de materia proporcionados en $S$ se pueden usar para resolver el valor de los campos en $p$.)
El conjunto de todos los puntos para los cuales podemos predecir lo que sucede sabiendo lo que sucede en $S$ es simplemente la unión $D^+(S)\cup D^-(S)$.

Podemos extender fácilmente estas ideas desde el subconjunto $S$ a toda la hipersuperficie $\Sigma$.
El punto importante es que $D^+(\Sigma)\cup D^-(\Sigma)$ podría no ser todo $M$, incluso si $\Sigma$ en sí parece una hipersuperficie perfectamente respetable que se extiende por todo el espacio.
Hay varias maneras en que esto puede suceder.
Una posibilidad es que hayamos elegido una hipersuperficie ``mala'' (aunque es difícil dar una prescripción general sobre cuándo una hipersuperficie es mala en este sentido).
Consideremos el espacio de Minkowski y una hipersuperficie espacial $\Sigma$ que permanece en el pasado del cono de luz de algún punto.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{imagenes/four12.pdf}
\end{figure}

\noindent
En este caso, $\Sigma$ es una bonita superficie espacial, pero está claro que $D^+(\Sigma)$ termina en el cono de luz y no podemos usar información sobre $\Sigma$ para predecir lo que sucede en todo el espacio de Minkowski.
Por supuesto, hay otras superficies que podríamos haber elegido para las cuales el dominio de dependencia habría sido la variedad completa, por lo que esto no nos preocupa demasiado.

Un ejemplo algo menos trivial se conoce como {\bf espacio de Misner}.
Este es un Espacio-Tiempo bidimensional con la topología de $\R^1\times S^1$ y una métrica para la cual los conos de luz se inclinan progresivamente a medida que avanza en el tiempo.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/four13.pdf}
\end{figure}
Pasado cierto punto, es posible viajar en una trayectoria temporal que rodea el $S^1$ y regresa a sí mismo; esto se conoce como {\bf curva temporal cerrada}.
Si hubiéramos especificado una superficie $\Sigma$ en este pasado de este punto, entonces ninguno de los puntos en la región que contiene curvas temporales cerradas está en el dominio de dependencia de $\Sigma$, ya que las curvas temporales cerradas en sí mismas no se cruzan con $\Sigma$.
Obviamente, este es un problema peor que el anterior, ya que no parece existir un problema de valor inicial bien definido en este Espacio-Tiempo.
(En realidad, problemas como este son objeto de algún interés de investigación actual, por lo que no afirmaré que el problema esté resuelto).

Un último ejemplo lo proporciona la existencia de singularidades, puntos que no están en la variedad aunque se pueda llegar a ellos viajando a lo largo de una geodésica a lo largo de una distancia finita.
Normalmente, esto ocurre cuando la curvatura se vuelve infinita en algún punto; si esto sucede, ya no se puede decir que el punto sea parte del Espacio-Tiempo.
Tal ocurrencia puede llevar al surgimiento de un horizonte de Cauchy --- un punto $p$ que está en el futuro de una singularidad no puede estar en el dominio de dependencia de una hipersuperficie con respecto al pasado de la singularidad, porque habrá curvas de $p$ que simplemente termina en la singularidad.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/four14.pdf}
\end{figure}

Todos estos obstáculos también pueden surgir en el problema del valor inicial de la Relatividad General, cuando intentamos desarrollar la métrica misma a partir de los datos iniciales.
Sin embargo, tienen diferentes grados de problemática.
La posibilidad de elegir una hipersuperficie inicial ``mala'' no surge muy a menudo, especialmente porque la mayoría de las soluciones se encuentran globalmente (resolviendo las ecuaciones de Einstein a lo largo del Espacio-Tiempo).
La única situación en la que hay que tener cuidado es en la solución numérica de las ecuaciones de Einstein, donde una mala elección de la hipersuperficie puede conducir a dificultades numéricas incluso si en principio existe una solución completa.
Las curvas temporales cerradas parecen ser algo que la Relatividad General se esfuerza por evitar; ciertamente hay soluciones que las contienen, pero la evolución a partir de datos iniciales genéricos generalmente no las produce.
Las singularidades, por el contrario, son prácticamente inevitables.
El simple hecho de que la fuerza gravitacional sea siempre atractiva tiende a unir la materia, aumentando la curvatura y, en general, conduciendo a algún tipo de singularidad.
Esto es algo con lo que aparentemente debemos aprender a vivir, aunque hay cierta esperanza de que una teoría bien definida de la gravedad cuántica elimine las singularidades de la RG clásica.





\chapter{Más Geometría}
%\addcontentsline{toc}{chapter}{Más Geometría}


Una vez que se comprende cómo se adaptan las leyes de la física al Espacio-Tiempo curvo, es innegable que resulta tentador comenzar con las aplicaciones.
Sin embargo, algunas técnicas matemáticas adicionales simplificarán mucho nuestra tarea, por lo que haremos una breve pausa para explorar un poco más la geometría de las variedades.

Cuando analizamos las variedades en la sección 2, introdujimos mapas entre dos variedades diferentes y cómo se podrían componer los mapas.
Pasamos ahora al uso de tales mapas para transportar campos tensoriales de una variedad a otra.
Por tanto, consideramos dos variedades $M$ y $N$, posiblemente de diferente dimensión, con sistemas de coordenadas $x^\mu$ y $y^\alpha$, respectivamente.
Imaginamos que tenemos un mapa $\phi:M\rightarrow N$ y una función $f:N\rightarrow \R$.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/five01.pdf}
\end{figure}

\noindent
Es obvio que podemos componer $\phi$ con $f$ para construir un mapa $(f\circ\phi):M\rightarrow\R$, que es simplemente una función en $M$.
Esta construcción es lo suficientemente útil como para recibir su propio nombre; definimos el {\bf pullback} de $f$ por $\phi$, denotado $\phi_*f$, por
\begin{equation}
\phi_* f = (f\circ\phi)\,.\label{5.1}
\end{equation}
El nombre tiene sentido, ya que pensamos en $\phi_*$ como ``retirar'' la función $f$ de $N$ a $M$.

Podemos retirar funciones, pero no podemos impulsarlas.
Si tenemos una función $g:M\rightarrow\R$, no hay manera de que podamos componer $g$ con $\phi$ para crear una función en $N$; Las flechas no encajan correctamente.
Pero recuerde que se puede considerar un vector como un operador derivativo que asigna funciones suaves a números reales.
Esto nos permite definir el {\bf pushforward} de un vector; Si $V(p)$ es un vector en un punto $p$ en $M$, definimos el vector de avance $\phi^*V$ en el punto $\phi(p)$ en $N$ dando su acción sobre funciones en $N$:
\begin{equation}
(\phi^*V)(f) = V(\phi_*f)\,.\label{5.2}
\end{equation}
Entonces, para impulsar un campo vectorial decimos ``la acción de $\phi^*V$ en cualquier función es simplemente la acción de $V$ en el retroceso de esa función''.

Esto es un poco abstracto y sería bueno tener una descripción más concreta.
Sabemos que una base para los vectores en $M$ viene dada por el conjunto de derivadas parciales $\p\mu={{\partial}\over{\partial x^\mu}}$, y una base en $N$ está dada por el conjunto de derivadas parciales $\p\alpha= {{\partial}\over{\partial y^\alpha}}$.
Por lo tanto nos gustaría relacionar los componentes de $V=V^\mu\p\mu$ con los de $(\phi^*V)=(\phi^*V)^\alpha \p\alpha$.
Podemos encontrar la relación buscada aplicando el vector empujado hacia adelante a una función de prueba y usando la regla de la cadena (2.3):
\begin{align}
(\phi^*V)^\alpha\p\alpha f  &=  V^\mu\p\mu (\phi_* f) \notag \\
&=  V^\mu\p\mu (f\circ\phi) \notag \\
&=  V^\mu {{\partial y^\alpha}\over{\partial x^\mu}}\p\alpha f\,.
\label{5.3}
\end{align}
Esta sencilla fórmula hace irresistible pensar en la operación de avance $\phi^*$ como un operador matricial, $(\phi^*V)^\alpha = (\phi^*)^\alpha{}_\mu V^\mu$, con la matriz dada por
\begin{equation}
(\phi^*)^\alpha{}_\mu = {{\partial y^\alpha}\over{\partial x^\mu}}
\,.\label{5.4}
\end{equation}
El comportamiento de un vector bajo un avance tiene, por tanto, un parecido inequívoco con la ley de transformación de vectores bajo un cambio de coordenadas.
De hecho es una generalización, ya que cuando $M$ y $N$ son la misma variedad las construcciones son (como veremos) idénticas; pero no te dejes engañar, ya que en general $\mu$ y $\alpha$ tienen diferentes valores permitidos, y no hay ninguna razón para que la matriz ${{\partial y^\alpha} /{\partial x^\mu}}$ sea invertible.

Es un ejercicio gratificante convencerse de que, aunque puede impulsar vectores hacia adelante desde $M$ a $N$ (dado un mapa $\phi: M\rightarrow N$), en general no puede retroceder; simplemente sigue intentando inventar una construcción apropiada hasta que se evidencie la inutilidad del intento.
Dado que las formas unicas son duales a los vectores, no debería sorprenderle saber que las formas unicas se pueden retirar (pero en general no avanzar).
Para hacer esto, recuerde que las formas uniformes son aplicaciones lineales de vectores a números reales.
Por lo tanto, el retroceso $\phi_*\omega$ de un $\omega$ de una sola forma en $N$ puede definirse por su acción sobre un vector $V$ en $M$, equiparándolo con la acción del propio $\omega$ en el avance de $V$:
\begin{equation}
(\phi_*\omega)(V)=\omega(\phi^*V)\,.\label{5.5}
\end{equation}
Una vez más, hay una descripción matricial simple del operador de retroceso en los formularios, $(\phi_*\omega)_\mu =(\phi_*)_\mu{}^\alpha \omega_\alpha$, que podemos derivar usando la regla de la cadena.
esta dado por
\begin{equation}
(\phi_*)_\mu{}^\alpha = {{\partial y^\alpha}\over{\partial x^\mu}}
\,.\label{5.6}
\end{equation}
Es decir, es la misma matriz que el pushforward (5.4), pero, por supuesto, se contrae un índice diferente cuando la matriz actúa para retroceder formas únicas.

Hay una manera de pensar por qué los retrocesos y los avances funcionan en algunos objetos pero no en otros, lo que puede ser útil o no.
Si denotamos el conjunto de funciones suaves en $M$ por ${\cal F}(M)$, entonces un vector $V(p)$ en un punto $p$ en $M$ ({\it ie}, un elemento del espacio tangente $T_pM$) Puede considerarse como un operador desde ${\cal F}(M)$ hasta $\R$.
Pero ya sabemos que el operador de retroceso en funciones asigna ${\cal F}(N)$ a ${\cal F}(M)$ (al igual que $\phi$ asigna $M$ a $N$, pero en la dirección opuesta).
Por lo tanto, podemos definir el avance $\phi_*$ que actúa sobre vectores simplemente componiendo mapas, como definimos primero el retroceso de funciones:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/five02.pdf}
\end{figure}

\noindent
De manera similar, si $T_qN$ es el espacio tangente en un punto $q$ en $N$, entonces un $\omega$ uniforme en $q$ ({\it ie}, un elemento del espacio cotangente $T_q^*N$) Puede considerarse como un operador desde $T_qN$ hasta $\R$.
Dado que el avance $\phi^*$ asigna $T_pM$ a $T_{\phi(p)}N$, el retroceso $\phi_*$ de una forma única también se puede considerar como una mera composición de mapas:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/five03.pdf}
\end{figure}

\noindent
Si esto no es útil, no te preocupes.
Pero mantenga claro lo que existe y lo que no; Los conceptos reales son simples, es solo recordar qué mapa va en qué dirección lo que genera confusión.

Recordará además que un tensor $(0,l)$ --- uno con $l$ índices inferiores y sin índices superiores --- es un mapa lineal del producto directo de los vectores $l$ a $\R$.
Por lo tanto, podemos retirar no sólo formas únicas, sino también tensores con un número arbitrario de índices inferiores.
La definición es simplemente la acción del tensor original sobre los vectores empujados hacia adelante:
\begin{equation}
(\phi_* T)(V^{(1)}, V^{(2)},\ldots ,V^{(l)})=T(\phi^*V^{(1)},
\phi^*V^{(2)},\ldots ,\phi^*V^{(l)})\ ,\label{5.7}
\end{equation}
donde $T_{\alpha_1 \cdots \alpha_l}$ es un tensor $(0,l)$ en $N$.
De manera similar, podemos impulsar cualquier tensor $(k,0)$ $S^{\mu_1 \cdots \mu_k}$ actuando en formas únicas retraídas:
\begin{equation}
(\phi^* S)(\omega^{(1)}, \omega^{(2)},\ldots ,\omega^{(k)})=
S(\phi_*\omega^{(1)}, \phi_*\omega^{(2)},\ldots ,\phi_*\omega^{(k)})
\,.\label{5.8}
\end{equation}
Afortunadamente, las representaciones matriciales de avance (5.4) y retroceso (5.6) se extienden a los tensores de rango superior simplemente asignando una matriz a cada índice; por lo tanto, para el retroceso de un tensor $(0,l)$, tenemos
\begin{equation}
(\phi_* T)_{\mu_1 \cdots \mu_l} = {{\partial y^{\alpha_1}}
\over{\partial x^{\mu_1}}}\cdots{{\partial y^{\alpha_l}}
\over{\partial x^{\mu_l}}}T_{\alpha_1 \cdots \alpha_l}\ ,\label{5.9}
\end{equation}
mientras que para el avance de un tensor $(k,0)$ tenemos
\begin{equation}
(\phi^* S)^{\alpha_1 \cdots \alpha_k} = {{\partial y^{\alpha_1}}
\over{\partial x^{\mu_1}}}\cdots{{\partial y^{\alpha_k}}
\over{\partial x^{\mu_k}}}S^{\mu_1 \cdots \mu_k}\,.\label{5.10}
\end{equation}
Por lo tanto, nuestra imagen completa es:

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/five04.pdf}
\end{figure}

\noindent
Tenga en cuenta que los tensores con índices superior e inferior generalmente no se pueden empujar hacia adelante ni hacia atrás.

Esta maquinaria se vuelve algo menos imponente una vez que la vemos en funcionamiento en un ejemplo sencillo.
Una ocurrencia común de un mapa entre dos variedades es cuando $M$ es en realidad una subvariedad de $N$; entonces hay un mapa obvio de $M$ a $N$ que simplemente lleva un elemento de $M$ al ``mismo'' elemento de $N$.
Considere nuestro ejemplo habitual, las dos esferas incrustadas en $\R^3$, como el lugar geométrico de los puntos a una unidad de distancia del origen.
Si ponemos las coordenadas $x^\mu=(\theta,\phi)$ en $M=S^2$ y $y^\alpha=(x,y,z)$ en $N=\R^3$, el mapa $\phi:M\rightarrow N$ viene dado por
\begin{equation}
\phi(\theta,\phi)=(\sin\theta \cos\phi,\sin\theta \sin\phi,
\cos\theta)\,.\label{5.11}
\end{equation}
En el pasado, consideramos la métrica $ds^2= d\,x^2+ d\,y^2+  d\,z^2$ en $\R^3$ y dijimos que induce una métrica $ d\,\theta^2 +\sin^2\theta~ d\,\phi^2$ en $S^2$, simplemente sustituyendo (5.11) en esta métrica plana en $\R^3$.
Realmente no justificamos tal afirmación en su momento, pero ahora podemos hacerlo.
(Por supuesto, sería más fácil si trabajáramos en coordenadas esféricas en $\R^3$, pero hacerlo de la manera más difícil es más ilustrativo).
La matriz de derivadas parciales viene dada por
\begin{equation}
{{\partial y^{\alpha}}\over{\partial x^{\mu}}}=
\left(\mqty{\cos\theta \cos\phi &\cos\theta \sin\phi &-\sin\theta \\
-\sin\theta \sin\phi &\sin\theta \cos\phi & 0 \\ }\right)\,.\label{5.12}
\end{equation}
La métrica de $S^2$ se obtiene simplemente retirando la métrica de $\R^3$,
\begin{align}
(\phi^* g)_\mn  &=  {{\partial y^{\alpha}}
\over{\partial x^{\mu}}}{{\partial y^{\beta}}
\over{\partial x^{\nu}}}g_{\alpha\beta} \notag \\
&=  \left(\mqty{1&0 \\  0& \sin^2\theta \\ }\right)\ ,\label{5.13}
\end{align}
como podrás comprobar fácilmente.
Una vez más, la respuesta es la misma que obtendríamos mediante una sustitución ingenua, pero ahora sabemos por qué.

Hemos tenido cuidado de enfatizar que un mapa $\phi:M\rightarrow N$ se puede usar para impulsar ciertas cosas hacia adelante y hacer retroceder otras.
La razón por la que generalmente no funciona en ambos sentidos se debe al hecho de que $\phi$ podría no ser invertible.
Si $\phi$ es invertible (y tanto $\phi$ como $\phi^{-1}$ son suaves, lo que siempre asumimos implícitamente), entonces define un difeomorfismo entre $M$ y $N$.
En este caso, $M$ y $N$ son la misma variedad abstracta.
La belleza de los difeomorfismos es que podemos usar tanto $\phi$ como $\phi^{-1}$ para mover tensores de $M$ a $N$; esto nos permitirá definir el avance y retroceso de tensores arbitrarios.
Específicamente, para un campo tensor $(k,l)$ $T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}$ en $M$, definimos el avance mediante
\begin{equation}
(\phi^*T)(\omega^{(1)},\ldots ,\omega^{(k)},V^{(1)},\ldots ,V^{(l)})
= T(\phi_*\omega^{(1)},\ldots ,\phi_*\omega^{(k)},
[\phi^{-1}]^*V^{(1)},\ldots ,[\phi^{-1}]^*V^{(l)})\ ,\label{5.14}
\end{equation}
donde los $\omega^{(i)}$ son formas únicas en $N$ y los $V^{(i)}$ son vectores en $N$.
En componentes esto se convierte en
\begin{equation}
(\phi^*T)^{\alpha_1 \cdots \alpha_k}{}_{\beta_1 \cdots \beta_l}
= {{\partial y^{\alpha_1}}
\over{\partial x^{\mu_1}}}\cdots{{\partial y^{\alpha_k}}
\over{\partial x^{\mu_k}}}{{\partial x^{\nu_1}}
\over{\partial y^{\beta_1}}}\cdots{{\partial x^{\nu_l}}
\over{\partial y^{\beta_l}}}T^{\mu_1 \cdots \mu_k}{}_{\nu_1
\cdots \nu_l}\,.\label{5.15}
\end{equation}
La aparición de la matriz inversa $\partial x^\nu/\partial y^\beta$ es legítima porque $\phi$ es invertible.
Tenga en cuenta que también podríamos definir el retroceso de la manera obvia, pero no es necesario escribir ecuaciones separadas porque el retroceso $\phi_*$ es el mismo que el avance a través del mapa inverso, $[\phi^{-1}]^*$.

Ahora estamos en condiciones de explicar la relación entre difeomorfismos y transformaciones de coordenadas.
La relación es que son dos maneras diferentes de hacer precisamente lo mismo.
Si lo desea, los difeomorfismos son ``transformaciones de coordenadas activas'', mientras que las transformaciones de coordenadas tradicionales son ``pasivas''. Considere una variedad $n$-dimensional $M$ con funciones de coordenadas $x^\mu :M\rightarrow \R^n$.
Para cambiar las coordenadas podemos simplemente introducir nuevas funciones $y^\mu :M\rightarrow \R^n$ (``mantener la variedad fija, cambiar los mapas de coordenadas''), o también podríamos introducir un difeomorfismo $\phi:M\rightarrow M$, después del cual las coordenadas serían simplemente las pullbacks $(\phi_*x)^\mu:M\rightarrow \R^n$ (``mover los puntos en el colector y luego evaluar las coordenadas de los nuevos puntos'').
En este sentido, (5.15) es realmente la ley de transformación del tensor, sólo que pensada desde un punto de vista diferente.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{imagenes/five05.pdf}
\end{figure}

Dado que un difeomorfismo nos permite retroceder y avanzar tensores arbitrarios, proporciona otra forma de comparar tensores en diferentes puntos de una variedad.
Dado un difeomorfismo $\phi:M\rightarrow M$ y un campo tensorial $T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(x)$, podemos formar la diferencia entre el valor del tensor en algún punto $p$ y $\phi_*[T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(\phi(p))]$, su valor en $\phi(p)$ retrocedido a $p$.
Esto sugiere que podríamos definir otro tipo de operador derivativo en campos tensoriales, uno que categorice la tasa de cambio del tensor a medida que cambia bajo el difeomorfismo.
Para ello, sin embargo, un único difeomorfismo discreto es insuficiente; requerimos una familia de difeomorfismos de un parámetro, $\phi_t$.
Esta familia se puede considerar como un mapa suave $\R\times M\rightarrow M$, de modo que para cada $t\in\R$ $\phi_t$ hay un difeomorfismo y $\phi_s\circ\phi_t=\phi_{s+t}$.
Tenga en cuenta que esta última condición implica que $\phi_0$ es el mapa de identidad.

Se puede considerar que las familias de difeomorfismos de un parámetro surgen de campos vectoriales (y viceversa).
Si consideramos lo que sucede con el punto $p$ bajo toda la familia $\phi_t$, está claro que describe una curva en $M$; dado que lo mismo será cierto para cada punto de $M$, estas curvas llenan la variedad (aunque puede haber degeneraciones donde los difeomorfismos tienen puntos fijos).
Podemos definir un campo vectorial $V^\mu(x)$ como el conjunto de vectores tangentes a cada una de estas curvas en cada punto, evaluado en $t=0$.
Un ejemplo de $S^2$ lo proporciona el difeomorfismo $\phi_t(\theta,\phi)=(\theta,\phi+t)$.

\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{imagenes/five06.pdf}
\end{figure}

Podemos invertir la construcción para definir una familia de difeomorfismos de un parámetro de cualquier campo vectorial.
Dado un campo vectorial $V^\mu(x)$, definimos las {\bf curvas integrales} del campo vectorial como aquellas curvas $x^\mu(t)$ que resuelven
\begin{equation}
{{dx^\mu}\over {dt}}=V^\mu\,.\label{5.16}
\end{equation}
Tenga en cuenta que esta ecuación de aspecto familiar debe interpretarse ahora en el sentido opuesto al habitual: se nos dan los vectores a partir de los cuales definimos las curvas.
Se garantiza que existirán soluciones a (5.16) siempre y cuando no hagamos nada tonto como toparse con el borde de nuestra variedad; cualquier texto estándar de geometría diferencial tendrá la prueba, que equivale a encontrar un sistema de coordenadas inteligente en el que el problema se reduzca al teorema fundamental de las ecuaciones diferenciales ordinarias.
Nuestros difeomorfismos $\phi_t$ representan ``flujo hacia abajo por las curvas integrales'', y el campo vectorial asociado se denomina {\bf generador} del difeomorfismo.
(Las curvas integrales se utilizan todo el tiempo en física elemental, pero no se les da el nombre.
Las ``líneas de flujo magnético'' trazadas por limaduras de hierro en presencia de un imán son simplemente las curvas integrales del vector del campo magnético {\bf B}.)

Entonces, dado un campo vectorial $V^\mu(x)$, tenemos una familia de difeomorfismos parametrizados por $t$, y podemos preguntar qué tan rápido cambia un tensor a medida que recorremos las curvas integrales.
Para cada $t$ podemos definir este cambio como
\begin{equation}
\Delta_t T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(p)
= \phi_{t*}[T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(\phi_t(p))]
- T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(p)\,.\label{5.17}
\end{equation}
Tenga en cuenta que ambos términos del lado derecho son tensores en $p$.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/five07.pdf}
\end{figure}

\noindent
Luego definimos la {\bf derivada de Lie} del tensor a lo largo del campo vectorial como
\begin{equation}
\lie_VT^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l} =
\lim_{t\rightarrow 0}\left({{\Delta_t
T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}}\over{t}}\right)\,.
\label{5.18}
\end{equation}
La derivada de Lie es un mapa de campos tensoriales $(k,l)$ a campos tensoriales $(k,l)$, que es manifiestamente independiente de las coordenadas.
Dado que la definición equivale esencialmente a la definición convencional de una derivada ordinaria aplicada a las funciones componentes del tensor, debe quedar claro que es lineal,
\begin{equation}
\lie_V(aT+bS) = a\lie_VT + b\lie_VS\ ,\label{5.19}
\end{equation}
y obedece la regla de Leibniz,
\begin{equation}
\lie_V(T\otimes S) = (\lie_VT)\otimes S+T\otimes(\lie_VS)\ ,\label{5.20}
\end{equation}
donde $S$ y $T$ son tensores y $a$ y $b$ son constantes.
La derivada de Lie es de hecho una noción más primitiva que la derivada covariante, ya que no requiere especificación de una conexión (aunque sí requiere un campo vectorial, por supuesto).
Un momento de reflexión muestra que se reduce a la derivada ordinaria de funciones,
\begin{equation}
\lie_Vf = V(f) = V^\mu\p\mu f\,.\label{5.21}
\end{equation}

Para discutir la acción de la derivada de Lie sobre los tensores en términos de otras operaciones que conocemos, es conveniente elegir un sistema de coordenadas adaptado a nuestro problema.
Específicamente, trabajaremos en coordenadas $x^\mu$ para las cuales $x^1$ es el parámetro a lo largo de las curvas integrales (y las otras coordenadas se eligen como queramos).
Entonces el campo vectorial toma la forma $V=\partial/\partial x^1$; es decir, tiene componentes $V^\mu =(1,0,0, \ldots, 0)$.
La magia de este sistema de coordenadas es que un difeomorfismo de $t$ equivale a una transformación de coordenadas de $x^\mu$ a $y^\mu=(x^1+t,x^2,\ldots,x^n)$.
Por lo tanto, de (5.6) la matriz de retroceso es simplemente
\begin{equation}
(\phi_{t*})_\mu{}^\nu = \delta^\nu_\mu\ ,\label{5.22}
\end{equation}
y los componentes del tensor retirados de $\phi_t(p)$ a $p$ son simplemente
\begin{equation}
\phi_{t*}[T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(\phi_t(p))]
=T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}(x^1+t,x^2,\ldots,x^n)
\,.\label{5.23}
\end{equation}
Entonces, en este sistema de coordenadas, la derivada de Lie se convierte en
\begin{equation}
\lie_VT^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l} =
{{\partial}\over{\partial x^1}}
T^{\mu_1 \cdots \mu_k}{}_{\nu_1 \cdots \mu_l}\ ,\label{5.24}
\end{equation}
y específicamente la derivada de un campo vectorial $U^\mu(x)$ es
\begin{equation}
\lie_VU^\mu = {{\partial U^\mu}\over{\partial x^1}}\,.\label{5.25}
\end{equation}
Aunque esta expresión claramente no es covariante, sabemos que el conmutador $[V,U]$ es un tensor bien definido, y en este sistema de coordenadas
\begin{align}
[V,U]^\mu  &=  V^\nu\p\nu U^\mu-U^\nu\p\nu V^\mu \notag \\
&=  {{\partial U^\mu}\over{\partial x^1}}\,.\label{5.26}
\end{align}
Por tanto la derivada de Lie de $U$ con respecto a $V$ tiene las mismas componentes en este sistema de coordenadas que el conmutador de $V$ y $U$; pero como ambos son vectores, deben ser iguales en cualquier sistema de coordenadas:
\begin{equation}
\lie_VU^\mu = [V,U]^\mu\,.\label{5.27}
\end{equation}
Como consecuencia inmediata, tenemos $\lie_VS=-\lie_WV$.
Debido a (5.27), al conmutador a veces se le llama ``corchete de mentira''.

Para derivar la acción de $\lie_V$ en un $\omega_\mu$ de una sola forma, comience considerando la acción en el escalar $\omega_\mu U^\mu$ para un campo vectorial arbitrario $U^\mu$.
Primero use el hecho de que la derivada de Lie con respecto a un campo vectorial se reduce a la acción del propio vector cuando se aplica a un escalar:
\begin{align}
\lie_V(\omega_\mu U^\mu)  &=  V(\omega_\mu U^\mu) \notag \\
&=  V^\nu\p\nu(\omega_\mu U^\mu) \notag \\
&=  V^\nu(\p\nu\omega_\mu)U^\mu + V^\nu\omega_\mu(\p\nu U^\mu)\,.
\label{5.28}
\end{align}
Luego use la regla de Leibniz en el escalar original:
\begin{align}
\lie_V(\omega_\mu U^\mu)  &=  (\lie_V\omega)_\mu U^\mu
+\omega_\mu (\lie_V U)^\mu  \notag \\
&=  (\lie_V\omega)_\mu U^\mu + \omega_\mu V^\nu\p\nu U^\mu
-\omega_\mu U^\nu\p\nu V^\mu\,.\label{5.29}
\end{align}
Al igualar estas expresiones entre sí y exigir que la igualdad se mantenga para $U^\mu$ arbitrario, vemos que
\begin{equation}
\lie_V \omega_\mu = V^\nu\p\nu \omega_\mu + (\p\mu V^\nu)
\omega_\nu\ ,\label{5.30}
\end{equation}
que (como la definición de conmutador) es completamente covariante, aunque no de manera manifiesta.

Mediante un procedimiento similar podemos definir la derivada de Lie de un campo tensorial arbitrario.
La respuesta se puede escribir.
\begin{align}
\lie_V T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\nu_2 \cdots \nu_l}  &=  V^\sigma\partial_\sigma T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}  \notag \\
& \quad -(\p\lambda V^{\mu_1}) T^{\lambda \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}
-(\p\lambda V^{\mu_2}) T^{\mu_1 \lambda \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l} -\cdots \notag \\
&\quad +(\p{\nu_1}V^\lambda)T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\lambda \nu_2 \cdots \nu_l}
+(\p{\nu_2}V^\lambda)T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\lambda \cdots \nu_l} + \cdots \,.\label{5.31}
\end{align}
Una vez más, esta expresión es covariante, a pesar de las apariencias.
Sin embargo, sin duda sería reconfortante tener una expresión equivalente que pareciera manifiestamente tensorial.
De hecho resulta que podemos escribir
\begin{align}
\lie_V T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\nu_2 \cdots \nu_l}  &=  V^\sigma\nabla_\sigma T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}  \notag \\
& \quad -(\nabla_\lambda V^{\mu_1}) T^{\lambda \mu_2 \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l}
-(\nabla_\lambda V^{\mu_2}) T^{\mu_1 \lambda \cdots
\mu_k}{}_{\nu_1 \nu_2 \cdots \nu_l} -\cdots \notag \\
&\quad +(\nabla_{\nu_1}V^\lambda)T^{\mu_1 \mu_2 \cdots
\mu_k}{}_{\lambda \nu_2 \cdots \nu_l}
+(\nabla_{\nu_2}V^\lambda)T^{\mu_1 \mu_2 \cdots \mu_k}{}_{\nu_1
\lambda \cdots \nu_l} + \cdots \ ,\label{5.32}
\end{align}
donde $\nabla_\mu$ representa {\it any} derivada covariante simétrica (sin torsión) (incluida, por supuesto, una derivada de una métrica).
Puedes comprobar que todos los términos que involucrarían coeficientes de conexión si expandiéramos (5.32) se cancelarían, dejando solo (5.31).
Ambas versiones de la fórmula para una derivada de Lie son útiles en diferentes momentos.
Una fórmula particularmente útil es la derivada de Lie de la métrica:
\begin{align}
\lie_V g_\mn  &=  V^\sigma\nabla_\sigma g_\mn
+(\nabla_{\mu}V^\lambda)g_{\lambda\nu} +(\nabla_{\nu}V^\lambda)
g_{\mu\lambda} \notag \\
&=  \nabla_\mu V_\nu + \nabla_\nu V_\mu \notag \\
&=  2\nabla_{(\mu} V_{\nu)}\ ,\label{5.33}
\end{align}
donde $\nabla_\mu$ es la derivada covariante derivada de $g_\mn$.

Pongamos algunas de estas ideas en el contexto de la Relatividad General.
A menudo se oirá proclamar que la Relatividad General es una teoría ``invariante de difeomorfismo''.
Lo que esto significa es que, si el universo está representado por una variedad $M$ con métrica $g_\mn$ y campos de materia $\psi$, y $\phi:M\rightarrow M$ es un difeomorfismo, entonces los conjuntos $(M,g_\mn,\psi)$ y $(M, \phi_*g_\mn,\phi_*\psi)$ representan lo mismo situación física.
Dado que los difeomorfismos son simplemente transformaciones de coordenadas activas, esta es una forma intelectual de decir que la teoría es invariante de coordenadas.
Aunque tal afirmación es cierta, es fuente de grandes malentendidos, por el simple hecho de que transmite muy poca información.
Cualquier teoría de la física semi-respetable es invariante de coordenadas, incluidas las basadas en la Relatividad Especial o la mecánica newtoniana; la Relatividad General no es el único en este sentido.
Cuando la gente dice que la Relatividad General es invariante del difeomorfismo, lo más probable es que tengan en mente uno de dos conceptos (estrechamente relacionados): la teoría está libre de ``geometría previa'' y no existe un sistema de coordenadas preferido para el Espacio-Tiempo.
El primero de ellos surge del hecho de que la métrica es una variable dinámica, y con ella el elemento de conexión y volumen, etc.
No se nos da nada por adelantado, a diferencia de la mecánica clásica o la Relatividad Especial.
Como consecuencia, no hay manera de simplificar la vida apegándose a un sistema de coordenadas específico adaptado a algunos elementos absolutos de la geometría.
Este estado de cosas nos obliga a tener mucho cuidado; es posible que dos configuraciones supuestamente distintas (de materia y métrica) en Relatividad General sean en realidad ``iguales'', relacionadas por un difeomorfismo.
En un enfoque integral de trayectoria de la gravedad cuántica, donde nos gustaría sumar todas las configuraciones posibles, se debe tener especial cuidado de no contar en exceso permitiendo que configuraciones físicamente indistinguibles contribuyan más de una vez.
Mientras tanto, en Relatividad Especial o mecánica newtoniana, la existencia de un conjunto preferido de coordenadas nos salva de tales ambigüedades.
El hecho de que la Relatividad General no tenga un sistema de coordenadas {\it preferido} a menudo se confunde con la afirmación de que es invariante de coordenadas (o ``generalmente covariante''); Ambas cosas son ciertas, pero una tiene más contenido que la otra.

Por otro lado, se puede aprovechar el hecho de la invariancia del difeomorfismo.
Recuerde que la acción completa de la gravedad acoplada a un conjunto de campos de materia $\psi^i$ viene dada por la suma de la acción de Hilbert para la Relatividad General más la acción de la materia,
\begin{equation}
S={1\over{8\pi G}}S_H[g_\mn]+S_M[g_\mn,\psi^i]\,.\label{5.34}
\end{equation}
La acción de Hilbert $S_H$ es invariante en difeomorfismo cuando se considera de forma aislada, por lo que la acción de materia $S_M$ también debe serlo para que la acción en su conjunto sea invariante.
Podemos escribir la variación en $S_M$ bajo un difeomorfismo como
\begin{equation}
\delta S_M=\int d^nx {{\delta S_M}\over{\delta g_\mn}}\delta g_\mn
+ \int d^nx {{\delta S_M}\over{\delta \psi^i}}\delta \psi^i\,.
\label{5.35}
\end{equation}
No estamos considerando variaciones arbitrarias de los campos, sólo aquellas que resultan de un difeomorfismo.
Sin embargo, las ecuaciones de movimiento de la materia nos dicen que la variación de $S_M$ con respecto a $\psi^i$ desaparecerá para cualquier variación (ya que la parte gravitacional de la acción no involucra los campos de materia).
Por tanto, para una teoría invariante del difeomorfismo, el primer término del lado derecho de (5.35) debe desaparecer.
Si el difeomorfismo es generado por un campo vectorial $V^\mu(x)$, el cambio infinitesimal en la métrica viene dado simplemente por su derivada de Lie a lo largo de $V^\mu$; por (5.33) tenemos
\begin{align}
\delta g_\mn  &=  \lie_V g_\mn \notag \\
&=  2\nabla_{(\mu} V_{\nu)}\,.\label{5.36}
\end{align}
Establecer $\delta S_M=0$ implica entonces
\begin{align}
0  &=  \int d^nx {{\delta S_M}\over{\delta g_\mn}}
\nabla_{\mu} V_{\nu}  \notag \\
&=  -\int d^nx\g V_\nu\nabla_\mu\left({1\over{\g}}{{\delta S_M}\over
{\delta g_\mn}}\right)\ ,\label{5.37}
\end{align}
donde podemos eliminar la simetrización de $\nabla_{(\mu} V_{\nu)}$ ya que $\delta S_M/\delta g_\mn$ ya es simétrico.
Exigiendo que (5.37) se cumpla para difeomorfismos generados por campos vectoriales arbitrarios $V^\mu$, y utilizando la definición (4.70) del tensor de energía-momento, obtenemos precisamente la ley de conservación del momento de energía,
\begin{equation}
\nabla_\mu T^\mn=0\,.\label{5.38}
\end{equation}
Por eso afirmamos anteriormente que la conservación de $T_\mn$ era más que una simple consecuencia del Principio de Equivalencia; es mucho más seguro que eso, ya que se basa únicamente en la invariancia del difeomorfismo de la teoría.

Hay un uso más al que le daremos la maquinaria que hemos configurado en esta sección: simetrías de tensores.
Decimos que un difeomorfismo $\phi$ es una {\bf simetría} de algún tensor $T$ si el tensor es invariante después de ser retirado por debajo de $\phi$:
\begin{equation}
\phi_*T = T\,.\label{5.39}
\end{equation}
Aunque las simetrías pueden ser discretas, es más común tener una familia de simetrías de un solo parámetro $\phi_t$.
Si la familia es generada por un campo vectorial $V^\mu(x)$, entonces (5.39) equivale a
\begin{equation}
\lie_V T=0\,.\label{5.40}
\end{equation}
Según (5.25), una implicación de una simetría es que, si $T$ es simétrico bajo alguna familia de difeomorfismos de un parámetro, siempre podemos encontrar un sistema de coordenadas en el que los componentes de $T$ sean todos independientes de uno de los coordenadas (la coordenada de curva integral del campo vectorial).
Lo contrario también es cierto; Si todos los componentes son independientes de una de las coordenadas, entonces el campo vectorial derivada parcial asociado con esa coordenada genera una simetría del tensor.

Las simetrías más importantes son las de la métrica, para la cual $\phi_*g_\mn = g_\mn$.
Un difeomorfismo de este tipo se llama {\bf isometría}.
Si un campo vectorial $V^\mu(x)$ genera una familia de isometrías de un parámetro, entonces $V^\mu$ se conoce como {\bf Killing vector field}.
La condición de que $V^\mu$ sea un vector de eliminación es, por tanto,
\begin{equation}
\lie_V g_\mn=0\ ,\label{5.41}
\end{equation}
o de (5.33),
\begin{equation}
\nabla_{(\mu}V_{\nu)}=0\,.\label{5.42}
\end{equation}
Esta última versión es la {\bf ecuación de Killing}.
Si un Espacio-Tiempo tiene un vector Killing, entonces sabemos que podemos encontrar un sistema de coordenadas en el que la métrica sea independiente de una de las coordenadas.

Con diferencia, el hecho más útil sobre los vectores Killing es que {\it Los vectores Killing implican cantidades conservadas asociadas con el movimiento de partículas libres}.
Si $x^\mu(\lambda)$ es una geodésica con un vector tangente $U^\mu=dx^\mu/d\lambda$ y $V^\mu$ es un vector Killing, entonces
\begin{align}
U^\nu\nabla_\nu(V_\mu U^\mu) &=  U^\nu U^\mu \nabla_\nu V_\mu
+V_\mu U^\nu \nabla_\nu U^\mu  \notag \\
&=  0\ ,\label{5.43}
\end{align}
donde el primer término desaparece de la ecuación de Killing y el segundo del hecho de que $x^\mu(\lambda)$ es una geodésica.
Por tanto, la cantidad $V_\mu U^\mu$ se conserva a lo largo de la línea mundial de la partícula.
Esto se puede entender físicamente: por definición, la métrica no cambia en la dirección del vector Killing.
Por lo tanto, en términos generales, una partícula libre no sentirá ninguna ``fuerza'' en esta dirección y, en consecuencia, la componente de su momento en esa dirección se conservará.

Hace tiempo nos referimos al concepto de espacio de máxima simetría, sin ofrecer una definición rigurosa.
La definición rigurosa es que un {\bf espacio máximamente simétrico} es aquel que posee el mayor número posible de vectores Killing, que en una variedad $n$-dimensional es $n(n+1)/2$.
No probaremos esta afirmación, pero es fácil de entender a un nivel informal.
Consideremos el espacio euclidiano $\R^n$, donde conocemos bien las isometrías: traslaciones y rotaciones.
En general habrá $n$ traducciones, una para cada dirección en la que podamos movernos.
También habrá rotaciones $n(n-1)/2$; para cada una de las dimensiones $n$ hay $n-1$ direcciones en las que podemos rotarla, pero debemos dividirla por dos para evitar el conteo excesivo (rotar $x$ en $y$ y rotar $y$ en $x$ son dos versiones de la misma cosa).
Por lo tanto tenemos
\begin{equation}
n+{{n(n-1)}\over 2}= {{n(n+1)}\over2} \label{5.44}
\end{equation}
Vectores de matanza independientes.
El mismo tipo de argumento de conteo se aplica a espacios máximamente simétricos con curvatura (como las esferas) o una firma no euclidiana (como el espacio de Minkowski), aunque los detalles son marginalmente diferentes.

Aunque puede que sea sencillo o no resolver la ecuación de Killing en cualquier Espacio-Tiempo dado, con frecuencia es posible escribir algunos vectores de Killing mediante inspección.
(Por supuesto, una métrica ``genérica'' no tiene ningún vector Killing, pero para simplificar las cosas a menudo tratamos con métricas con altos grados de simetría).
Por ejemplo, en $\R^2$ con la métrica $ds^2 =  d\,x^2+ d\,y^2$, la independencia de los componentes de la métrica con respecto a $x$ y $y$ produce inmediatamente dos vectores Killing:
\begin{align}
X^\mu &=  (1,0)\ , \notag \\
Y^\mu &=  (0,1)\,.\label{5.45}
\end{align}
Estos representan claramente las dos traducciones.
La rotación correspondería al vector $R=\partial/\partial\theta$ si estuviéramos en coordenadas polares; en coordenadas cartesianas esto se convierte en
\begin{equation}
R^\mu = (-y,x)\,.\label{5.46}
\end{equation}
Puedes comprobar por ti mismo que esto realmente resuelve la ecuación de Killing.

Tenga en cuenta que en las dimensiones $n\geq 2$, puede haber más vectores Killing que dimensiones.
Esto se debe a que un conjunto de campos vectoriales Killing puede ser linealmente independiente, aunque en cualquier punto de la variedad los vectores en ese punto sean linealmente dependientes.
Es trivial demostrar (así que deberías hacerlo tú mismo) que una combinación lineal de vectores Killing con coeficientes {\it constante} sigue siendo un vector Killing (en cuyo caso la combinación lineal no cuenta como un vector Killing independiente), pero esto no es necesariamente cierto con coeficientes que varían en la variedad.
También demostrará que el conmutador de dos campos vectoriales Killing es un campo vectorial Killing; Es muy útil saber esto, pero puede darse el caso de que el conmutador le proporcione un campo vectorial que no sea linealmente independiente (o que simplemente pueda desaparecer).
Por lo tanto, el problema de encontrar todos los vectores Killing de una métrica es algo complicado, ya que a veces no está claro cuándo dejar de buscar.





\chapter{Campos Débiles y Radiación Gravitacional}
%\addcontentsline{toc}{chapter}{Campos Débiles y Radiación Gravitacional}


Cuando derivamos por primera vez las ecuaciones de Einstein, comprobamos que estábamos en el camino correcto al considerar el límite newtoniano.
Esto equivalía a los requisitos de que el campo gravitacional fuera débil, estático (sin derivadas del tiempo) y que las partículas de prueba se movieran lentamente.
En esta sección consideraremos una situación menos restrictiva, en la que el campo aún es débil pero puede variar con el tiempo y no hay restricciones en el movimiento de las partículas de prueba.
Esto nos permitirá discutir fenómenos que están ausentes o son ambiguos en la teoría newtoniana, como la radiación gravitacional (donde el campo varía con el tiempo) y la desviación de la luz (que involucra partículas que se mueven rápidamente).

La debilidad del campo gravitacional se expresa una vez más como nuestra capacidad de descomponer la métrica en la métrica plana de Minkowski más una pequeña perturbación,
\begin{equation}
g_\mn = \eta_\mn + h_\mn\ ,\qquad |h_\mn |<<1\,.\label{6.1}
\end{equation}
Nos limitaremos a las coordenadas en las que $\eta_\mn$ toma su forma canónica, $\eta_\mn = {\rm diag}(-1,+1,+1,+1)$.
La suposición de que $h_\mn$ es pequeña nos permite ignorar cualquier cosa que sea mayor que el primer orden en esta cantidad, de lo cual obtenemos inmediatamente
\begin{equation}
g^\mn = \eta^\mn - h^\mn\ ,\label{6.2}
\end{equation}
donde $h^\mn = \eta^{\mu\rho}\eta^{\nu\sigma}h_{\rho\sigma}$.
Como antes, podemos subir y bajar índices usando $\eta^\mn$ y $\eta_\mn$, ya que las correcciones serían de orden superior en la perturbación.
De hecho, podemos pensar que la versión linealizada de la Relatividad General (donde se desprecian los efectos de orden superior al de primer orden en $h_\mn$) describe una teoría de un campo tensorial simétrico $h_\mn$ que se propaga en un Espacio-Tiempo de fondo plano.
Esta teoría es invariante de Lorentz en el sentido de la Relatividad Especial; bajo una transformación de Lorentz $x^{\mu'} = \Lambda^{\mu'}{}_\mu x^\mu$, la métrica plana $\eta_\mn$ es invariante, mientras que la perturbación se transforma como
\begin{equation}
h_{\mu'\nu'}=\Lambda_{\mu'}{}^\mu \Lambda_{\nu'}{}^\nu h_\mn\,.
\label{6.3}
\end{equation}
(Tenga en cuenta que podríamos haber considerado pequeñas perturbaciones sobre algún otro Espacio-Tiempo de fondo además del espacio de Minkowski.
En ese caso, la métrica se habría escrito $g_\mn = g_\mn^{(0)}+h_\mn$ y habríamos derivado una teoría de un tensor simétrico que se propaga en el espacio curvo con la métrica $g_\mn^{(0)}$.
Este enfoque es necesario, por ejemplo, en cosmología).

Queremos encontrar la ecuación de movimiento obedecida por las perturbaciones $h_\mn$, que surgen al examinar las ecuaciones de Einstein al primer orden.
Comenzamos con los símbolos de Christoffel, que están dados por
\begin{align}
\Gamma^\rho_{\mn} &=  {1\over 2} g^{\rho\lambda}
(\p\mu g_{\nu\lambda} + \p\nu g_{\lambda\mu} - \p\lambda g_{\mn}) \notag \\
&=  {1\over 2}\eta^{\rho\lambda}(\p\mu h_{\nu\lambda}
+ \p\nu h_{\lambda\mu} - \p\lambda h_{\mn})\,. \label{6.4}
\end{align}
Dado que los coeficientes de conexión son cantidades de primer orden, la única contribución al tensor de Riemann provendrá de las derivadas de los términos $\Gamma$, no de los términos $\Gamma^2$.
Reduciendo un índice por conveniencia, obtenemos
\begin{align}
R_{\mn\rho\sigma}  &=  \eta_{\mu\lambda}\p\rho
\Gamma^\lambda_{\nu\sigma} - \eta_{\mu\lambda}\p\sigma
\Gamma^\lambda_{\nu\rho}  \notag \\
&=  {1\over 2}(\p\rho\p\nu h_{\mu\sigma} + \p\sigma\p\mu h_{\nu\rho}
-\p\sigma\p\nu h_{\mu\rho}-\p\rho\p\mu h_{\nu\sigma})\,.
\label{6.5}
\end{align}
El tensor de Ricci surge de la contracción sobre $\mu$ y $\rho$, dando
\begin{equation}
R_\mn = {1\over 2}(\p\sigma\p\nu h^\sigma{}_\mu
+\p\sigma\p\mu h^\sigma{}_\nu - \p\mu\p\nu h - \Box h_\mn)\ ,
\label{6.6}
\end{equation}
que es manifiestamente simétrico en $\mu$ y $\nu$.
En esta expresión hemos definido la traza de la perturbación como $h=\eta^\mn h_\mn = h^\mu{}_\mu$, y la D'alembertiana es simplemente la del espacio plano, $\Box = -\p{t}^2+\p{x}^2+\p{y}^2+\p{z}^2$.
Contraer nuevamente para obtener los rendimientos escalares de Ricci
\begin{equation}
R = \p\mu\p\nu h^\mn - \Box h\,.\label{6.7}
\end{equation}
Sumando todo esto obtenemos el tensor de Einstein:
\begin{align}
G_\mn  &=  R_\mn - {1\over 2}\eta_\mn R \notag \\
&=  {1\over 2}(\p\sigma\p\nu h^\sigma{}_\mu
+\p\sigma\p\mu h^\sigma{}_\nu - \p\mu\p\nu h - \Box h_\mn
-\eta_\mn \p\mu\p\nu h^\mn + \eta_\mn \Box h)\,. \label{6.8}
\end{align}
De acuerdo con nuestra interpretación de la teoría linealizada como una que describe un tensor simétrico sobre un fondo plano, el tensor linealizado de Einstein (6.8) se puede derivar variando el siguiente lagrangiano con respecto a $h_\mn$:
\begin{equation}
{\cal L} = {1\over 2}\left[(\p\mu h^\mn)(\p\nu h) -
(\p\mu h^{\rho\sigma})(\p\rho h^\mu{}_\sigma) + {1\over 2}
\eta^\mn(\p\mu h^{\rho\sigma})(\p\nu h_{\rho\sigma})
-{1\over 2}\eta^\mn(\p\mu h)(\p\nu h)\right]\,.\label{6.9}
\end{equation}
Te ahorraré los detalles.

La ecuación de campo linealizada es, por supuesto, $G_\mn=8\pi GT_\mn$, donde $G_\mn$ viene dada por (6.8) y $T_\mn$ es el tensor de energía-momento, calculado en orden cero en $h_\mn$.
No incluimos correcciones de orden superior al tensor de energía-momento porque la cantidad de energía y momento debe ser pequeña para que se aplique el límite del campo débil.
En otras palabras, el orden más bajo que no desaparece en $T_\mn$ es automáticamente del mismo orden de magnitud que la perturbación.
Observe que la ley de conservación al orden más bajo es simplemente $\p\mu T^\mn=0$.
La mayoría de las veces nos ocuparemos de las ecuaciones del vacío, que, como siempre, son simplemente $R_\mn=0$, donde $R_\mn$ está dada por (6.6).

Con las ecuaciones de campo linealizadas en la mano, estamos casi preparados para empezar a resolverlas.
Sin embargo, primero debemos abordar la espinosa cuestión de la invariancia del calibre.
Este problema surge debido a la demanda de que $g_\mn=\eta_\mn+h_\mn$ no especifique completamente el sistema de coordenadas en el Espacio-Tiempo; Puede haber otros sistemas de coordenadas en los que la métrica aún pueda escribirse como la métrica de Minkowski más una pequeña perturbación, pero la perturbación será diferente.
Por tanto, la descomposición de la métrica en un fondo plano más una perturbación no es única.

Podemos pensar en esto desde un punto de vista intelectual.
La noción de que se puede considerar la teoría linealizada como una que gobierna el comportamiento de campos tensoriales sobre un fondo plano puede formalizarse en términos de un ``espaciotiempo de fondo'' $M_b$, un ``espaciotiempo físico'' $M_p$, y un difeomorfismo $\phi:M_b\rightarrow M_p$.
Como las variedades $M_b$ y $M_p$ son ``iguales'' (ya que son difeomorfas), pero imaginamos que poseen algunos campos tensoriales diferentes; en $M_b$ hemos definido la métrica plana de Minkowski $\eta_\mn$, mientras que en $M_p$ tenemos alguna métrica $g_{\alpha\beta}$ que obedece a las ecuaciones de Einstein.
(Imaginamos que $M_b$ está equipado con las coordenadas $x^\mu$ y $M_p$ está equipado con las coordenadas $y^\alpha$, aunque estas no desempeñarán un papel destacado).
El difeomorfismo $\phi$ nos permite mover tensores hacia adelante y hacia atrás entre el fondo y los espacios-tiempos físicos.
Dado que nos gustaría construir nuestra teoría linealizada como una que tiene lugar en el Espacio-Tiempo de fondo plano, estamos interesados en el retroceso $(\phi_*g)_\mn$ de la métrica física.
Podemos definir la perturbación como la diferencia entre la métrica física retraída y la plana:
\begin{equation}
h_\mn = (\phi_*g)_\mn - \eta_\mn\,.\label{6.10}
\end{equation}
Según esta definición, no hay razón para que los componentes de $h_\mn$ sean pequeños; sin embargo, si los campos gravitacionales en $M_p$ son débiles, entonces para {\it some} difeomorfismos $\phi$ tendremos $|h_\mn| << 1$.
Por lo tanto, limitamos nuestra atención sólo a aquellos difeomorfismos para los cuales esto es cierto.
Entonces, el hecho de que $g_{\alpha\beta}$ obedezca las ecuaciones de Einstein en el Espacio-Tiempo físico significa que $h_\mn$ obedecerá a las ecuaciones linealizadas en el Espacio-Tiempo de fondo (ya que $\phi$, como difeomorfismo, se puede utilizar para retirar las ecuaciones de Einstein).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/six01.pdf}
\end{figure}

En este lenguaje, la cuestión de la invariancia de calibre es simplemente el hecho de que hay una gran cantidad de difeomorfismos permitidos entre $M_b$ y $M_p$ (donde ``permisible'' significa que la perturbación es pequeña).
Considere un campo vectorial $\xi^\mu(x)$ en el Espacio-Tiempo de fondo.
Este campo vectorial genera una familia de difeomorfismos de un parámetro $\psi_\epsilon:M_b\rightarrow M_b$.
Para $\epsilon$ suficientemente pequeño, si $\phi$ es un difeomorfismo para el cual la perturbación definida por (6.10) es pequeña, también lo será $(\phi\circ\psi_\epsilon)$, aunque la perturbación tendrá un valor diferente.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/six02.pdf}
\end{figure}

\noindent
Específicamente, podemos definir una familia de perturbaciones parametrizadas por $\epsilon$:
\begin{align}
h_\mn^{(\epsilon)}  &=  [(\phi\circ\psi_\epsilon)_*g]_\mn
- \eta_\mn \notag \\
&=  [\psi_{\epsilon *}(\phi_*g)]_\mn - \eta_\mn\,. \label{6.11}
\end{align}
La segunda igualdad se basa en el hecho de que el retroceso bajo una composición está dado por la composición de los retrocesos en el orden opuesto, lo que se deriva del hecho de que el retroceso mismo mueve las cosas en la dirección opuesta al mapa original.
Sustituyendo la relación (6.10), encontramos
\begin{align}
h_\mn^{(\epsilon)}  &=  \psi_{\epsilon *}(h +\eta)_\mn
-\eta_\mn  \notag \\
&=  \psi_{\epsilon *}(h_\mn) +\psi_{\epsilon *}(\eta_\mn)-\eta_\mn
\label{6.12}
\end{align}
(ya que el retroceso de la suma de dos tensores es la suma de los retrocesos).
Ahora utilizamos nuestra suposición de que $\epsilon$ es pequeño; en este caso $\psi_{\epsilon *}(h_\mn)$ será igual a $h_\mn$ al orden más bajo, mientras que los otros dos términos nos dan una derivada de Lie:
\begin{align}
h_\mn^{(\epsilon)}  &=  \psi_{\epsilon *}(h_\mn)
+\epsilon\left[{{\psi_{\epsilon *}(\eta_\mn)-\eta_\mn}\over
\epsilon}\right]  \notag \\
&=  h_\mn + \epsilon \lie_\xi\eta_\mn  \notag \\
&=  h_\mn + 2\epsilon\partial_{(\mu}\xi_{\nu)}\,. \label{6.13}
\end{align}
La última igualdad se desprende de nuestro cálculo previo de la derivada de Lie de la métrica, (5.33), más el hecho de que las derivadas covariantes son simplemente derivadas parciales de orden más bajo.

Los difeomorfismos infinitesimales $\phi_\epsilon$ proporcionan una representación diferente de la misma situación física, manteniendo nuestro requisito de que la perturbación sea pequeña.
Por lo tanto, el resultado (6.12) nos dice qué tipo de perturbaciones métricas denotan espacios-tiempos físicamente equivalentes: aquellos relacionados entre sí por $2\epsilon\partial_{(\mu}\xi_{\nu)}$, para algún vector $\xi^\mu$.
La invariancia de nuestra teoría bajo tales transformaciones es análoga a la invariancia de calibre tradicional del electromagnetismo bajo $A_\mu \rightarrow A_\mu + \p\mu\lambda$.
(La analogía es diferente de la analogía anterior que trazamos con el electromagnetismo, relacionando las transformaciones locales de Lorentz en el formalismo del marco ortonormal con cambios de base en un paquete de vectores interno).
En el electromagnetismo, la invariancia se produce porque la intensidad del campo $F_\mn = \p\mu A_\nu - \p\nu A_\mu$ no se modifica mediante transformaciones de calibre; de manera similar, encontramos que la transformación (6.13) cambia el tensor de Riemann linealizado en
\begin{align}
\delta R_{\mn\rho\sigma}  &=
{1\over 2}(\p\rho\p\nu\p\mu\xi_\sigma +\p\rho\p\nu\p\sigma\xi_\mu
+ \p\sigma\p\mu\p\nu\xi_\rho + \p\sigma\p\mu\p\rho\xi_\nu  \notag \\
& \qquad - \p\sigma\p\nu\p\mu\xi_\rho - \p\sigma\p\nu\p\rho\xi_\mu
- \p\rho\p\mu\p\nu\xi_\sigma - \p\rho\p\mu\p\sigma\xi_\nu ) \notag \\
&= 0\,. \label{6.14}
\end{align}
Nuestra derivación abstracta de la transformación de calibre apropiada para la perturbación métrica se verifica por el hecho de que deja la curvatura (y por tanto el Espacio-Tiempo físico) sin cambios.

La invariancia de calibre también se puede entender a partir de la ruta un poco más sencilla pero considerablemente más directa de las transformaciones de coordenadas infinitesimales.
Se puede considerar que nuestro difeomorfismo $\psi_\epsilon$ cambia las coordenadas de $x^\mu$ a $x^\mu -\epsilon\xi^\mu$.
(El signo menos, poco convencional, procede del hecho de que la ``nueva'' métrica se retira desde una pequeña distancia hacia delante a lo largo de las curvas integrales, lo que equivale a sustituir las coordenadas por las de una pequeña distancia hacia atrás a lo largo de las curvas).
Siguiendo las reglas habituales para transformar tensores mediante transformaciones de coordenadas, se puede derivar con precisión (6.13), aunque hay que hacer un poco de trampa al igualar componentes de tensores en dos sistemas de coordenadas diferentes.
Véase Schutz o Weinberg como ejemplo.

Cuando nos enfrentamos a un sistema que es invariante ante algún tipo de transformación de calibre, nuestro primer instinto es fijar un calibre.
Ya hemos analizado el sistema de coordenadas armónicas y volveremos a él ahora en el contexto del límite de campo débil.
Recuerde que este indicador fue especificado por $\Box x^\mu=0$, que mostramos era equivalente a
\begin{equation}
g^\mn \Gamma^\rho_\mn =0\,.\label{6.15}
\end{equation}
En el límite de campo débil esto se convierte en
\begin{equation}
{1\over 2}\eta^\mn \eta^{\lambda\rho}(\p\mu h_{\nu\lambda}
+\p\nu h_{\lambda\mu} -\p\lambda h_\mn)=0\ ,\label{6.16}
\end{equation}
o
\begin{equation}
\p\mu h^\mu{}_\lambda - {1\over 2}\p\lambda h = 0\,.\label{6.17}
\end{equation}
Esta condición también se conoce como calibre de Lorentz (o calibre de Einstein o calibre de Hilbert o calibre de Donder o calibre de Fock).
Como antes, todavía nos queda algo de libertad de calibre, ya que podemos cambiar nuestras coordenadas mediante funciones armónicas (infinitesimales).

En este indicador, las ecuaciones linealizadas de Einstein $G_\mn = 8\pi GT_\mn$ se simplifican un poco, para
\begin{equation}
\Box h_\mn - {1\over 2}\eta_\mn \Box h=-16\pi GT_\mn\ ,\label{6.18}
\end{equation}
mientras que las ecuaciones de vacío $R_\mn=0$ adoptan la forma elegante
\begin{equation}
\Box h_\mn=0\ ,\label{6.19}
\end{equation}
que es simplemente la ecuación de onda relativista convencional.
Juntas, (6.19) y (6.17) determinan la evolución de una perturbación en el campo gravitacional en el vacío en el calibre armónico.

A menudo resulta conveniente trabajar con una descripción ligeramente diferente de la perturbación métrica.
Definimos la perturbación ``traza invertida'' $\bar h_\mn$ por
\begin{equation}
\bar h_\mn= h_\mn - {1\over 2}\eta_\mn h\,.\label{6.20}
\end{equation}
El nombre tiene sentido, ya que $\bar h^\mu{}_\mu=-h^\mu{}_\mu$.
(El tensor de Einstein es simplemente el tensor de Ricci de traza invertida).
En términos de $\bar h_\mn$, la condición del indicador de armónicos se vuelve
\begin{equation}
\p\mu \bar h^\mu{}_\lambda =0\,.\label{6.21}
\end{equation}
Las ecuaciones de campo completo son
\begin{equation}
\Box \bar h_\mn = -16\pi G T_\mn\ ,\label{6.22}
\end{equation}
de lo cual se deduce inmediatamente que las ecuaciones de vacío son
\begin{equation}
\Box \bar h_\mn = 0\,.\label{6.23}
\end{equation}

A partir de (6.22) y de nuestra exploración previa del límite newtoniano, es sencillo derivar la métrica del campo débil para una fuente esférica estacionaria como un planeta o una estrella.
Recuerde que anteriormente encontramos que las ecuaciones de Einstein predijeron que $h_{00}$ obedecía la ecuación de Poisson (4.51) en el límite del campo débil, lo que implicaba
\begin{equation}
h_{00} = -2\Phi\ ,\label{6.24}
\end{equation}
donde $\Phi$ es el potencial newtoniano convencional, $\Phi=-GM/r$.
Supongamos ahora que el tensor de energía-momento de nuestra fuente está dominado por su densidad de energía en reposo $\rho=T_{00}$.
(Esta suposición generalmente no es necesaria en el límite del campo débil, pero ciertamente será válida para un planeta o una estrella, que es lo que nos gustaría considerar por el momento).
Entonces los otros componentes de $T_\mn$ serán mucho más pequeños que $T_{00}$, y desde (6.22) lo mismo debe ser válido para $\bar h_\mn$.
Si $\bar h_{00}$ es mucho mayor que $\bar h_{ij}$, tendremos
\begin{equation}
h = -\bar h=-\eta^\mn \bar h_\mn = \bar h_{00}\ ,\label{6.25}
\end{equation}
y luego de (6.20) obtenemos inmediatamente
\begin{equation}
\bar h_{00} = 2 h_{00} =-4\Phi\,.\label{6.26}
\end{equation}
Los otros componentes de $\bar h_{\mn}$ son insignificantes, de lo que podemos derivar
\begin{equation}
h_{i0} = \bar h_{i0} - {1\over 2}\eta_{i0}\bar h = 0\ ,\label{6.27}
\end{equation}
y
\begin{equation}
h_{ij}= \bar h_{ij} - {1\over 2}\eta_{ij}\bar h = -2\Phi\delta_{ij}\,.
\label{6.28}
\end{equation}
Por lo tanto, la métrica para una estrella o planeta en el límite del campo débil es
\begin{equation}
ds^2 = -(1+2\Phi) d\,t^2 +(1-2\Phi)( d\,x^2 + d\,y^2 + d\,z^2)\,.
\label{6.29}
\end{equation}

Una aplicación algo menos simplista del límite del campo débil es la radiación gravitacional.
Aquellos que estén familiarizados con el problema análogo del electromagnetismo notarán que el procedimiento es casi exactamente el mismo.
Comenzamos considerando las ecuaciones linealizadas en el vacío (6.23).
Dado que el espacio plano D'alembertiano tiene la forma $\Box = -\p{t}^2 +\nabla^2$, la ecuación de campo tiene la forma de una ecuación de onda para $\bh_\mn$.
Como todos los buenos físicos saben, lo que hay que hacer cuando nos enfrentamos a una ecuación de este tipo es escribir soluciones con valores complejos y luego tomar la parte real al final del día.
Entonces reconocemos que un conjunto particularmente útil de soluciones a esta ecuación de onda son las ondas planas, dadas por
\begin{equation}
\bh_\mn = C_\mn e^{ik_\sigma x^\sigma}\ ,\label{6.30}
\end{equation}
donde $C_\mn$ es un tensor $(0,2)$ constante y simétrico, y $k^\sigma$ es un vector constante conocido como {\bf vector de onda}.
Para comprobar que es una solución enchufamos:
\begin{align}
0 &=  \Box \bh_\mn \notag \\
&=  \eta^{\rho\sigma}\p\rho\p\sigma
\bh_\mn \notag \\
&=  \eta^{\rho\sigma}\p\rho (i k_\sigma\bh_\mn) \notag \\
&=  - \eta^{\rho\sigma}k_\rho k_\sigma\bh_\mn \notag \\
&=  -k_\sigma k^\sigma \bh_\mn\,. \label{6.31}
\end{align}
Dado que (para una solución interesante) no todos los componentes de $h_\mn$ serán cero en todas partes, debemos tener
\begin{equation}
k_\sigma k^\sigma=0\,.\label{6.32}
\end{equation}
La onda plana (6.30) es, por tanto, una solución de las ecuaciones linealizadas si el vector de onda es nulo; esto se traduce libremente en la afirmación de que las ondas gravitacionales se propagan a la velocidad de la luz.
La componente temporal del vector de onda a menudo se denomina {\bf frecuencia} de la onda, y escribimos $k^\sigma = (\omega, k^1,k^2,k^3)$.
(De manera más general, un observador que se mueve con cuatro velocidades $U^\mu$ observaría que la onda tiene una frecuencia $\omega=-k_\mu U^\mu$.)
Entonces la condición de que el vector de onda sea nulo se convierte en
\begin{equation}
\omega^2 = \delta_{ij}k^i k^j\,.\label{6.33}
\end{equation}
Por supuesto, nuestra ola está lejos de ser la solución más general; cualquier número (posiblemente infinito) de ondas planas distintas se puede sumar y aun así resolverá la ecuación lineal (6.23).
De hecho, cualquier solución puede escribirse como tal superposición.

Hay una serie de parámetros libres para especificar la onda: diez números para los coeficientes $C_\mn$ y tres para el vector nulo $k^\sigma$.
Gran parte de ellos son el resultado de la libertad de coordenadas y de medición, que ahora nos propusimos eliminar.
Comenzamos imponiendo la condición de calibre armónico (6.21).
Esto implica que
\begin{align}
0  &=  \p\mu\bh^\mn  \notag \\   &=  \p\mu(C^\mn e^{ik_\sigma x^\sigma}) \notag \\
&=  iC^\mn k_\mu e^{ik_\sigma x^\sigma}\ , \label{6.34}
\end{align}
lo cual solo es cierto si
\begin{equation}
k_\mu C^\mn=0\,.\label{6.35}
\end{equation}
Decimos que el vector de onda es ortogonal a $C^\mn$.
Estas son cuatro ecuaciones que reducen el número de componentes independientes de $C_\mn$ de diez a seis.

Aunque ahora hemos impuesto la condición de calibre armónico, todavía queda cierta libertad de coordenadas.
Recuerde que cualquier transformación de coordenadas de la forma
\begin{equation}
x^\mu \rightarrow x^\mu + \zeta^\mu\label{6.36}
\end{equation}
abandonará la condición de coordenadas armónicas
\begin{equation}
\Box x^\mu=0 \label{6.37}
\end{equation}
satisfecho mientras tengamos
\begin{equation}
\Box \zeta^\mu=0\,.\label{6.38}
\end{equation}
Por supuesto, (6.38) es en sí misma una ecuación de onda para $\zeta^\mu$; Una vez que elijamos una solución, habremos agotado toda nuestra libertad de calibre.
Elijamos la siguiente solución:
\begin{equation}
\zeta_\mu = B_\mu e^{ik_\sigma x^\sigma}\ ,\label{6.39}
\end{equation}
donde $k_\sigma$ es el vector de onda de nuestra onda gravitacional y $B_\mu$ son coeficientes constantes.

Ahora afirmamos que esta libertad restante nos permite convertir cualquier coeficiente $C^{\rm (old)}_\mn$ que caracterice nuestra onda gravitacional a un nuevo conjunto $C^{\rm (new)}_\mn$, tal que
\begin{equation}
C^{{\rm (new)}\mu}{}_\mu = 0\label{6.40}
\end{equation}
y
\begin{equation}
C^{\rm (new)}_{0\nu}=0\,.\label{6.41}
\end{equation}
(En realidad, esta última condición es a la vez una elección de calibre y una elección del cuadro de Lorentz.
La elección del indicador establece $U^\mu C^{\rm (new)}_{\mu\nu}=0$ para algún vector temporal constante $U^\mu$, mientras que la elección del fotograma hace que $U^\mu$ apunte a lo largo del eje del tiempo).
Veamos cómo esto es posible resolviendo explícitamente los coeficientes necesarios $B_\mu$.
Bajo la transformación (6.36), el cambio resultante en nuestra perturbación métrica se puede escribir
\begin{equation}
h^{\rm (new)}_\mn = h^{\rm (old)}_\mn -\p\mu \zeta_\nu
-\p\nu \zeta_\mu\ ,\label{6.42}
\end{equation}
lo que induce un cambio en la perturbación de traza invertida,
\begin{align}
\bh^{\rm (new)}_\mn  &=  h^{\rm (new)}_\mn -{1\over 2}
\eta_\mn h^{\rm (new)} \notag \\  &= h^{\rm (old)}_\mn -\p\mu \zeta_\nu
-\p\nu \zeta_\mu -{1\over 2}\eta_\mn(h^{\rm (old)}
-2\p\lambda \zeta^\lambda) \notag \\
&=  \bh^{\rm (old)}_\mn -\p\mu \zeta_\nu -\p\nu \zeta_\mu
+\eta_\mn\p\lambda \zeta^\lambda\,. \label{6.43}
\end{align}
Usando las formas específicas (6.30) para la solución y (6.39) para la transformación, obtenemos
\begin{equation}
C^{\rm (new)}_\mn =C^{\rm (old)}_\mn - ik_\mu B_\nu -i k_\nu B_\mu
+i\eta_\mn k_\lambda B^\lambda\,.\label{6.44}
\end{equation}
Por lo tanto, imponer (6.40) significa
\begin{equation}
0= C^{{\rm (old)}\mu}{}_\mu +2ik_\lambda B^\lambda\ ,\label{6.45}
\end{equation}
o
\begin{equation}
k_\lambda B^\lambda = {{i}\over 2}C^{{\rm (old)}\mu}{}_\mu\,.
\label{6.46}
\end{equation}
Entonces podemos imponer (6.41), primero para $\nu=0$:
\begin{align}
0  &=  C^{\rm (old)}_{00}-2ik_0B_0-ik_\lambda B^\lambda \notag \\
&=  C^{\rm (old)}_{00}-2ik_0B_0 +{1\over 2}C^{{\rm (old)}\mu}{}_\mu
\ , \label{6.47}
\end{align}
o
\begin{equation}
B_0=-{{i}\over {2k_0}}\left(C^{\rm (old)}_{00}+{1\over 2}
C^{{\rm (old)}\mu}{}_\mu\right)\,.\label{6.48}
\end{equation}
Luego impone (6.41) para $\nu=j$:
\begin{align}
0  &=  C^{\rm (old)}_{0j}-ik_0 B_j -ik_jB_0 \notag \\
&=  C^{\rm (old)}_{0j}-ik_0B_j -ik_j\left[
{{-i}\over {2k_0}}\left(C^{\rm (old)}_{00}+{1\over 2}
C^{{\rm (old)}\mu}{}_\mu\right)\right]\ , \label{6.49}
\end{align}
o
\begin{equation}
B_j={{i}\over{2(k_0)^2}}\left[-2k_0C^{\rm (old)}_{0j}
+k_j\left(C^{\rm (old)}_{00}+{1\over 2}
C^{{\rm (old)}\mu}{}_\mu\right)\right]\,.\label{6.50}
\end{equation}
Para comprobar que estas opciones son mutuamente consistentes, debemos reemplazar (6.48) y (6.50) nuevamente en (6.40), lo cual dejaré a usted.
Supongamos que hemos realizado esta transformación y nos referimos a los nuevos componentes $C_\mn^{\rm (new)}$ simplemente como $C_\mn$.

Así, comenzamos con los diez números independientes en la matriz simétrica $C_\mn$.
La elección del calibre armónico implicó las cuatro condiciones (6,35), lo que redujo el número de componentes independientes a seis.
El uso de nuestra libertad de calibre restante nos llevó a una condición (6.40) y a las cuatro condiciones (6.41); pero cuando $\nu=0$ (6.41) implica (6.35), tenemos un total de cuatro restricciones adicionales, lo que nos lleva a dos componentes independientes.
Hemos agotado toda nuestra libertad posible, por lo que estos dos números representan la información física que caracteriza nuestra onda plana en este medidor.
Esto se puede ver más explícitamente eligiendo nuestras coordenadas espaciales de modo que la onda viaje en la dirección $x^3$; eso es,
\begin{equation}
k^\mu = (\omega,0,0,k^3) = (\omega,0,0,\omega)\ ,\label{6.51}
\end{equation}
donde sabemos que $k^3=\omega$ porque el vector de onda es nulo.
En este caso, $k^\mu C_\mn=0$ y $C_{0\nu}=0$ juntos implican
\begin{equation}
C_{3\nu}=0\,.\label{6.52}
\end{equation}
Por lo tanto, los únicos componentes distintos de cero de $C_\mn$ son $C_{11}$, $C_{12}$, $C_{21}$ y $C_{22}$.
Pero $C_\mn$ no tiene rastro y es simétrico, por lo que en general podemos escribir
\begin{equation}
C_\mn = \left(\mqty{0&0&0&0 \\  0&C_{11}&C_{12}&0 \\
0&C_{12}&-C_{11}&0 \\  0&0&0&0 \\ }\right)\,.\label{6.53}
\end{equation}
Por lo tanto, para una onda plana en este ancho de banda que viaja en la dirección $x^3$, los dos componentes $C_{11}$ y $C_{12}$ (junto con la frecuencia $\omega$) caracterizan completamente la onda.

Al utilizar toda nuestra libertad de calibre, hemos pasado a un submedidor del calibre armónico conocido como {\bf calibre transversal sin trazas} (o, a veces, ``medidor de radiación'').
El nombre proviene del hecho de que la perturbación métrica no tiene trazas y es perpendicular al vector de onda.
Por supuesto, hemos estado trabajando con la perturbación de traza invertida $\bh_\mn$ en lugar de la perturbación $h_\mn$ en sí; pero como $\bh_\mn$ no tiene rastro (porque $C_\mn$ sí lo es) y es igual al trazo inverso de $h_\mn$, en este indicador tenemos
\begin{equation}
\bh_\mn^{\rm TT} = h_\mn^{\rm TT}\qquad
{\rm (transverse~traceless~gauge)}\,.\label{6.54}
\end{equation}
Por lo tanto podemos dejar caer las barras sobre $h_\mn$, siempre y cuando estemos en este indicador.

Una característica interesante del medidor transversal sin rastro es que si se le dan los componentes de una onda plana en algún medidor arbitrario, puede convertirlos fácilmente en componentes transversales sin rastro.
Primero definimos un tensor $P_\mn$ que actúa como operador de proyección:
\begin{equation}
P_\mn = \eta_\mn - n_\mu n_\nu\,.\label{6.55}
\end{equation}
Puede comprobar que esto proyecta vectores en hiperplanos ortogonales al vector unitario $n_\mu$.
Aquí tomamos $n_\mu$ como un vector unitario {\it espacial}, que elegimos para que se encuentre a lo largo de la dirección de propagación de la onda:
\begin{equation}
n_0=0\ ,\qquad n_j = k_j/\omega\,.\label{6.56}
\end{equation}
Entonces la parte transversal de alguna perturbación $h_\mn$ es simplemente la proyección $P_\mu{}^\rho P_\nu{}^\sigma h_{\rho\sigma}$, y la parte transversal sin traza se obtiene restando la traza:
\begin{equation}
h^{\rm TT}_\mn = P_\mu{}^\rho P_\nu{}^\sigma h_{\rho\sigma}
-{1\over 2}P_\mn P^{\rho\sigma} h_{\rho\sigma}\,.\label{6.57}
\end{equation}
Para obtener detalles apropiados para casos más generales, consulte la discusión en Misner, Thorne y Wheeler.

Para tener una idea de los efectos físicos debidos a las ondas gravitacionales, es útil considerar el movimiento de las partículas de prueba en presencia de una onda.
Ciertamente es insuficiente resolver la trayectoria de una sola partícula, ya que eso sólo nos informaría sobre los valores de las coordenadas a lo largo de la línea universal.
(De hecho, para cualquier partícula individual podemos encontrar coordenadas transversales sin traza en las que la partícula parece estacionaria de primer orden en $h_\mn$.)
Para obtener una medida independiente de las coordenadas de los efectos de la onda, consideramos el movimiento relativo de las partículas cercanas, como lo describe la ecuación de desviación geodésica.
Si consideramos algunas partículas cercanas con cuatro velocidades descritas por un único campo vectorial $U^\mu(x)$ y un vector de separación $S^\mu$, tenemos
\begin{equation}
{{D^2}\over{d\tau^2}}S^\mu = R^\mu{}_{\nu\rho\sigma}
U^\nu U^\rho S^\sigma\,.\label{6.58}
\end{equation}
Nos gustaría calcular el lado izquierdo de primer orden en $h_\mn$.
Si tomamos que nuestras partículas de prueba se mueven lentamente, entonces podemos expresar la cuádruple velocidad como un vector unitario en la dirección del tiempo más correcciones de orden $h_\mn$ y superiores; pero sabemos que el tensor de Riemann ya es de primer orden, así que las correcciones a $U^\nu$ pueden ignorarse, y escribimos
\begin{equation}
U^\nu = (1,0,0,0)\,.\label{6.59}
\end{equation}
Por lo tanto, sólo necesitamos calcular $R^\mu{}_{00\sigma}$ o, equivalentemente, $R_{\mu 00\sigma}$.
De (6.5) tenemos
\begin{equation}
R_{\mu 00\sigma}={1\over 2}(\p0\p0 h_{\mu\sigma} + \p\sigma
\p\mu h_{00} - \p\sigma\p0 h_{\mu 0} - \p\mu\p0 h_{\sigma 0})
\,.\label{6.60}
\end{equation}
Pero $h_{\mu 0}=0$, entonces
\begin{equation}
R_{\mu 00\sigma}={1\over 2}\p0\p0 h_{\mu\sigma}\,.\label{6.61}
\end{equation}
Mientras tanto, para nuestras partículas que se mueven lentamente tenemos $\tau=x^0=t$ al orden más bajo, por lo que la ecuación de desviación geodésica se convierte en
\begin{equation}
{{\partial^2}\over{\partial t^2}}S^\mu = {1\over 2}S^\sigma
{{\partial^2}\over{\partial t^2}} h^\mu{}_\sigma\,.\label{6.62}
\end{equation}
Para nuestra onda que viaja en la dirección $x^3$, esto implica que solo $S^1$ y $S^2$ se verán afectados; las partículas de prueba solo se perturban en direcciones perpendiculares al vector de onda.
Por supuesto, esto es familiar en el electromagnetismo, donde los campos eléctrico y magnético en una onda plana son perpendiculares al vector de onda.

Nuestra ola se caracteriza por los dos números, que para mayor comodidad cambiaremos el nombre a $C_+ = C_{11}$ y $C_\times = C_{12}$.
Consideremos sus efectos por separado, comenzando con el caso $C_\times=0$.
Entonces nosotros tenemos
\begin{equation}
{{\partial^2}\over{\partial t^2}}S^1 = {1\over 2} S^1
{{\partial^2}\over{\partial t^2}}
(C_+ e^{ik_\sigma x^\sigma})\label{6.63}
\end{equation}
y
\begin{equation}
{{\partial^2}\over{\partial t^2}}S^2 = -{1\over 2} S^2
{{\partial^2}\over{\partial t^2}}
(C_+ e^{ik_\sigma x^\sigma})\,.\label{6.64}
\end{equation}
Estos pueden resolverse inmediatamente para producir, al orden más bajo,
\begin{equation}
S^1 = \left(1+{1\over 2}C_+ e^{ik_\sigma x^\sigma}
\right)S^1(0)\label{6.65}
\end{equation}
y
\begin{equation}
S^2 = \left(1-{1\over 2}C_+ e^{ik_\sigma x^\sigma}
\right)S^2(0)\,.\label{6.66}
\end{equation}
Así, las partículas inicialmente separadas en la dirección $x^1$ oscilarán hacia adelante y hacia atrás en la dirección $x^1$, y lo mismo para aquellas con una separación inicial $x^2$.
Es decir, si comenzamos con un anillo de partículas estacionarias en el plano $x$-$y$, a medida que pasa la onda rebotarán hacia adelante y hacia atrás en forma de ``$+$'':

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{imagenes/six03.pdf}
\end{figure}

\noindent
Por otro lado, el análisis equivalente para el caso en el que $C_+=0$ pero $C_\times\neq 0$ arrojaría la solución
\begin{equation}
S^1 = S^1(0)+{1\over 2}C_\times e^{ik_\sigma x^\sigma}
S^2(0)\label{6.67}
\end{equation}
y
\begin{equation}
S^2 = S^2(0)+{1\over 2}C_\times e^{ik_\sigma x^\sigma}
S^1(0)\,.\label{6.68}
\end{equation}
En este caso, el círculo de partículas rebotaría hacia adelante y hacia atrás en forma de ``$\times$'':

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{imagenes/six04.pdf}
\end{figure}

\noindent
Por tanto, la notación $C_+$ y $C_\times$ debe quedar clara.
Estas dos cantidades miden los dos modos independientes de polarización lineal de la onda gravitacional.
Si quisiéramos, podríamos considerar modos polarizados circularmente hacia la derecha y hacia la izquierda definiendo
\begin{align}
C_R  &=  {1\over {\sqrt2}}(C_+ +i C_\times)\ , \notag \\
C_L  &=  {1\over {\sqrt2}}(C_+ -i C_\times)\,. \label{6.69}
\end{align}
El efecto de una onda $C_R$ pura sería rotar las partículas en sentido diestro,

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{imagenes/six05.pdf}
\end{figure}

\noindent
y lo mismo para el modo zurdo $C_L$.
(Obsérvese que las partículas individuales no viajan alrededor del anillo; simplemente se mueven en pequeños epiciclos).

Podemos relacionar los estados de polarización de las ondas gravitacionales clásicas con los tipos de partículas que esperaríamos encontrar tras la cuantificación.
El campo electromagnético tiene dos estados de polarización independientes que se describen mediante vectores en el plano $x$-$y$; de manera equivalente, un modo de polarización único es invariante bajo una rotación de $360^\circ$ en este plano.
Tras la cuantificación, esta teoría produce el fotón, una partícula de espín uno sin masa.
El neutrino, por el contrario, es también una partícula sin masa, descrita por un campo que adquiere un signo menos al rotar $360^\circ$; es invariante bajo rotaciones de $720^\circ$ y decimos que tiene espín-${1\over 2}$.
La regla general es que el giro $S$ está relacionado con el ángulo $\theta$ bajo el cual los modos de polarización son invariantes por $S=360^\circ/\theta$.
El campo gravitacional, cuyas ondas se propagan a la velocidad de la luz, debería dar lugar a partículas sin masa en la teoría cuántica.
Al notar que los modos de polarización que hemos descrito son invariantes bajo rotaciones de $180^\circ$ en el plano $x$-$y$, esperamos que las partículas asociadas --- ``gravitones'' --- sean espín-2.
Estamos muy lejos de detectar tales partículas (y no sería una sorpresa si nunca las detectáramos directamente), pero cualquier teoría cuántica de la gravedad respetable debería predecir su existencia.

Una vez que disponemos de soluciones de ondas planas para las ecuaciones de vacío linealizadas, queda por discutir la generación de radiación gravitacional por fuentes.
Para ello es necesario considerar las ecuaciones acopladas a la materia,
\begin{equation}
\Box \bh_\mn = -16\pi G T_\mn\,.\label{6.70}
\end{equation}
La solución a dicha ecuación se puede obtener utilizando la función de Green, exactamente de la misma manera que el problema análogo del electromagnetismo.
Aquí revisaremos el esquema del método.

La función de Green $G(x^\sigma - y^\sigma)$ para el operador d'alembertiano $\Box$ es la solución de la ecuación de onda en presencia de una fuente de función delta:
\begin{equation}
{\Box}_x G(x^\sigma - y^\sigma) = \delta^{(4)}(x^\sigma - y^\sigma)
\ ,\label{6.71}
\end{equation}
donde ${\Box}_x$ denota el d'alembertiano con respecto a las coordenadas $x^\sigma$.
La utilidad de dicha función reside en el hecho de que la solución general de una ecuación como (6.70) se puede escribir
\begin{equation}
\bh_\mn(x^\sigma) = -16\pi G \int G(x^\sigma - y^\sigma)T_\mn(y^\sigma)
~d^4y\ ,\label{6.72}
\end{equation}
como se puede comprobar inmediatamente.
(Observe que no son necesarios factores de $\g$, ya que nuestro fondo es simplemente Espacio-Tiempo plano).
Por supuesto, las soluciones a (6.71) se han resuelto hace mucho tiempo y pueden considerarse ``retardadas'' o ``avanzadas'', dependiendo de si representan ondas que viajan hacia adelante o hacia atrás en el tiempo.
Nuestro interés está en la función de Green retardada, que representa los efectos acumulados de las señales en el pasado del punto considerado.
esta dado por
\begin{equation}
G(x^\sigma - y^\sigma) = -{{1}\over{4\pi |\x-\y |}}\delta
[|\x-\y | - (x^0-y^0)] ~\theta(x^0-y^0)\,.\label{6.73}
\end{equation}
Aquí hemos utilizado negrita para indicar los vectores espaciales $\x = (x^1,x^2,x^3)$ y $\y=(y^1,y^2,y^3)$, con norma $|\x-\y |=[\delta_{ij}(x^i-y^i)(x^j-y^j)]^{1/2}$.
La función theta $\theta(x^0-y^0)$ es igual a $1$ cuando es $x^0> y^0$ y cero en caso contrario.
La derivación de (6.73) nos llevaría demasiado lejos, pero se puede encontrar en cualquier texto estándar sobre electrodinámica o ecuaciones diferenciales parciales en física.

Al sustituir (6.73) en (6.72), podemos usar la función delta para realizar la integral sobre $y^0$, dejándonos con
\begin{equation}
\bh_\mn(t,\x) =4G\int {{1}\over {|\x-\y |}}T_\mn(t-|\x-\y |,\y)
~d^3y\ ,\label{6.74}
\end{equation}
donde $t=x^0$.
El término ``tiempo retardado'' se utiliza para referirse a la cantidad
\begin{equation}
t_r = t-|\x-\y |\,.\label{6.75}
\end{equation}
La interpretación de (6.74) debe ser clara: la perturbación en el campo gravitacional en $(t,\x)$ es una suma de las influencias de las fuentes de energía y momento en el punto $(t_r,\x-\y)$ del cono de luz pasado.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/six06.pdf}
\end{figure}

Tomemos esta solución general y consideremos el caso en el que la radiación gravitacional es emitida por una fuente aislada, bastante lejana, compuesta de materia no relativista; estas aproximaciones se harán más precisas a medida que avancemos.
Primero necesitamos establecer algunas convenciones para las transformadas de Fourier, que siempre facilitan la vida cuando se trata de fenómenos oscilatorios.
Dada una función del Espacio-Tiempo $\phi(t,\x)$, tenemos
% will sometimes want to take its Fourier transform (and inverse) with
están interesados en su transformada de Fourier (e inversa) con respecto al tiempo solo,
\begin{align}
\widetilde\phi(\omega,\x)  &=  {{1}\over{\sqrt{2\pi}}}\int
dt~e^{i\omega t}\phi(t,\x)\ , \notag \\
\phi(t,\x)  &=  {{1}\over{\sqrt{2\pi}}}\int d\omega~e^{-i\omega t}
\widetilde\phi(\omega,\x)\,. \label{6.76}
\end{align}
Tomando la transformada de la perturbación métrica, obtenemos
\begin{align}
\widetilde\bh_\mn(\omega,\x)  &=  {{1}\over{\sqrt{2\pi}}}
\int dt~e^{i\omega t}\bh_\mn(t,\x) \notag \\
&=  {{4G}\over{\sqrt{2\pi}}}\int dt~ d^3y~e^{i\omega t}~
{{T_\mn(t-|\x-\y |,\y)}\over {|\x-\y |}} \notag \\
&=  {{4G}\over{\sqrt{2\pi}}}\int dt_r~d^3y~e^{i\omega t_r}
e^{i\omega |\x-\y |}{{T_\mn(t_r,\y)}\over {|\x-\y |}} \notag \\
&=  4G \int d^3y~e^{i\omega |\x-\y |}{{\widetilde T_\mn(\omega,\y)}
\over {|\x-\y |}}\,. \label{6.77}
\end{align}
En esta secuencia, la primera ecuación es simplemente la definición de la transformada de Fourier, la segunda línea proviene de la solución (6.74), la tercera línea es un cambio de variables de $t$ a $t_r$ y la cuarta línea es una vez De nuevo la definición de la transformada de Fourier.

Ahora hacemos las aproximaciones de que nuestra fuente está aislada, lejana y moviéndose lentamente.
Esto significa que podemos considerar que la fuente está centrada a una distancia (espacial) $R$, con las diferentes partes de la fuente a distancias $R+\delta R$ tales que $\delta R << R$.
Dado que se mueve lentamente, la mayor parte de la radiación emitida estará en frecuencias $\omega$ suficientemente bajas que $\delta R<<\omega^{-1}$.
(Esencialmente, la luz atraviesa la fuente mucho más rápido que los componentes de la propia fuente).

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/six07.pdf}
\end{figure}

\noindent
Según estas aproximaciones, el término $e^{i\omega |\x-\y |}/|\x-\y |$ puede reemplazarse por $e^{i\omega R}/R$ y sacarse de la integral.
Esto nos deja con
\begin{equation}
\widetilde\bh_\mn(\omega,\x) = 4G {{e^{i\omega R}}\over R}
\int d^3y~\widetilde T_\mn(\omega,\y)\,.\label{6.78}
\end{equation}

De hecho, no es necesario calcular todos los componentes de $\widetilde\bh_\mn(\omega,\x)$, ya que la condición de calibre armónico $\p\mu \bh^\mn(t,\x)=0$ en el espacio de Fourier implica
\begin{equation}
\widetilde\bh{}^{0\nu} = {i\over \omega}\p{i}\widetilde\bh{}^{i\nu}\,.
\label{6.79}
\end{equation}
Por lo tanto, sólo debemos preocuparnos de los componentes espaciales de $\widetilde\bh_\mn(\omega,\x)$.
Por lo tanto, de (6.78) queremos tomar la integral de las componentes espaciales de $\widetilde T_\mn(\omega,\y)$.
Comenzamos integrando por partes a la inversa:
\begin{equation}
\int d^3y~\widetilde T^{ij}(\omega,\y)=\int \p{k}
(y^i\widetilde T^{kj})~d^3y - \int y^i(\p{k}\widetilde T^{kj})~d^3y
\,.\label{6.80}
\end{equation}
El primer término es una integral de superficie que desaparecerá ya que la fuente está aislada, mientras que el segundo puede relacionarse con $\widetilde T^{0j}$ mediante la versión del espacio de Fourier de $\p\mu T^\mn=0$:
\begin{equation}
-\p{k}\widetilde T^{k\mu}=i\omega \widetilde T^{0\mu}\,.
\label{6.81}
\end{equation}
De este modo,
\begin{align}
\int d^3y~\widetilde T^{ij}(\omega,\y) &=
i\omega \int y^i \widetilde T^{0j}~d^3y  \notag \\
&=  {{i\omega}\over 2}\int (y^i \widetilde T^{0j}
+y^j \widetilde T^{0i})~d^3y  \notag \\
&=  {{i\omega}\over 2}\int\left[\p{l}(y^i y^j \widetilde T^{0l})
-y^i y^j (\p{l}\widetilde T^{0l})\right]~d^3y  \notag \\
&=  -{{\omega^2}\over 2}\int y^i y^j \widetilde T^{00}~d^3y\,.
\label{6.82}
\end{align}
La segunda línea está justificada ya que sabemos que el lado izquierdo es simétrico en $i$ y $j$, mientras que la tercera y cuarta líneas son simplemente repeticiones de integración inversa por partes y conservación de $T^\mn$.
Es convencional definir el {\bf tensor de momento cuadrupolar} de la densidad de energía de la fuente,
\begin{equation}
q_{ij}(t) = 3\int y^i y^j T^{00}(t,\y)~d^3y\ ,\label{6.83}
\end{equation}
un tensor constante en cada superficie de tiempo constante.
En términos de la transformada de Fourier del momento cuadrupolar, nuestra solución toma la forma compacta
\begin{equation}
\widetilde\bh_{ij}(\omega,\x) = -{{2G\omega^2}\over 3}
{{e^{i\omega R}}\over R} \widetilde q_{ij}(\omega)\ ,\label{6.84}
\end{equation}
o, transformándose nuevamente a $t$,
\begin{align}
\bh_{ij}(t,\x)  &=  -{1\over{\sqrt{2\pi}}}{{2G}\over {3R}}
\int d\omega~e^{-i\omega(t-R)}\omega^2\widetilde q_{ij}(\omega) \notag \\
&=  {1\over{\sqrt{2\pi}}}{{2G}\over {3R}}{{d^2}\over{dt^2}}
\int d\omega~e^{-i\omega t_r}\widetilde q_{ij}(\omega) \notag \\
&=  {{2G}\over {3R}}{{d^2 q_{ij}}\over{dt^2}}(t_r) \ , \label{6.85}
\end{align}
donde como antes $t_r = t-R$.

La onda gravitacional producida por un objeto aislado no relativista es, por tanto, proporcional a la segunda derivada del momento cuadrupolar de la densidad de energía en el punto donde el cono de luz pasado del observador cruza la fuente.
Por el contrario, la principal contribución a la radiación electromagnética proviene del momento cambiante {\it dipolo} de la densidad de carga.
La diferencia se remonta a la naturaleza universal de la gravitación.
Un momento dipolar cambiante corresponde al movimiento del centro de densidad: densidad de carga en el caso del electromagnetismo, densidad de energía en el caso de la gravitación.
Si bien no hay nada que impida que el centro de carga de un objeto oscile, la oscilación del centro de masa de un sistema aislado viola la conservación del momentum.
(Puedes sacudir un cuerpo hacia arriba y hacia abajo, pero tú y la tierra se sacuden ligeramente en la dirección opuesta para compensar).
El momento cuadripolar, que mide la forma del sistema, es generalmente más pequeño que el momento dipolar, y por esta razón (así como por el débil acoplamiento de la materia con la gravedad) la radiación gravitacional suele ser mucho más débil que la radiación electromagnética.

Siempre es educativo tomar una solución general y aplicarla a un caso específico de interés.
Un caso de auténtico interés es la radiación gravitacional emitida por una estrella binaria (dos estrellas en órbita una alrededor de la otra).
Para simplificar, consideremos dos estrellas de masa $M$ en una órbita circular en el plano $x^1$-$x^2$, a una distancia $r$ de su centro de masa común.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/six08.pdf}
\end{figure}

\noindent
Trataremos el movimiento de las estrellas en la aproximación newtoniana, donde podemos analizar su órbita tal como lo habría hecho Kepler.
Las órbitas circulares se caracterizan más fácilmente equiparando la fuerza debida a la gravedad con la fuerza ``centrífuga'' hacia afuera:
\begin{equation}
{{GM^2}\over{(2r)^2}} = {{Mv^2}\over r}\ ,\label{6.86}
\end{equation}
que nos da
\begin{equation}
v=\left( {{GM}\over{4r}}\right)^{1/2}\,.\label{6.87}
\end{equation}
El tiempo que lleva completar una sola órbita es simplemente
\begin{equation}
T = {{2\pi r}\over v}\ ,\label{6.88}
\end{equation}
pero más útil para nosotros es la frecuencia angular de la órbita,
\begin{equation}
\Omega = {{2\pi}\over T} = \left( {{GM}\over{4r^3}}\right)^{1/2}
\,.\label{6.89}
\end{equation}
En términos de $\Omega$ podemos escribir la ruta explícita de la estrella $a$,
\begin{equation}
x^1_a = r\cos\Omega t\ ,\qquad x^2_a = r\sin\Omega t\ ,\label{6.90}
\end{equation}
y estrella $b$,
\begin{equation}
x^1_b = -r\cos\Omega t\ ,\qquad x^2_b =-r\sin\Omega t\,.\label{6.91}
\end{equation}
La densidad de energía correspondiente es
\begin{equation}
T^{00}(t,\x) = M\delta(x^3)\left[\delta(x^1-r\cos\Omega t)
\delta(x^2-r\sin\Omega t) + \delta(x^1+r\cos\Omega t)
\delta(x^2+r\sin\Omega t)\right]\,.\label{6.92}
\end{equation}
La profusión de funciones delta nos permite integrar esto directamente para obtener el momento cuadrupolar de (6.83):
\begin{align}
q_{11}  &=  6Mr^2\cos^2\Omega t = 3Mr^2(1+\cos2\Omega t) \notag \\
q_{22}  &=  6Mr^2\sin^2\Omega t = 3Mr^2(1-\cos2\Omega t) \notag \\
q_{12} =q_{21}  &=  6Mr^2(\cos\Omega t)(\sin\Omega t) =
3Mr^2\sin2\Omega t \notag \\  q_{i3}  &=  0\,.\label{6.93}
\end{align}
A partir de esto, a su vez, es fácil obtener las componentes de la perturbación métrica de (6.85):
\begin{equation}
\bh_{ij}(t,\x) = {{8GM}\over R}\Omega^2r^2\left(\mqty{
-\cos2\Omega t_r & -\sin2\Omega t_r & 0 \\
-\sin2\Omega t_r & \cos2\Omega t_r & 0  \\
0 & 0 & 0 \\ }\right)\,.\label{6.94}
\end{equation}
Los componentes restantes de $\bh_\mn$ podrían derivarse de exigir que se cumpla la condición de calibre armónico.
(No hemos impuesto una condición de calibre subsidiario, por lo que todavía somos libres de hacerlo).

Es natural en este punto hablar de la energía emitida mediante radiación gravitacional.
Sin embargo, tal discusión se ve inmediatamente plagada de problemas, tanto técnicos como filosóficos.
Como hemos mencionado antes, no existe una verdadera medida local de la energía en el campo gravitacional.
Por supuesto, en el límite del campo débil, donde pensamos que la gravitación se describe mediante un tensor simétrico que se propaga sobre una métrica de fondo fija, podríamos esperar derivar un tensor de energía-momento para las fluctuaciones $h_\mn$, tal como lo haríamos para electromagnetismo o cualquier otra teoría de campo.
Hasta cierto punto esto es posible, pero todavía existen dificultades.
Como resultado de estas dificultades, existen varias propuestas diferentes en la literatura sobre lo que deberíamos utilizar como tensor de energía-momento para la gravitación en el límite del campo débil; Todos ellos son diferentes, pero en su mayor parte dan las mismas respuestas a preguntas físicamente bien planteadas, como la tasa de energía emitida por un sistema binario.

A nivel técnico, las dificultades empiezan a surgir cuando consideramos qué forma debe adoptar el tensor de energía-momento.
Anteriormente mencionamos los tensores de energía-momento para el electromagnetismo y la teoría de campos escalares, y ambos compartían una característica importante: eran cuadráticos en los campos relevantes.
Por hipótesis, nuestro enfoque para el límite del campo débil ha sido mantener únicamente los términos que son lineales en la perturbación métrica.
Por lo tanto, para realizar un seguimiento de la energía transportada por las ondas gravitacionales, tendremos que extender nuestros cálculos al menos al segundo orden en $h_\mn$.
De hecho, hemos estado haciendo un poco de trampa todo el tiempo.
Al discutir los efectos de las ondas gravitacionales en las partículas de prueba, y la generación de ondas por un sistema binario, hemos estado utilizando el hecho de que las partículas de prueba se mueven a lo largo de geodésicas.
Pero como sabemos, esto se deriva de la conservación covariante del momento de energía, $\nabla_\mu T^\mn=0$.
Sin embargo, en el orden en el que hemos estado trabajando, en realidad tenemos $\p\mu T^\mn=0$, lo que implicaría que las partículas de prueba se mueven en línea recta en la métrica de fondo plano.
Este es un síntoma de la inconsistencia fundamental del límite del campo débil.
En la práctica, lo mejor que se puede hacer es resolver las ecuaciones de campo débil en algún orden apropiado y luego justificar a posteriori la validez de la solución.

Teniendo estas cuestiones en mente, consideremos las ecuaciones de Einstein (en el vacío) de segundo orden y veamos cómo se puede interpretar el resultado en términos de un tensor de energía-momento para el campo gravitacional.
Si escribimos la métrica como $g_\mn = \eta_\mn + h_\mn$, entonces en el primer orden tenemos
\begin{equation}
G^{(1)}_\mn[\eta+h] = 0\ ,\label{6.95}
\end{equation}
donde $G^{(1)}_\mn$ es el tensor de Einstein expandido al primer orden en $h_\mn$.
Estas ecuaciones determinan $h_\mn$ hasta transformaciones de calibre (inevitables), por lo que para satisfacer las ecuaciones de segundo orden tenemos que agregar una perturbación de orden superior y escribir
\begin{equation}
g_\mn = \eta_\mn + h_\mn + h^{(2)}_\mn\,.\label{6.96}
\end{equation}
La versión de segundo orden de las ecuaciones de Einstein consta de todos los términos cuadráticos en $h_\mn$ o lineales en $h^{(2)}_\mn$.
Dado que cualquier término cruzado sería de al menos tercer orden, tenemos
\begin{equation}
G^{(1)}_\mn[\eta+h^{(2)}] +G^{(2)}_\mn[\eta+h] =0\,.
\label{6.97}
\end{equation}
Aquí, $G^{(2)}_\mn$ es la parte del tensor de Einstein que es de segundo orden en la perturbación métrica.
Se puede calcular a partir del tensor de Ricci de segundo orden, que viene dado por
\begin{align}
R^{(2)}_\mn  &=  {1\over 2}h^{\rho\sigma}\p\mu
\p\nu h_{\rho\sigma} - h^{\rho\sigma}\p\rho\p{(\mu}
h_{\nu)\sigma} +{1\over 4}(\p\mu h_{\rho\sigma})\p\nu
h^{\rho\sigma} +(\partial^\sigma h^\rho{}_\nu)
\p{[\sigma}h_{\rho]\mu}  \notag \\
&\quad +{1\over 2}\p\sigma(h^{\rho\sigma}\p\rho h_\mn)
-{1\over 4}(\p\rho h_\mn)\partial^\rho h - (\p\sigma
h^{\rho\sigma} -{1\over 2}\partial^\rho h)\p{(\mu}
h_{\nu)\rho}\,. \label{6.98}
\end{align}
Podemos expresar (6.97) en la forma sugerente
\begin{equation}
G^{(1)}_\mn[\eta+h^{(2)}] = 8\pi G t_\mn \ ,\label{6.99}
\end{equation}
simplemente definiendo
\begin{equation}
t_\mn = -{1\over {8\pi G}}G^{(2)}_\mn[\eta+h]\,.\label{6.100}
\end{equation}
Por supuesto, la notación pretende sugerir que pensemos en $t_\mn$ como un tensor de energía-momento, específicamente el del campo gravitacional (al menos en el régimen de campo débil).
Para que esta afirmación parezca plausible, tenga en cuenta que la identidad de Bianchi para $G^{(1)}_\mn[\eta+h^{(2)}]$ implica que $t_\mn$ se conserva en el sentido de espacio plano,
\begin{equation}
\p\mu t^\mn =0\,.\label{6.101}
\end{equation}

Desafortunadamente, existen algunas limitaciones en nuestra interpretación de $t_\mn$ como tensor de energía-momento.
Por supuesto, no es un tensor en absoluto en la teoría completa, pero lo dejamos de lado por hipótesis.
Más importante aún, no es invariante bajo transformaciones de calibre (difeomorfismos infinitesimales), como se puede verificar mediante cálculo directo.
Sin embargo, podemos construir cantidades globales que sean invariantes bajo ciertos tipos especiales de transformaciones de calibre (básicamente, aquellas que desaparecen con suficiente rapidez en el infinito; ver Wald).
Estos incluyen la energía total en una superficie $\Sigma$ de tiempo constante,
\begin{equation}
E=\int_\Sigma t_{00}~d^3x\ ,\label{6.102}
\end{equation}
y la energía total irradiada hasta el infinito,
\begin{equation}
\Delta E = \int_S t_{0\mu} n^\mu ~d^2x~dt\,.\label{6.103}
\end{equation}
Aquí, la integral se toma sobre una superficie temporal $S$ formada por dos esferas espaciales en el infinito y en algún intervalo de tiempo, y $n^\mu$ es un vector unitario espacial normal a $S$.

Evaluar estas fórmulas en términos del momento cuadrupolar de una fuente radiante implica un cálculo largo que no reproduciremos aquí.
Sin más, la cantidad de energía radiada se puede escribir
\begin{equation}
\Delta E = \int P ~dt\ ,\label{6.104}
\end{equation}
donde la potencia $P$ está dada por
\begin{equation}
P = {{G}\over {45}}\left[{{d^3 Q^{ij}}\over{dt^3}}
{{d^3 Q_{ij}}\over{dt^3}}\right]_{t_r}\ ,\label{6.105}
\end{equation}
y aquí $Q_{ij}$ es la parte sin rastro del momento cuadrupolar,
\begin{equation}
Q_{ij} = q_{ij}-{1\over 3}\delta_{ij} \delta^{kl}
q_{kl}\,.\label{6.106}
\end{equation}

Para el sistema binario representado por (6.93), la parte sin traza del cuadrupolo es
\begin{equation}
Q_{ij} = Mr^2 \left(\mqty{(1+3\cos2\Omega t)&
3\sin2\Omega t & 0 \\  3\sin2\Omega t & (1-3\cos2\Omega t) &0 \\
0 & 0 & -2 \\ }\right)\ ,\label{6.107}
\end{equation}
y su derivada tercera es por lo tanto
\begin{equation}
{{d^3 Q_{ij}}\over{dt^3}}=
24 M r^2\Omega^3\left(\mqty{\sin2\Omega t &
-\cos2\Omega t & 0 \\  -\cos2\Omega t & -\sin2\Omega t & 0 \\
0&0&0 \\ }\right)\,.\label{6.108}
\end{equation}
La potencia radiada por el binario es entonces
\begin{equation}
P = {{2^7}\over 5}GM^2 r^4\Omega^6\ ,\label{6.109}
\end{equation}
o, usando la expresión (6.89) para la frecuencia,
\begin{equation}
P = {2\over 5}{{G^4 M^5}\over {r^5}}\,.\label{6.110}
\end{equation}

Por supuesto, esto realmente se ha observado.
En 1974, Hulse y Taylor descubrieron un sistema binario, PSR1913+16, en el que ambas estrellas son muy pequeñas (por lo que los efectos clásicos son insignificantes, o al menos están bajo control) y una es un púlsar.
El período de la órbita es de ocho horas, extremadamente pequeño para los estándares astrofísicos.
El hecho de que una de las estrellas sea un púlsar proporciona un reloj muy preciso, con respecto al cual se puede medir el cambio en el período a medida que el sistema pierde energía.
El resultado es consistente con la predicción de la Relatividad General para la pérdida de energía debido a la radiación gravitacional.
Hulse y Taylor recibieron el Premio Nobel en 1993 por sus esfuerzos.





\chapter{La Solución de Schwarzschild y los Agujeros Negros}
%\addcontentsline{toc}{chapter}{La Solución de Schwarzschild y los Agujeros Negros}


Ahora pasamos del dominio del límite del campo débil a las soluciones de las ecuaciones no lineales completas de Einstein.
Con la posible excepción del espacio de Minkowski, la solución más importante con diferencia es la descubierta por Schwarzschild, que describe espacios-tiempos de vacío esféricamente simétricos.
Como estamos en el vacío, las ecuaciones de Einstein se convierten en $R_\mn =0$.
Por supuesto, si tenemos una solución propuesta para un conjunto de ecuaciones diferenciales como ésta, sería suficiente introducir la solución propuesta para verificarla; Sin embargo, nos gustaría hacerlo mejor.
De hecho, esbozaremos una prueba del teorema de Birkhoff, que establece que la solución de Schwarzschild es la solución {\it única} esféricamente simétrica de las ecuaciones de Einstein en el vacío.
El procedimiento será presentar primero algunos argumentos no rigurosos de que cualquier métrica esféricamente simétrica (resuelva o no las ecuaciones de Einstein) debe adoptar una determinada forma, y luego trabajar a partir de ahí para derivar con más cuidado la solución real en tal caso.

``Esféricamente simétrico'' significa ``que tiene las mismas simetrías que una esfera''. (En esta sección la palabra ``esfera'' significa $S^2$, no esferas de dimensión superior).
Dado que el objeto de interés para nosotros es la métrica de una variedad diferenciable, nos interesan aquellas métricas que tienen tales simetrías.
Sabemos cómo caracterizar las simetrías de la métrica: están dadas por la existencia de vectores Killing.
Además, sabemos cuáles son los vectores de muerte de $S^2$ y que hay tres.
Por lo tanto, una variedad esféricamente simétrica es aquella que tiene tres campos vectoriales Killing que son como los de $S^2$.
Por ``igual'' queremos decir que el conmutador de los vectores Killing es el mismo en cualquier caso; en un lenguaje más sofisticado, que el álgebra generada por los vectores es la misma.
Algo que no mostramos, pero que es cierto, es que podemos elegir nuestros tres vectores Killing en $S^2$ para que sean $(V^{(1)},V^{(2)},V^{(3)})$, de modo que
\begin{align}
[V^{(1)},V^{(2)}] &= V^{(3)} \notag \\
[V^{(2)},V^{(3)}] &= V^{(1)} \notag \\
[V^{(3)},V^{(1)}] &= V^{(2)} \,. \label{7.1}
\end{align}
Las relaciones de conmutación son exactamente las de SO(3), el grupo de rotaciones en tres dimensiones.
Por supuesto, esto no es una coincidencia, pero no lo abordaremos aquí.
Todo lo que necesitamos es que una variedad esféricamente simétrica sea aquella que posea tres campos vectoriales Killing con las relaciones de conmutación anteriores.

En la sección tres mencionamos el teorema de Frobenius, que establece que si tienes un conjunto de campos vectoriales conmutantes, entonces existe un conjunto de funciones de coordenadas tales que los campos vectoriales son las derivadas parciales con respecto a estas funciones.
De hecho, el teorema no se detiene ahí, sino que continúa diciendo que si tenemos algunos campos vectoriales que no conmutan, pero cuyo conmutador se cierra, el conmutador de dos campos cualesquiera en el conjunto es una combinación lineal de otros campos en el conjunto --- entonces las curvas integrales de estos campos vectoriales ``encajan'' para describir subvariedades de la variedad en la que están todos definidos.
La dimensionalidad de la subvariedad puede ser menor que el número de vectores, o podría ser igual, pero obviamente no mayor.
Los campos vectoriales que obedecen a (7.1) formarán, por supuesto, 2 esferas.
Dado que los campos vectoriales se extienden por todo el espacio, cada punto estará exactamente en una de estas esferas.
(En realidad, se trata de casi todos los puntos; a continuación mostraremos cómo puede no ser absolutamente todos los puntos).
Por lo tanto, decimos que una variedad esféricamente simétrica puede foliarse en esferas.

Consideremos algunos ejemplos para poner esto en práctica.
El ejemplo más simple es el espacio euclidiano plano tridimensional.
Si elegimos un origen, entonces $\R^3$ es claramente esféricamente simétrico con respecto a las rotaciones alrededor de este origen.
Bajo tales rotaciones ({\it ie}, bajo el flujo de los campos vectoriales Killing), los puntos se mueven entre sí, pero cada punto permanece en un $S^2$ a una distancia fija del origen.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven01.pdf}
\end{figure}

\noindent
Son estas esferas las que folian $\R^3$.
Por supuesto, en realidad no folian todo el espacio, ya que el origen en sí simplemente permanece bajo rotaciones, no se mueve en ninguna doble esfera.
Pero conviene tener claro que casi todo el espacio está debidamente foliado, y esto nos resultará suficiente.

También podemos tener simetría esférica sin un ``origen'' para rotar las cosas.
Un ejemplo lo proporciona un ``agujero de gusano'', con topología $\R\times S^2$.
Si suprimimos una dimensión y dibujamos nuestras dos esferas como círculos, dicho espacio podría verse así:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven02.pdf}
\end{figure}

\noindent
En este caso, toda la variedad puede estar foliada por dos esferas.

Esta estructura foliada sugiere que coloquemos coordenadas en nuestra variedad de una manera que se adapte a la foliación.
Con esto queremos decir que, si tenemos una variedad de dimensión $n$ foliada por subvariedades de dimensión $m$, podemos usar un conjunto de funciones de coordenadas $m$ $u^i$ en las subvariedades y un conjunto de coordenadas de $n-m$. funciones $v^I$ para decirnos en qué subvariedad estamos.
(Entonces, $i$ va de 1 a $m$, mientras que $I$ va de 1 a $n-m$.)
Luego, la colección de $v$ y $u$ coordina todo el espacio.
Si las subvariedades son espacios máximamente simétricos (como lo son las dos esferas), entonces existe el siguiente poderoso teorema: siempre es posible elegir las coordenadas $u$ de modo que la métrica de toda la variedad sea de la forma
\begin{equation}
ds^2 = g_\mn  d\,x^\mu  d\,x^\nu = g_{IJ}(v) d\,v^I  d\,v^J
+f(v)\gamma_{ij}(u) d\,u^i  d\,u^j\,.\label{7.2}
\end{equation}
Aquí $\gamma_{ij}(u)$ es la métrica en la subcolectora.
Este teorema dice dos cosas a la vez: que no hay términos cruzados $ d\,v^I  d\,u^j$ y que tanto $g_{IJ}(v)$ como $f(v)$ son funciones de $v^I$ únicamente, independientes de $u^i$.
Demostrar el teorema es un desastre, pero te animamos a consultar el capítulo 13 de Weinberg.
Sin embargo, es un resultado perfectamente sensato.
En términos generales, si $g_{IJ}$ o $f$ dependieran de $u^i$, entonces la métrica cambiaría a medida que nos moviéramos en una única subvariedad, lo que viola el supuesto de simetría.
Mientras tanto, los términos cruzados no deseados se pueden eliminar asegurándose de que los vectores tangentes $\partial/\partial v^I$ sean ortogonales a las subvariedades; en otras palabras, que alineemos nuestras subvariedades de la misma manera en todo el espacio.

Ya hemos terminado con los gestos y podemos comenzar a hacer algunos cálculos honestos.
Para el caso que nos ocupa, nuestras subvariedades son dos esferas, en las que normalmente elegimos las coordenadas $(\theta,\phi)$ en las que la métrica toma la forma
\begin{equation}
d\Omega^2 =  d\,\theta^2 + \sin^2\theta\  d\,\phi^2\,.\label{7.3}
\end{equation}
Como estamos interesados en un Espacio-Tiempo de cuatro dimensiones, necesitamos dos coordenadas más, que podemos llamar $a$ y $b$.
El teorema (7.2) nos dice entonces que la métrica en un espaciotiempo esféricamente simétrico se puede expresar en la forma
\begin{equation}
ds^2 = g_{aa}(a,b) d\,a^2 + g_{ab}(a,b)( d\,a d\,b+ d\,b d\,a)
+g_{bb}(a,b) d\,b^2 + r^2(a,b)d\Omega^2\,.\label{7.4}
\end{equation}
Aquí $r(a,b)$ hay una función aún indeterminada, a la que simplemente le hemos dado una etiqueta sugerente.
Sin embargo, nada nos impide cambiar las coordenadas de $(a,b)$ a $(a,r)$, invirtiendo $r(a,b)$.
(Lo único que podría detenernos sería si $r$ fuera una función de $a$ únicamente; en este caso, podríamos cambiar fácilmente a $(b,r)$, por lo que no consideraremos esta situación por separado).
La métrica es entonces
\begin{equation}
ds^2 = g_{aa}(a,r) d\,a^2 + g_{ar}(a,r)( d\,a d\,r+ d\,r d\,a)
+g_{rr}(a,r) d\,r^2 + r^2 d\Omega^2\,.\label{7.5}
\end{equation}
Nuestro siguiente paso es encontrar una función $t(a,r)$ tal que, en el sistema de coordenadas $(t,r)$, no haya términos cruzados $ d\,t d\,r+ d\,r d\,t$ en la métrica.
Darse cuenta de
\begin{equation}
 d\,t = {{\partial t}\over{\partial a}} d\,a + {{\partial t}\over
{\partial r}} d\,r \ ,\label{7.6}
\end{equation}
entonces
\begin{equation}
 d\,t^2 = \left({{\partial t}\over{\partial a}}\right)^2 d\,a^2
+ \left({{\partial t}\over{\partial a}}\right)\left({{\partial t}
\over{\partial r}}\right)
( d\,a d\,r+ d\,r d\,a) + \left({{\partial t}\over{\partial r}}\right)^2
 d\,r^2\,.\label{7.7}
\end{equation}
Nos gustaría reemplazar los primeros tres términos de la métrica (7.5) por
\begin{equation}
m d\,t^2 + n d\,r^2\ ,\label{7.8}
\end{equation}
para algunas funciones $m$ y $n$.
Esto equivale a los requisitos
\begin{equation}
m\left({{\partial t}\over{\partial a}}\right)^2 = g_{aa}\ ,\label{7.9}
\end{equation}
\begin{equation}
n+m\left({{\partial t}\over{\partial r}}\right)^2 = g_{rr}\ ,\label{7.10}
\end{equation}
y
\begin{equation}
m\left({{\partial t}\over{\partial a}}\right)\left({{\partial t}\over
{\partial r}}\right)=g_{ar}\,.\label{7.11}
\end{equation}
Por lo tanto, tenemos tres ecuaciones para las tres incógnitas $t(a,r)$, $m(a,r)$ y $n(a,r)$, lo suficiente para determinarlas con precisión (hasta las condiciones iniciales para $t$).
(Por supuesto, están ``determinadas'' en términos de las funciones desconocidas $g_{aa}$, $g_{ar}$ y $g_{rr}$, por lo que en este sentido todavía están indeterminadas).
Por lo tanto, podemos poner nuestra métrica en la forma
\begin{equation}
ds^2 = m(t,r) d\,t^2 + n(t,r) d\,r^2+ r^2 d\Omega^2\,.\label{7.12}
\end{equation}

Hasta este punto, la única diferencia entre las dos coordenadas $t$ y $r$ es que hemos elegido $r$ para que sea la que multiplique la métrica de las dos esferas.
Esta elección fue motivada por lo que sabemos sobre la métrica del espacio plano de Minkowski, que se puede escribir $ds^2 = - d\,t^2 +  d\,r^2+ r^2 d\Omega^2$.
Sabemos que el Espacio-Tiempo considerado es lorentziano, por lo que $m$ o $n$ tendrán que ser negativos.
Elijamos que $m$, el coeficiente de $ d\,t^2$, sea negativo.
Esta no es una elección que simplemente se nos permita tomar y, de hecho, veremos más adelante que puede salir mal, pero lo asumiremos por ahora.
La suposición no es completamente descabellada, ya que sabemos que el espacio de Minkowski es en sí mismo esféricamente simétrico y, por lo tanto, será descrito por (7.12).
Con esta elección podemos intercambiar las funciones $m$ y $n$ por nuevas funciones $\alpha$ y $\beta$, de modo que
\begin{equation}
ds^2 = -e^{2\alpha(t,r)} d\,t^2 + e^{2\beta(t,r)} d\,r^2
+ r^2 d\Omega^2\,.\label{7.13}
\end{equation}

Esto es lo mejor que podemos hacer para una métrica general en un Espacio-Tiempo esféricamente simétrico.
El siguiente paso es resolver las ecuaciones de Einstein, lo que nos permitirá determinar explícitamente las funciones $\alpha(t,r)$ y $\beta(t,r)$.
Desafortunadamente, es necesario calcular los símbolos de Christoffel para (7.13), de los cuales podemos obtener el tensor de curvatura y, por tanto, el tensor de Ricci.
Si utilizamos las etiquetas $(0,1,2,3)$ para $(t,r,\theta,\phi)$ de la forma habitual, los símbolos de Christoffel vienen dados por
\begin{align}
&\Gamma^0_{00}=\p0\alpha\qquad\quad
\Gamma^0_{01} = \p1\alpha \qquad\quad
\Gamma^0_{11} = e^{2(\beta-\alpha)}\p0\beta & \notag \\  &
\Gamma^1_{00} = e^{2(\alpha-\beta)}\p1\alpha\qquad
\Gamma^1_{01} = \p0\beta \qquad\quad
\Gamma^1_{11} = \p1\beta &  \notag \\  &
\Gamma^2_{12} = {1\over r}\qquad
\Gamma^1_{22} = - r e^{-2\beta}\qquad
\Gamma^3_{13} = {1\over r} & \notag \\  &
\Gamma^1_{33} = -r e^{-2\beta}\sin^2\theta\qquad
\Gamma^2_{33} = -\sin\theta \cos\theta \qquad
\Gamma^3_{23} = {{\cos\theta}\over {\sin\theta}}\,.&\label{7.14}
\end{align}
(Todo lo que no esté escrito explícitamente debe ser cero o estar relacionado con lo que está escrito mediante simetrías).
De estos obtenemos los siguientes componentes que no desaparecen del tensor de Riemann:
\begin{align}
R^0{}_{101}  &=  e^{2(\beta-\alpha)}[\p0^2\beta +(\p0\beta)^2
-\p0\alpha \p0\beta]+[\p1\alpha\p1\beta-\p1^2\alpha -(\p1\alpha)^2] \notag \\
R^0{}_{202}  &=  -r e^{-2\beta}\p1\alpha  \notag \\
R^0{}_{303}  &=  -r e^{-2\beta}\sin^2\theta\ \p1\alpha  \notag \\
R^0{}_{212}  &=  -r e^{-2\alpha}\p0\beta  \notag \\
R^0{}_{313}  &=  -r e^{-2\alpha}\sin^2\theta\ \p0\beta  \notag \\
R^1{}_{212}  &=  r e^{-2\beta}\p1\beta  \notag \\
R^1{}_{313}  &=  r e^{-2\beta}\sin^2\theta\ \p1\beta  \notag \\
R^2{}_{323}  &=  (1-e^{-2\beta})\sin^2\theta\,.\label{7.15}
\end{align}
Tomando la contracción como de costumbre se obtiene el tensor de Ricci:
\begin{align}
R_{00}  &=  [\p0^2\beta +(\p0\beta)^2-\p0\alpha \p0\beta] +
e^{2(\alpha-\beta)}[\p1^2\alpha +(\p1\alpha)^2-\p1\alpha\p1\beta
+{2\over{r}}\p1\alpha] \notag \\
R_{11}  &=  -[\p1^2\alpha +(\p1\alpha)^2-\p1\alpha\p1\beta
-{2\over{r}}\p1\beta] + e^{2(\beta-\alpha)}[\p0^2\beta +(\p0\beta)^2
-\p0\alpha \p0\beta] \notag \\
R_{01}  &=  {2\over{r}}\p0\beta  \notag \\
R_{22}  &=  e^{-2\beta}[r(\p1\beta-\p1\alpha)-1]+1 \notag \\
R_{33}  &=  R_{22}\sin^2\theta\,.\label{7.16}
\end{align}

Nuestro trabajo es configurar $R_\mn=0$.
De $R_{01}=0$ obtenemos
\begin{equation}
\p0\beta = 0\,.\label{7.17}
\end{equation}
Si consideramos tomar la derivada del tiempo de $R_{22}=0$ y usar $\p0\beta = 0$, obtenemos
\begin{equation}
\p0\p1\alpha =0\,.\label{7.18}
\end{equation}
Por lo tanto podemos escribir
\begin{align}
\beta  &=  \beta(r) \notag \\
\alpha  &=  f(r)+g(t)\,.\label{7.19}
\end{align}
Por tanto, el primer término de la métrica (7.13) es $-e^{2f(r)}e^{2g(t)}  d\,t^2$.
Pero siempre podríamos simplemente redefinir nuestra coordenada de tiempo reemplazando $ d\,t\rightarrow e^{-g(t)} d\,t$; en otras palabras, somos libres de elegir $t$ tal que $g(t)=0$, de donde $\alpha(t,r)=f(r)$.
Por lo tanto tenemos
\begin{equation}
ds^2 = -e^{2\alpha(r)} d\,t^2 + e^{\beta(r)} d\,r^2
+ r^2 d\Omega^2\,.\label{7.20}
\end{equation}
Todos los componentes métricos son independientes de la coordenada $t$.
Por lo tanto, hemos demostrado un resultado crucial: \textit{si cualquier métrica de vacío esféricamente simétrica posee un vector Killing temporal.}

Esta propiedad es tan interesante que recibe su propio nombre: una métrica que posee un vector Killing temporal se llama {\bf estacionario}.
También hay una propiedad más restrictiva: una métrica se llama {\bf estática} si posee un vector Killing temporal que es ortogonal a una familia de hipersuperficies.
(Una hipersuperficie en una variedad de dimensiones $n$ es simplemente una subvariedad de dimensiones ($n-1$).)
La métrica (7.20) no sólo es estacionaria, sino también estática; el campo del vector Killing $\p0$ es ortogonal a las superficies $t=const$ (ya que no hay términos cruzados como $ d\,t d\,r$, etc.).
En términos generales, una métrica estática es aquella en la que nada se mueve, mientras que una métrica estacionaria permite que las cosas se muevan pero sólo de forma simétrica.
Por ejemplo, la métrica estática esféricamente simétrica (7.20) describirá estrellas no giratorias o agujeros negros, mientras que los sistemas giratorios (que siguen girando de la misma manera en todo momento) se describirán mediante métricas estacionarias.
Es difícil recordar qué palabra corresponde a cada concepto, pero la distinción entre los dos conceptos debería ser comprensible.

Sigamos buscando la solución.
Como tanto $R_{00}$ como $R_{11}$ desaparecen, podemos escribir
\begin{equation}
0=e^{2(\beta-\alpha)}R_{00} + R_{11} = {2\over r}(\p1\alpha+
\p1\beta)\ ,\label{7.21}
\end{equation}
lo que implica $\alpha = -\beta + {\rm~constant}$.
Una vez más, podemos deshacernos de la constante escalando nuestras coordenadas, por lo que tenemos
\begin{equation}
\alpha = -\beta\,.\label{7.22}
\end{equation}
A continuación pasemos a $R_{22}=0$, que ahora dice
\begin{equation}
e^{2\alpha}(2r\p1\alpha+1)=1\,.\label{7.23}
\end{equation}
Esto es completamente equivalente a
\begin{equation}
\p1(r e^{2\alpha})=1\,.\label{7.24}
\end{equation}
Podemos resolver esto para obtener
\begin{equation}
e^{2\alpha}=1+{\mu\over r}\ ,\label{7.25}
\end{equation}
donde $\mu$ es una constante indeterminada.
Con (7.22) y (7.25), nuestra métrica se convierte en
\begin{equation}
ds^2 = -\left(1+{\mu\over r}\right) d\,t^2 +
\left(1+{\mu\over r}\right)^{-1} d\,r^2
+ r^2 d\Omega^2\,.\label{7.26}
\end{equation}
Ahora no nos queda libertad excepto para la constante única $\mu$, por lo que esta forma resuelve mejor las ecuaciones restantes $R_{00}=0$ y $R_{11}=0$; es sencillo comprobar que así es, para cualquier valor de $\mu$.

Lo único que queda por hacer es interpretar la constante $\mu$ en términos de algún parámetro físico.
El uso más importante de una solución de vacío esféricamente simétrica es representar el Espacio-Tiempo fuera de una estrella, un planeta o cualquier otra cosa.
En ese caso, esperaríamos recuperar el límite de campo débil como $r\rightarrow\infty$.
En este límite, (7.26) implica
\begin{align}
g_{00}(r\rightarrow\infty)  &=  -\left(1+{\mu\over r}\right)\ , \notag \\
g_{rr}(r\rightarrow\infty)  &=  \left(1-{\mu\over r}\right)\,.
\label{7.27}
\end{align}
El límite de campo débil, por otro lado, tiene
\begin{align}
g_{00}  &=  -\left(1+2\Phi\right)\ , \notag \\
g_{rr}  &=  \left(1-2\Phi\right)\ ,
\label{7.28}
\end{align}
con el potencial $\Phi=-GM/r$.
Por lo tanto las métricas sí coinciden en este límite, si configuramos $\mu = -2GM$.

Nuestro resultado final es la célebre {\bf métrica de Schwarzschild},
\begin{equation}
ds^2 = -\left(1-{{2GM}\over r}\right) d\,t^2 +
\left(1-{{2GM}\over r}\right)^{-1} d\,r^2
+ r^2 d\Omega^2\,.\label{7.29}
\end{equation}
Esto es cierto para cualquier solución de vacío esféricamente simétrica de las ecuaciones de Einstein; $M$ funciona como un parámetro, que sabemos que puede interpretarse como la masa newtoniana convencional que mediríamos estudiando órbitas a grandes distancias de la fuente gravitatoria.
Tenga en cuenta que como $M\rightarrow 0$ recuperamos espacio de Minkowski, lo cual es de esperarse.
Tenga en cuenta también que la métrica se vuelve progresivamente minkowskiana a medida que avanzamos hasta $r\rightarrow\infty$; esta propiedad se conoce como {\bf planitud asintótica}.

El hecho de que la métrica de Schwarzschild no sea sólo una buena solución, sino que sea la única solución de vacío esféricamente simétrica, se conoce como {\bf teorema de Birkhoff}.
Es interesante observar que el resultado es una métrica estática.
No dijimos nada sobre la fuente excepto que fuera esféricamente simétrica.
Específicamente, no exigimos que la fuente en sí fuera estática; podría ser una estrella en colapso, siempre que el colapso fuera simétrico.
Por lo tanto, se esperaría que un proceso como la explosión de una supernova, que es básicamente esférica, generara muy poca radiación gravitacional (en comparación con la cantidad de energía liberada a través de otros canales).
Este es el mismo resultado que habríamos obtenido en el electromagnetismo, donde los campos electromagnéticos alrededor de una distribución de carga esférica no dependen de la distribución radial de las cargas.

Antes de explorar el comportamiento de las partículas de prueba en la geometría de Schwarzschild, deberíamos decir algo sobre las singularidades.
A partir de la forma de (7.29), los coeficientes métricos se vuelven infinitos en $r=0$ y $r=2GM$, una señal aparente de que algo va mal.
Los coeficientes métricos, por supuesto, son cantidades que dependen de las coordenadas y, como tales, no debemos darle demasiada importancia a sus valores; ciertamente es posible tener una ``singularidad de coordenadas'' que resulte de una ruptura de un sistema de coordenadas específico en lugar de la variedad subyacente.
Un ejemplo ocurre en el origen de las coordenadas polares del plano, donde la métrica $ds^2 =  d\,r^2 + r^2  d\,\theta^2$ se degenera y la componente $g^{\theta\theta}=r^{-2}$ de la métrica inversa explota, aunque ese punto de la variedad no sea diferente de cualquier otro.

¿Qué tipo de señal independiente de las coordenadas deberíamos buscar como advertencia de que algo en la geometría está fuera de control? Ésta resulta ser una pregunta difícil de responder y se han escrito libros enteros sobre la naturaleza de las singularidades en la Relatividad General.
No entraremos en este tema en detalle, sino que recurriremos a un criterio simple para determinar cuándo algo salió mal: cuando la curvatura se vuelve infinita.
La curvatura se mide mediante el tensor de Riemann y es difícil decir cuándo un tensor se vuelve infinito, ya que sus componentes dependen de las coordenadas.
Pero a partir de la curvatura podemos construir varias cantidades escalares, y dado que los escalares son independientes de las coordenadas, tendrá sentido decir que se vuelven infinitos.
Este escalar más simple es el escalar de Ricci $R=g^\mn R_\mn$, pero también podemos construir escalares de orden superior como $R^\mn R_\mn$, $R^{\mn\rho\sigma}R_{\mn\rho\sigma}$, $R_{\mn\rho\sigma} R^{\rho\sigma\lambda\tau} R_{\lambda\tau}{}^{\mn}$, etc.
Si alguno de estos escalares (no necesariamente todos) llega al infinito cuando nos acercamos a algún punto, consideraremos ese punto como una singularidad de la curvatura.
También deberíamos comprobar que el punto no esté ``infinitamente lejos''; es decir, que se puede llegar recorriendo una distancia finita a lo largo de una curva.

Tenemos por tanto una condición suficiente para que un punto sea considerado una singularidad.
Sin embargo, no es una condición necesaria y generalmente es más difícil demostrar que un punto dado no es singular; Para nuestros propósitos, simplemente probaremos para ver si las geodésicas se comportan bien en el punto en cuestión y, de ser así, consideraremos el punto no singular.
En el caso de la métrica de Schwarzschild (7.29), el cálculo directo revela que
\begin{equation}
R^{\mn\rho\sigma}R_{\mn\rho\sigma} = {{12 G^2 M^2}\over {r^6}}\,.
\label{7.30}
\end{equation}
Esto es suficiente para convencernos de que $r=0$ representa una singularidad honesta.
En el otro punto problemático, $r=2GM$, puedes comprobar y ver que ninguna de las invariantes de curvatura explota.
Por lo tanto, empezamos a pensar que en realidad no es singular y que simplemente hemos elegido un mal sistema de coordenadas.
Lo mejor que puedes hacer es transformarte a coordenadas más apropiadas si es posible.
Pronto veremos que en este caso sí es posible, y la superficie $r=2GM$ se comporta muy bien (aunque interesante) en la métrica de Schwarzschild.

Habiendonos preocupado un poco por las singularidades, debemos señalar que el comportamiento de Schwarzschild en $r\leq 2GM$ tiene pocas consecuencias en el día a día.
La solución que dedujimos es válida sólo en el vacío y esperamos que se mantenga fuera de un cuerpo esférico como una estrella.
Sin embargo, en el caso del Sol estamos ante un cuerpo que se extiende hasta un radio de
\begin{equation}
R_\odot = 10^6 G M_\odot\,.\label{7.31}
\end{equation}
Por lo tanto, $r=2GM_\odot$ está muy dentro del interior solar, donde no esperamos que lo implique la métrica de Schwarzschild.
De hecho, las soluciones interiores estelares y realistas son de la forma
\begin{equation}
ds^2 = -\left(1-{{2Gm(r)}\over r}\right) d\,t^2 +
\left(1-{{2Gm(r)}\over r}\right)^{-1} d\,r^2
+ r^2 d\Omega^2\,.\label{7.32}
\end{equation}
Consulte Schutz para obtener más detalles.
Aquí $m(r)$ es una función de $r$ que llega a cero más rápido que el propio $r$, por lo que no hay singularidades con las que lidiar.
Sin embargo, hay objetos para los que se requiere la métrica completa de Schwarzschild (los agujeros negros) y, por lo tanto, en esta sección dejaremos volar nuestra imaginación más allá del sistema solar.

El primer paso que daremos para comprender mejor esta métrica es considerar el comportamiento de las geodésicas.
Necesitamos los símbolos de Christoffel distintos de cero para Schwarzschild:
\begin{align}
&\Gamma^1_{00} = {{GM}\over {r^3}}(r-2GM)\qquad
\Gamma^1_{11} = {{-GM}\over{r(r-2GM)}} \qquad
\Gamma^0_{01} = {{GM}\over{r(r-2GM)}}  \notag \\
&\quad\qquad\Gamma^2_{12} = {1\over r}\qquad\quad
\Gamma^1_{22} = - (r-2GM)\qquad\quad
\Gamma^3_{13} = {1\over r} \notag \\
&\quad\Gamma^1_{33} = -(r-2GM)\sin^2\theta\qquad
\Gamma^2_{33} = -\sin\theta \cos\theta \qquad
\Gamma^3_{23} = {{\cos\theta}\over {\sin\theta}}\,.\label{7.33}
\end{align}
Por lo tanto, la ecuación geodésica se convierte en las siguientes cuatro ecuaciones, donde $\lambda$ es un parámetro afín:
\begin{equation}
{{d^2 t}\over{d\lambda^2}} + {{2GM}\over{r(r-2GM)}}
{{dr}\over{d\lambda}}{{dt}\over{d\lambda}} =0\ ,\label{7.34}
\end{equation}
\begin{align}
\lefteqn{{{d^2 r}\over{d\lambda^2}} + {{GM}\over {r^3}}(r-2GM)
\left({{dt}\over{d\lambda}}\right)^2 - {{GM}\over{r(r-2GM)}}
\left({{dr}\over{d\lambda}}\right)^2}  \notag \\
& -(r-2GM)\left[
\left({{d\theta}\over{d\lambda}}\right)^2+\sin^2\theta
\left({{d\phi}\over{d\lambda}}\right)^2\right] = 0 \ ,\label{7.35}
\end{align}
\begin{equation}
{{d^2 \theta}\over{d\lambda^2}} + {2\over r}{{d\theta}\over{d\lambda}}
{{dr}\over{d\lambda}} - \sin\theta \cos\theta
\left({{d\phi}\over{d\lambda}}\right)^2 = 0\ ,\label{7.36}
\end{equation}
y
\begin{equation}
{{d^2 \phi}\over{d\lambda^2}} + {2\over r}{{d\phi}\over{d\lambda}}
{{dr}\over{d\lambda}} + 2{{\cos\theta}\over {\sin\theta}}
{{d\theta}\over{d\lambda}}{{d\phi}\over{d\lambda}} = 0\,.\label{7.37}
\end{equation}
No parece haber muchas esperanzas de resolver simplemente este conjunto de ecuaciones acopladas mediante inspección.
Afortunadamente, nuestra tarea se simplifica enormemente gracias al alto grado de simetría de la métrica de Schwarzschild.
Sabemos que hay cuatro vectores Killing: tres para la simetría esférica y uno para las traslaciones temporales.
Cada uno de estos conducirá a una constante del movimiento de una partícula libre; si $K^\mu$ es un vector de muerte, sabemos que
\begin{equation}
K_\mu {{dx^\mu}\over{d\lambda}} = {\rm constant}\,.\label{7.38}
\end{equation}
Además, existe otra constante del movimiento que siempre tenemos para las geodésicas; La compatibilidad métrica implica que a lo largo del camino la cantidad
\begin{equation}
\epsilon = -g_\mn {{dx^\mu}\over{d\lambda}}{{dx^\nu}\over{d\lambda}}
\label{7.39}
\end{equation}
es constante.
Por supuesto, para una partícula masiva normalmente elegimos $\lambda = \tau$, y esta relación simplemente se convierte en $\epsilon = -g_\mn U^\mu U^\nu=+1$.
Para una partícula sin masa siempre tenemos $\epsilon =0$.
También nos ocuparemos de las geodésicas espaciales (aunque no corresponden a trayectorias de partículas), para las cuales elegiremos $\epsilon = -1$.

En lugar de escribir inmediatamente expresiones explícitas para las cuatro cantidades conservadas asociadas con los vectores Killing, pensemos en lo que nos dicen.
Observe que las simetrías que representan también están presentes en el Espacio-Tiempo plano, donde las cantidades conservadas a las que conducen son muy familiares.
La invariancia bajo traslaciones temporales conduce a la conservación de la energía, mientras que la invariancia bajo rotaciones espaciales conduce a la conservación de los tres componentes del momento angular.
En esencia, lo mismo se aplica a la métrica de Schwarzschild.
Podemos pensar en el momento angular como un vector de tres con una magnitud (un componente) y una dirección (dos componentes).
La conservación de la dirección del momento angular significa que la partícula se moverá en un plano.
Podemos elegir que este sea el plano ecuatorial de nuestro sistema de coordenadas; si la partícula no está en este plano, podemos rotar coordenadas hasta que esté.
Por tanto, los dos vectores de Killing que conducen a la conservación de la dirección del momento angular implican
\begin{equation}
\theta = {\pi\over 2}\,.\label{7.40}
\end{equation}
Los dos vectores Killing restantes corresponden a la energía y la magnitud del momento angular.
La energía surge del vector de muerte temporal $K = \partial_t$, o
\begin{equation}
K_\mu = \left(-\left(1-{{2GM}\over r}\right),0,0,0\right)
\,.\label{7.41}
\end{equation}
El vector Killing cuya cantidad conservada es la magnitud del momento angular es $L = \p\phi$, o
\begin{equation}
L_\mu = \left(0,0,0,r^2\sin^2\theta \right)\,.\label{7.42}
\end{equation}
Dado que (7.40) implica que $\sin\theta = 1$ a lo largo de las geodésicas que nos interesan, las dos cantidades conservadas son
\begin{equation}
\left(1-{{2GM}\over r}\right){{dt}\over{d\lambda}}=E\ ,\label{7.43}
\end{equation}
y
\begin{equation}
r^2{{d\phi}\over{d\lambda}}=L\,.\label{7.44}
\end{equation}
Para partículas sin masa, estos pueden considerarse como energía y momento angular; para partículas masivas son la energía y el momento angular por unidad de masa de la partícula.
Tenga en cuenta que la constancia de (7.44) es el equivalente en Relatividad General de la segunda ley de Kepler (áreas iguales se barren en tiempos iguales).

Juntas, estas cantidades conservadas proporcionan una manera conveniente de comprender las órbitas de las partículas en la geometría de Schwarzschild.
Expandamos la expresión (7.39) para $\epsilon$ para obtener
\begin{equation}
-\left(1-{{2GM}\over r}\right)\left({{dt}\over{d\lambda}}\right)^2 +
\left(1-{{2GM}\over r}\right)^{-1}\left({{dr}\over{d\lambda}}\right)^2
+r^2\left({{d\phi}\over{d\lambda}}\right)^2 = -\epsilon\,.\label{7.45}
\end{equation}
Si multiplicamos esto por $(1-2GM/r)$ y usamos nuestras expresiones para $E$ y $L$, obtenemos
\begin{equation}
-E^2+\left({{dr}\over{d\lambda}}\right)^2+\left(1-{{2GM}\over r}\right)
\left({{L^2}\over{r^2}} +\epsilon\right) =0\,.\label{7.46}
\end{equation}
Esto ciertamente es un progreso, ya que tomamos un sistema desordenado de ecuaciones acopladas y obtuvimos una sola ecuación para $r(\lambda)$.
Se ve aún mejor si lo reescribimos como
\begin{equation}
{1\over 2}\left({{dr}\over{d\lambda}}\right)^2+ V(r) =
{1\over 2}E^2\ ,\label{7.47}
\end{equation}
dónde
\begin{equation}
V(r) = {1\over 2}\epsilon - \epsilon{{GM}\over r} +
{{L^2}\over{2r^2}} - {{GML^2}\over {r^3}}\,.\label{7.48}
\end{equation}
En (7.47) tenemos precisamente la ecuación para una partícula clásica de unidad de masa y ``energía'' ${1\over 2}E^2$ que se mueve en un potencial unidimensional dado por $V(r)$.
(La energía verdadera por unidad de masa es $E$, pero el potencial efectivo para la coordenada $r$ responde a ${1\over 2}E^2$.)

Por supuesto, nuestra situación física es bastante diferente a la de una partícula clásica que se mueve en una dimensión.
Las trayectorias consideradas son órbitas alrededor de una estrella u otro objeto:

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven03.pdf}
\end{figure}

\noindent
Las cantidades que nos interesan no sólo son $r(\lambda)$, sino también $t(\lambda)$ y $\phi(\lambda)$.
Sin embargo, podemos avanzar mucho hacia la comprensión de todas las órbitas si comprendemos su comportamiento radial, y es de gran ayuda reducir este comportamiento a un problema que sabemos cómo resolver.

Un análisis similar de las órbitas en la gravedad newtoniana habría producido un resultado similar; la ecuación general (7.47) habría sido la misma, pero el potencial efectivo (7.48) no habría tenido el último término.
(Tenga en cuenta que esta ecuación no es una serie de potencias en $1/r$, es exacta).
En el potencial (7.48), el primer término es simplemente una constante, el segundo término corresponde exactamente al potencial gravitacional newtoniano y el tercer término es una contribución del momento angular que toma la misma forma en la gravedad newtoniana y la Relatividad General.
El último término, la contribución de la Relatividad General, marcará una gran diferencia, especialmente en el caso del pequeño $r$.

Examinemos los tipos de órbitas posibles, como se ilustra en las figuras.
Existen diferentes curvas $V(r)$ para diferentes valores de $L$; Para cualquiera de estas curvas, el comportamiento de la órbita se puede juzgar comparando ${1\over 2}E^2$ con $V(r)$.
El comportamiento general de la partícula será moverse en el potencial hasta llegar a un ``punto de inflexión'' donde $V(r)={1\over 2}E^2$, donde comenzará a moverse en la otra dirección.
A veces puede que no haya un punto de inflexión hacia el cual impactar, en cuyo caso la partícula simplemente continúa su camino.
En otros casos, la partícula puede simplemente moverse en una órbita circular en el radio $r_c= const$; esto puede suceder si el potencial es plano, $dV/dr=0$.
Derivando (7.48), encontramos que las órbitas circulares ocurren cuando
\begin{equation}
\epsilon GM r_c^2 - L^2 r_c +3GML^2\gamma =0\ ,\label{7.49}
\end{equation}
donde $\gamma=0$ en gravedad newtoniana y $\gamma=1$ en Relatividad General.
Las órbitas circulares serán estables si corresponden a un mínimo del potencial, e inestables si corresponden a un máximo.
Las órbitas limitadas que no son circulares oscilarán alrededor del radio de la órbita circular estable.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven04.pdf}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven05.pdf}
\end{figure}

Volviendo a la gravedad newtoniana, encontramos que las órbitas circulares aparecen en
\begin{equation}
r_c = {{L^2}\over{\epsilon GM}}\,.\label{7.50}
\end{equation}
Para partículas sin masa $\epsilon=0$, y no hay órbitas circulares; esto es consistente con la figura, que ilustra que no hay órbitas limitadas de ningún tipo.
Aunque está algo oscurecido en este sistema de coordenadas, las partículas sin masa en realidad se mueven en línea recta, ya que la fuerza gravitacional newtoniana sobre una partícula sin masa es cero.
(Por supuesto, la posición de las partículas sin masa en la teoría newtoniana es algo problemática, pero la ignoraremos por ahora.)
En términos del potencial efectivo, un fotón con una energía dada $E$ vendrá desde $r=\infty$ y gradualmente ``se desacelerará'' (en realidad, $dr/d\lambda$ disminuirá, pero la velocidad de la luz no cambia) hasta que llega al punto de inflexión, cuando comenzará a alejarse de regreso a $r=\infty$.
Los valores más bajos de $L$, para los cuales el fotón se acercará antes de comenzar a alejarse, son simplemente aquellas trayectorias que inicialmente apuntan más cerca del cuerpo gravitante.
Para las partículas masivas habrá órbitas circulares estables en el radio (7,50), así como órbitas limitadas que oscilarán alrededor de este radio.
Si la energía es mayor que el valor asintótico $E=1$, las órbitas estarán libres, describiendo una partícula que se acerca a la estrella y luego se aleja.
Sabemos que las órbitas en la teoría de Newton son secciones cónicas (las órbitas ligadas son círculos o elipses, mientras que las no ligadas son parábolas o hipérbolas), aunque no lo mostraremos aquí.

En la Relatividad General la situación es diferente, pero sólo para $r$ suficientemente pequeño.
Dado que la diferencia reside en el término $-GML^2/r^3$, al igual que $r\rightarrow\infty$ los comportamientos son idénticos en las dos teorías.
Pero como $r\rightarrow 0$ el potencial va a $-\infty$ en lugar de a $+\infty$ como en el caso newtoniano.
En $r=2GM$ el potencial es siempre cero; dentro de este radio está el agujero negro, del que hablaremos más a fondo más adelante.
Para las partículas sin masa siempre hay una barrera (excepto para $L=0$, cuyo potencial desaparece de manera idéntica), pero un fotón suficientemente energético atravesará la barrera y será arrastrado inexorablemente hacia el centro.
(Tenga en cuenta que ``suficientemente energético'' significa ``en comparación con su momento angular''; de hecho, la frecuencia del fotón es irrelevante, sólo la dirección en la que apunta).
En la parte superior de la barrera hay órbitas circulares inestables.
Para $\epsilon=0$, $\gamma=1$, podemos resolver fácilmente (7.49) para obtener
\begin{equation}
r_c = 3GM\,.\label{7.51}
\end{equation}
Esto lo confirma la figura, que muestra un máximo de $V(r)$ en $r=3GM$ por cada $L$.
Esto significa que un fotón puede orbitar para siempre en un círculo con este radio, pero cualquier perturbación hará que se aleje hacia $r=0$ o $r=\infty$.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven06.pdf}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven07.pdf}
\end{figure}

Para las partículas masivas también existen diferentes regímenes dependiendo del momento angular.
Las órbitas circulares están en
\begin{equation}
r_c = {{L^2\pm \sqrt{L^4 - 12 G^2 M^2 L^2}}\over{2GM}}\,.\label{7.52}
\end{equation}
Para $L$ grande habrá dos órbitas circulares, una estable y otra inestable.
En el límite $L\rightarrow\infty$ sus radios están dados por
\begin{equation}
r_c = {{L^2\pm L^2(1 - 6 G^2 M^2/L^2)}\over{2GM}} =
\left({{L^2}\over{GM}} ,\ 3GM\right)\,.\label{7.53}
\end{equation}
En este límite, la órbita circular estable se aleja cada vez más, mientras que la inestable se acerca a $3GM$, comportamiento que es paralelo al caso sin masa.
A medida que disminuimos $L$ las dos órbitas circulares se acercan; coinciden cuando el discriminante en (7.52) desaparece, en
\begin{equation}
L = \sqrt{12}GM\ ,\label{7.54}
\end{equation}
para cual
\begin{equation}
r_c = 6GM\ ,\label{7.55}
\end{equation}
y desaparecerá por completo para $L$ más pequeños.
Por tanto, $6GM$ es el radio más pequeño posible de una órbita circular estable en la métrica de Schwarzschild.
También hay órbitas libres, que vienen del infinito y giran, y órbitas limitadas pero no circulares, que oscilan alrededor del radio circular estable.
Tenga en cuenta que tales órbitas, que describirían secciones cónicas exactas en la gravedad newtoniana, no lo harán en Relatividad General, aunque tendríamos que resolver la ecuación para $d\phi/dt$ para demostrarlo.
Finalmente, hay órbitas que vienen desde el infinito y continúan hasta $r=0$; Esto puede suceder si la energía es más alta que la barrera, o para $L<\sqrt{12}GM$, cuando la barrera desaparece por completo.

Por lo tanto, hemos descubierto que la solución de Schwarzschild posee órbitas circulares estables para $r>6GM$ y órbitas circulares inestables para $3GM < r < 6GM$.
Es importante recordar que estas son sólo las geodésicas; no hay nada que impida que una partícula en aceleración caiga por debajo de $r=3GM$ y emerja, siempre y cuando permanezca más allá de $r=2GM$.

La mayoría de las pruebas experimentales de la Relatividad General implican el movimiento de partículas de prueba en el sistema solar, y por tanto geodésicas de la métrica de Schwarzschild; éste es por tanto un buen lugar para hacer una pausa y considerar estas pruebas.
Einstein sugirió tres pruebas: la desviación de la luz, la precesión del perihelio y el corrimiento al rojo gravitacional.
La desviación de la luz es observable en el límite del campo débil y, por tanto, no es realmente una buena prueba de la forma exacta de la geometría de Schwarzschild.
Se han realizado observaciones de esta desviación durante eclipses de Sol, con resultados que concuerdan con la predicción de la Relatividad General (aunque no es un experimento especialmente limpio).
La precesión del perihelio refleja el hecho de que las órbitas no circulares no son elipses cerradas; en buena aproximación son elipses que precesan, describiendo un patrón floral.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/seven08.pdf}
\end{figure}

Usando nuestras ecuaciones geodésicas, podríamos resolver $d\phi/d\lambda$ como una serie de potencias en la excentricidad $e$ de la órbita, y a partir de ahí obtener la frecuencia absidal $\omega_a$, definida como $2\pi$ dividida por el tiempo que tarda en elipse a precesión una vez alrededor.
Para más detalles puedes buscar en Weinberg; la respuesta es
\begin{equation}
\omega_a={{3(GM)^{3/2}}\over {c^2(1-e^2)r^{5/2}}}\ ,\label{7.56}
\end{equation}
donde hemos restaurado el $c$ para que sea más fácil de comparar con la observación.
(Es un buen ejercicio derivar esto usted mismo al orden más bajo que no desaparece, en cuyo caso falta $e^2$.)
Históricamente, la precesión de Mercurio fue la primera prueba de la Relatividad General.
Para Mercurio los números relevantes son
\begin{align}
{{GM_\odot}\over{c^2}} &=  1.48\times 10^5 {\rm ~cm}\ , \notag \\
a  &=  5.55\times 10^{12}{\rm ~cm}\ ,\label{7.57}
\end{align}
y por supuesto $c=3.00\times 10^{10}$ cm/seg.
Esto da $\omega_a = 2.35\times 10^{-14}$ seg$^{-1}$.
En otras palabras, el eje mayor de la órbita de Mercurio precede a una velocidad de $42.9$ segundos de arco cada 100 años.
El valor observado es $5601$ segundos de arco/100 años.
Sin embargo, mucho de eso se debe a la precesión de los equinoccios en nuestro sistema de coordenadas geocéntricas; $5025$ segundos de arco/100 años, para ser precisos.
Las perturbaciones gravitacionales de los otros planetas contribuyen con $532$ segundos de arco/100 años adicionales, lo que deja $43$ segundos de arco/100 años para ser explicados por la Relatividad General, lo cual lo hace bastante bien.

El corrimiento al rojo gravitacional, como hemos visto, es otro efecto que está presente en el límite del campo débil y, de hecho, será predicho por cualquier teoría de la gravedad que obedezca el Principio de Equivalencia.
Sin embargo, esto sólo se aplica a regiones del Espacio-Tiempo suficientemente pequeñas; en distancias mayores, la cantidad exacta de desplazamiento al rojo dependerá de la métrica y, por tanto, de la teoría en cuestión.
Por tanto, vale la pena calcular el corrimiento al rojo en la geometría de Schwarzschild.
Consideramos dos observadores que no se mueven en geodésicas, pero que están atrapados en valores de coordenadas espaciales fijos $(r_1,\theta_1,\phi_1)$ y $(r_2,\theta_2,\phi_2)$.
Según (7.45), el tiempo propio del observador $i$ estará relacionado con el tiempo de coordenadas $t$ por
\begin{equation}
{{d\tau_i}\over{dt}} = \left(1-{{2GM}\over{r_i}}\right)^{1/2}\,.
\label{7.58}
\end{equation}
Supongamos que el observador ${\cal O}_1$ emite un pulso de luz que viaja hasta el observador ${\cal O}_2$, de modo que ${\cal O}_1$ mide el tiempo entre dos crestas sucesivas de la onda de luz como $\Delta\tau_1$.
Cada cresta sigue el mismo camino hasta ${\cal O}_2$, excepto que están separadas por una coordenada de tiempo.
\begin{equation}
\Delta t= \left(1-{{2GM}\over{r_1}}\right)^{-1/2}\Delta \tau_1\,.
\label{7.59}
\end{equation}
Esta separación en el tiempo coordinado no cambia a lo largo de las trayectorias de los fotones, pero el segundo observador mide un tiempo entre crestas sucesivas dado por
\begin{align}
\Delta\tau_2  &=  \left(1-{{2GM}\over{r_2}}\right)^{1/2}
\Delta t \notag \\
&=  \left({{1-{{2GM}/{r_2}}}\over{1-{{2GM}/{r_1}}}}\right)^{1/2}
\Delta\tau_1\,.\label{7.60}
\end{align}
Dado que estos intervalos $\Delta\tau_i$ miden el tiempo adecuado entre dos crestas de una onda electromagnética, las frecuencias observadas estarán relacionadas por
\begin{align}
{{\omega_2}\over{\omega_1}} &=  {{\Delta\tau_1}\over
{\Delta\tau_2}} \notag \\
&=  \left({{1-{{2GM}/{r_1}}}\over{1-{{2GM}/{r_2}}}}\right)^{1/2}
\,.\label{7.61}
\end{align}
Este es un resultado exacto para el cambio de frecuencia; en el limite $r>>2GM$ tenemos
\begin{align}
{{\omega_2}\over{\omega_1}} &=  1-{{GM}\over{r_1}}+
{{GM}\over{r_2}} \notag \\
&=  1+\Phi_1-\Phi_2\,.\label{7.62}
\end{align}
Esto nos dice que la frecuencia disminuye a medida que aumenta $\Phi$, lo que sucede cuando salimos de un campo gravitacional; por tanto, un corrimiento al rojo.
Puedes comprobar que concuerda con nuestro cálculo anterior basado en el principio de equivalencia.

Desde la propuesta de Einstein de las tres pruebas clásicas, se han propuesto más pruebas de la Relatividad General.
El más famoso es, por supuesto, el púlsar binario, analizado en la sección anterior.
Otro es el retraso gravitacional, descubierto (y observado por) Shapiro.
Este es simplemente el hecho de que el tiempo transcurrido a lo largo de dos trayectorias diferentes entre dos eventos no tiene por qué ser el mismo.
Se ha medido reflejando señales de radar de Venus y Marte, y una vez más es consistente con la predicción de la Relatividad General.
Un efecto que aún no se ha observado es el efecto Lense-Thirring, o efecto de arrastre de fotogramas.
Ha habido un esfuerzo a largo plazo dedicado a una propuesta de satélite, denominada Gravity Probe B, que implicaría giroscopios extraordinariamente precisos cuya precesión podría medirse y determinarse la contribución de la Relatividad General.
Sin embargo, aún queda mucho camino por recorrer antes de su lanzamiento, y la supervivencia de este tipo de proyectos siempre es de año tras año.

Ahora sabemos algo sobre el comportamiento de las geodésicas fuera del radio problemático $r=2GM$, que es el régimen de interés para el sistema solar y la mayoría de otras situaciones astrofísicas.
A continuación pasaremos al estudio de los objetos descritos por la solución de Schwarzschild incluso en radios menores que $2GM$: agujeros negros.
(Utilizaremos el término ``agujero negro'' por el momento, aunque no hemos introducido un significado preciso para tal objeto.)

Una forma de entender una geometría es explorar su estructura causal, definida por los conos de luz.
Consideramos por tanto curvas nulas radiales, aquellas para las que $\theta$ y $\phi$ son constantes y $ds^2=0$:
\begin{equation}
ds^2 = 0 = -\left(1-{{2GM}\over r}\right) d\,t^2
+\left(1-{{2GM}\over r}\right)^{-1} d\,r^2 \ ,\label{7.63}
\end{equation}
de donde vemos que
\begin{equation}
{{dt}\over{dr}}=\pm \left(1-{{2GM}\over r}\right)^{-1}\,.\label{7.64}
\end{equation}
Por supuesto, esto mide la pendiente de los conos de luz en un diagrama espacio-temporal del plano $t$-$r$.
Para $r$ grande la pendiente es $\pm 1$, como sería en un espacio plano, mientras que a medida que nos acercamos a $r=2GM$ obtenemos $dt/dr\rightarrow \pm\infty$, y los conos de luz ``se acercan'':

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven09.pdf}
\end{figure}

\noindent
Así, un rayo de luz que se aproxima a $r=2GM$ nunca parece llegar allí, al menos en este sistema de coordenadas; en cambio, parece una asíntota con respecto a este radio.

Como veremos, esto es una ilusión y el rayo de luz (o una partícula masiva) en realidad no tiene problemas para alcanzar $r=2GM$.
Pero un observador lejano nunca podría saberlo.
Si nos quedáramos afuera mientras un intrépido relativista general observacional se sumergiera en el agujero negro, enviando señales todo el tiempo, simplemente veríamos que las señales nos llegan cada vez más lentamente.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven10.pdf}
\end{figure}
Esto debería quedar claro en las imágenes y lo confirma nuestro cálculo de $\Delta\tau_1/\Delta\tau_2$ cuando analizamos el desplazamiento al rojo gravitacional (7,61).
A medida que los astronautas que caen se acercan a $r=2GM$, cualquier intervalo fijo $\Delta\tau_1$ de su tiempo adecuado corresponde a un intervalo cada vez más largo $\Delta\tau_2$ desde nuestro punto de vista.
Esto continúa para siempre; nunca veríamos al astronauta cruzar $r=2GM$, solo los veríamos moverse cada vez más lentamente (y ponerse cada vez más rojos, casi como si les avergonzara haber hecho algo tan estúpido como sumergirse en un agujero negro).

El hecho de que nunca veamos a los astronautas llegar a $r=2GM$ es una afirmación significativa, pero el hecho de que su trayectoria en el avión $t$-$r$ nunca llegue allí, no lo es.
Depende en gran medida de nuestro sistema de coordenadas, y nos gustaría hacer una pregunta más independiente de las coordenadas (por ejemplo, ¿alcanzan los astronautas este radio en una cantidad finita de su tiempo adecuado?).
La mejor manera de hacerlo es cambiar las coordenadas a un sistema que se comporte mejor en $r=2GM$.
Existe un conjunto de tales coordenadas, que ahora nos proponemos encontrar.
No hay forma de ``derivar'' una transformación de coordenadas, por supuesto, simplemente decimos cuáles son las nuevas coordenadas y agregamos las fórmulas.
Pero desarrollaremos estas coordenadas en varios pasos, con la esperanza de que las elecciones parezcan algo motivadas.

El problema con nuestras coordenadas actuales es que $dt/dr\rightarrow \infty$ a lo largo de geodésicas radiales nulas que se acercan a $r=2GM$; el progreso en la dirección $r$ se vuelve cada vez más lento con respecto al tiempo de coordenadas $t$.
Podemos intentar solucionar este problema reemplazando $t$ con una coordenada que ``se mueve más lentamente'' a lo largo de geodésicas nulas.
Primero observe que podemos resolver explícitamente la condición (7.64) que caracteriza las curvas radiales nulas para obtener
\begin{equation}
t = \pm r^* +{\rm ~constant}\ ,\label{7.65}
\end{equation}
donde la {\bf coordenada de la tortuga} $r^*$ está definida por
\begin{equation}
r^* = r+2GM \ln\left({{r}\over{2GM}}-1\right)\,.\label{7.66}
\end{equation}
(La coordenada de la tortuga solo está sensiblemente relacionada con $r$ cuando $r\geq 2GM$, pero más allá de allí nuestras coordenadas no son muy buenas de todos modos).
En términos de la coordenada de la tortuga, la métrica de Schwarzschild se convierte en
\begin{equation}
ds^2 = \left(1-{{2GM}\over r}\right)\left(- d\,t^2
+ d\,{r^*}^2 \right) + r^2 d\Omega^2\ ,\label{7.67}
\end{equation}
donde $r$ se considera una función de $r^*$.
Esto representa un cierto progreso, ya que los conos de luz ahora no parecen cerrarse; además, ninguno de los coeficientes métricos se vuelve infinito en $r=2GM$ (aunque tanto $g_{tt}$ como $g_{r^* r^*}$ se vuelven cero).
Sin embargo, el precio que pagamos es que la superficie de interés en $r=2GM$ acaba de ampliarse hasta el infinito.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven11.pdf}
\end{figure}

Nuestro próximo paso es definir coordenadas que se adapten naturalmente a las geodésicas nulas.
si dejamos
\begin{align}
\tilde u  &=  t+r^* \notag \\  \tilde v  &=  t-r^*\ ,\label{7.68}
\end{align}
luego, las geodésicas radiales nulas entrantes se caracterizan por la constante $\tilde u = $, mientras que las salientes satisfacen la constante $\tilde v = $.
Ahora considere volver a la coordenada radial original $r$, pero reemplazando la coordenada temporal $t$ con la nueva coordenada $\tilde u$.
Éstas se conocen como {\bf coordenadas de Eddington-Finkelstein}.
En términos de ellos, la métrica es
\begin{equation}
ds^2 = -\left(1-{{2GM}\over r}\right) d\,{\tilde u}^2 +
( d\,\tilde u  d\,r +  d\,r d\,\tilde u) + r^2 d\Omega^2\,.\label{7.69}
\end{equation}
Aquí vemos nuestra primera señal de progreso real.
Aunque el coeficiente métrico $g_{\tilde u \tilde u}$ desaparece en $r=2GM$, no hay una degeneración real; el determinante de la métrica es
\begin{equation}
g = -r^4 \sin^2\theta\ ,\label{7.70}
\end{equation}
lo cual es perfectamente regular en $r=2GM$.
Por lo tanto, la métrica es invertible y vemos de una vez por todas que $r=2GM$ es simplemente una singularidad de coordenadas en nuestro sistema $(t,r,\theta,\phi)$ original.
En las coordenadas de Eddington-Finkelstein, la condición para curvas nulas radiales se resuelve mediante
\begin{equation}
{{d\tilde u}\over {dr}} =
\begin{cases}
0\ ,& {\rm (entrantes)} \\
2\left(1-{{2GM}\over r}\right)^{-1}\,.& {\rm (salientes)}
\end{cases}
\label{7.71}
\end{equation}
Por tanto, podemos ver lo que ha sucedido: en este sistema de coordenadas los conos de luz se comportan bien en $r=2GM$, y esta superficie tiene un valor de coordenadas finito.
No hay ningún problema en rastrear las trayectorias de partículas nulas o temporales más allá de la superficie.
Por otro lado, ciertamente está sucediendo algo interesante.
Aunque los conos de luz no se cierran, sí se inclinan, de modo que para $r< 2GM$ todos los caminos dirigidos al futuro están en la dirección decreciente $r$.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven12.pdf}
\end{figure}

La superficie $r=2GM$, aunque localmente es perfectamente regular, globalmente funciona como un punto sin retorno: una vez que una partícula de prueba se sumerge debajo de ella, nunca puede regresar.
Por esta razón $r=2GM$ se conoce como {\bf horizonte de sucesos}; ningún evento en $r\leq 2GM$ puede influir en ningún otro evento en $r>2GM$.
Observe que el horizonte de sucesos es una superficie nula, no temporal.
Observe también que, dado que nada puede escapar del horizonte de sucesos, nos resulta imposible ``ver el interior'', de ahí el nombre {\bf agujero negro}.

Consideremos lo que hemos hecho.
Actuando bajo la sospecha de que nuestras coordenadas pueden no haber sido buenas para toda la variedad, hemos cambiado de nuestra coordenada original $t$ a la nueva $\tilde u$, que tiene la agradable propiedad de que si disminuimos $r$ a lo largo de un radial curva curva nula $\tilde u =$ constante, atravesamos el horizonte de eventos sin ningún problema.
(De hecho, un observador local que realmente hiciera el viaje no sabría necesariamente cuándo se había cruzado el horizonte de sucesos; la geometría local no es diferente a la de cualquier otro lugar).
Por lo tanto, concluimos que nuestra sospecha era correcta y que nuestro sistema de coordenadas inicial no hizo un buen trabajo al cubrir toda la variedad.
La región $r\leq 2GM$ ciertamente debería incluirse en nuestro Espacio-Tiempo, ya que las partículas físicas pueden llegar allí y atravesarla fácilmente.
Sin embargo, no hay garantía de que hayamos terminado; tal vez haya otras direcciones en las que podamos ampliar nuestra variedad.

De hecho los hay.
Observe que en el sistema de coordenadas $(\tilde u, r)$ podemos cruzar el horizonte de eventos en caminos dirigidos al futuro, pero no en caminos dirigidos al pasado.
Esto no parece razonable, ya que partimos de una solución independiente del tiempo.
Pero podríamos haber elegido $\tilde v$ en lugar de $\tilde u$, en cuyo caso la métrica habría sido
\begin{equation}
ds^2 = -\left(1-{{2GM}\over r}\right) d\,{\tilde v}^2
-( d\,\tilde v d\,r +  d\,r d\,\tilde v) + r^2 d\Omega^2\,.\label{7.72}
\end{equation}
Ahora podemos volver a atravesar el horizonte de sucesos, pero esta vez sólo a lo largo de curvas dirigidas al pasado.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven13.pdf}
\end{figure}

Esto quizás sea una sorpresa: podemos seguir consistentemente curvas dirigidas al futuro o al pasado a través de $r=2GM$, pero llegamos a lugares diferentes.
En realidad era de esperarse, ya que de las definiciones (7.68), si mantenemos $\tilde u$ constante y disminuimos $r$ debemos tener $t\rightarrow +\infty$, mientras que si mantenemos $\tilde v$ constante y disminuimos $r$ debemos tener $t\rightarrow -\infty$.
(La coordenada de la tortuga $r^*$ va a $-\infty$ como $r\rightarrow 2GM$.)
Así que hemos extendido el Espacio-Tiempo en dos direcciones diferentes, una hacia el futuro y otra hacia el pasado.

El siguiente paso sería seguir geodésicas de tipo espacial para ver si podemos descubrir aún más regiones.
La respuesta es sí, alcanzaríamos otra porción del Espacio-Tiempo, pero acortaremos el proceso definiendo coordenadas que sean buenas en todas partes.
Una primera suposición podría ser utilizar $\tilde u$ y $\tilde v$ a la vez (en lugar de $t$ y $r$), lo que lleva a
\begin{equation}
ds^2 = {1\over 2}\left(1-{{2GM}\over r}\right)( d\,{\tilde u}
 d\,{\tilde v}+ d\,{\tilde v} d\,{\tilde u}) +r^2 d\Omega^2\ ,\label{7.73}
\end{equation}
con $r$ definido implícitamente en términos de $\tilde u$ y $\tilde v$ por
\begin{equation}
{1\over 2}(\tilde u-\tilde v)=
r+2GM \ln\left({{r}\over{2GM}}-1\right)\,.\label{7.74}
\end{equation}
De hecho, hemos reintroducido la degeneración con la que empezamos; en estas coordenadas $r=2GM$ está ``infinitamente lejos'' (ya sea en $\tilde u=-\infty$ o $\tilde v=+\infty$).
Lo que hay que hacer es cambiar a coordenadas que conviertan estos puntos en valores de coordenadas finitos; una buena elección es
\begin{align}
u'  &=  e^{\tilde u/4GM}  \notag \\  v'  &=  e^{-\tilde v/4GM}\ ,
\label{7.75}
\end{align}
que en términos de nuestro sistema original $(t,r)$ es
\begin{align}
u'  &=  \left({{r}\over{2GM}}-1\right)^{1/2}e^{(r+t)/4GM}  \notag \\
v'  &=  \left({{r}\over{2GM}}-1\right)^{1/2}e^{(r-t)/4GM}\,.
\label{7.76}
\end{align}
En el sistema $(u',v',\theta,\phi)$ la métrica de Schwarzschild es
\begin{equation}
ds^2 =-{{16 G^3M^3}\over{r}}e^{-r/2GM}( d\,u'  d\,v'+  d\,v'  d\,u')
+r^2 d\Omega^2\,.\label{7.77}
\end{equation}
Finalmente, la naturaleza no singular de $r=2GM$ se vuelve completamente manifiesta; de esta forma ninguno de los coeficientes métricos se comporta de manera especial en el horizonte de sucesos.

Tanto $u'$ como $v'$ son coordenadas nulas, en el sentido de que sus derivadas parciales $\partial/\partial u'$ y $\partial/\partial v'$ son vectores nulos.
No hay nada de malo en esto, ya que la colección de cuatro vectores de derivadas parciales (dos nulos y dos espaciales) en este sistema sirve como una base perfectamente buena para el espacio tangente.
Sin embargo, nos sentimos algo más cómodos trabajando en un sistema donde una coordenada es temporal y el resto espacial.
Por lo tanto definimos
\begin{align}
u  &=  {1\over 2}(u'-v') \notag \\
&=  \left({{r}\over{2GM}}-1\right)^{1/2}e^{r/4GM}\cosh(t/4GM)
\label{7.78}
\end{align}
y
\begin{align}
v  &=  {1\over 2}(u'+v') \notag \\
&=  \left({{r}\over{2GM}}-1\right)^{1/2}e^{r/4GM}\sinh(t/4GM)\ ,
\label{7.79}
\end{align}
en términos de los cuales la métrica se convierte
\begin{equation}
ds^2 ={{32 G^3M^3}\over{r}}e^{-r/2GM}(- d\,v^2+ d\,u^2)
+r^2 d\Omega^2\ ,\label{7.80}
\end{equation}
donde $r$ se define implícitamente desde
\begin{equation}
(u^2-v^2)=
\left({{r}\over{2GM}}-1\right)e^{r/2GM}\,.\label{7.81}
\end{equation}
Las coordenadas $(v,u,\theta,\phi)$ se conocen como {\bf coordenadas Kruskal} o, a veces, coordenadas Kruskal-Szekres.
Tenga en cuenta que $v$ es la coordenada temporal.

Las coordenadas de Kruskal tienen varias propiedades milagrosas.
Al igual que las coordenadas $(t,r^*)$, las curvas nulas radiales se ven como en un espacio plano:
\begin{equation}
v= \pm u + {\rm constant}\,.\label{7.82}
\end{equation}
Sin embargo, a diferencia de las coordenadas $(t,r^*)$, el horizonte de sucesos $r=2GM$ no está infinitamente lejos; de hecho está definido por
\begin{equation}
v = \pm u\ ,\label{7.83}
\end{equation}
consistente con que sea una superficie nula.
De manera más general, podemos considerar las superficies $r=$ constantes.
De (7.81) estos satisfacen
\begin{equation}
u^2-v^2 = {\rm ~constant}\,.\label{7.84}
\end{equation}
Por tanto, aparecen como hipérbolas en el plano $u$-$v$.
Además, las superficies de la constante $t$ están dadas por
\begin{equation}
{v\over u} = \tanh(t/4GM)\ ,\label{7.85}
\end{equation}
que define líneas rectas que pasan por el origen con pendiente $\tanh(t/4GM)$.
Tenga en cuenta que como $t\rightarrow \pm\infty$ esto se convierte en lo mismo que (7.83); por lo tanto estas superficies son iguales que $r=2GM$.

Ahora, a nuestras coordenadas $(v,u)$ se les debe permitir abarcar todos los valores que puedan tomar sin alcanzar la singularidad real en $r=2GM$; por lo tanto, la región permitida es $-\infty \leq u \leq \infty$ y $v^2 < u^2+1$.
Ahora podemos dibujar un diagrama de Espacio-Tiempo en el plano $v$-$u$ (con $\theta$ y $\phi$ suprimidos), conocido como ``diagrama de Kruskal'', que representa todo el Espacio-Tiempo correspondiente a la métrica de Schwarzschild.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven14.pdf}
\end{figure}

\noindent
Cada punto del diagrama es una doble esfera.

Nuestras coordenadas originales $(t,r)$ solo eran válidas para $r>2GM$, que es solo una parte de la variedad representada en el diagrama de Kruskal.
Es conveniente dividir el diagrama en cuatro regiones:

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/seven15.pdf}
\end{figure}

\noindent
La región en la que iniciamos fue la región I; siguiendo rayos nulos dirigidos al futuro llegamos a la región II, y siguiendo rayos nulos dirigidos al pasado alcanzamos la región III.
Si hubiéramos explorado geodésicas espaciales, nos habrían llevado a la región IV.
Las definiciones (7.78) y (7.79) que relacionan $(u,v)$ con $(t,r)$ en realidad sólo son buenas en la región I; en las demás regiones es necesario introducir signos menos apropiados para evitar que las coordenadas se vuelvan imaginarias.

Habiendo extendido la geometría de Schwarzschild al máximo, hemos descrito un Espacio-Tiempo notable.
La Región II, por supuesto, es lo que consideramos el agujero negro.
Una vez que algo viaja de la región I a la II, nunca podrá regresar.
De hecho, cada camino dirigido al futuro en la región II termina chocando con la singularidad en $r=0$; una vez que entras en el horizonte de sucesos, estás completamente condenado.
Vale la pena recalcar esto; no sólo no puedes escapar de regreso a la región I, sino que ni siquiera puedes evitar moverte en la dirección decreciente $r$, ya que esta es simplemente la dirección temporal.
(Esto podría haberse visto en nuestro sistema de coordenadas original; para $r<2GM$, $t$ se vuelve espacial y $r$ se vuelve temporal).
Por lo tanto, no puedes dejar de avanzar hacia la singularidad, como tampoco puedes dejar de envejecer.
Dado que el tiempo adecuado se maximiza a lo largo de una geodésica, vivirás más tiempo si no luchas, sino que simplemente te relajas a medida que te acercas a la singularidad.
No es que tengas mucho tiempo para relajarte.
% (
Tampoco que el viaje vaya a ser muy relajante; A medida que te acercas a la singularidad, las fuerzas de marea se vuelven infinitas.
A medida que caes hacia la singularidad, tus pies y tu cabeza se separarán entre sí, mientras que tu torso quedará reducido a una delgadez infinitesimal.
La espantosa desaparición de un astrofísico que cae en un agujero negro se detalla en Misner, Thorne y Wheeler, sección 32.6.
Tenga en cuenta que utilizan marcos ortonormales [aunque eso no hace que el viaje sea más agradable].
% )

Las Regiones III y IV podrían resultar algo inesperadas.
La región III no es más que el reverso temporal de la región II, una parte del espaciotiempo desde la que las cosas pueden escapar hacia nosotros, mientras que nosotros nunca podemos llegar hasta allí.
Se puede considerar como un ``agujero blanco''. Hay una singularidad en el pasado, de la cual el universo parece surgir.
El límite de la región III a veces se denomina horizonte de sucesos pasado, mientras que el límite de la región II se denomina horizonte de sucesos futuro.
La Región IV, por su parte, no puede ser alcanzada desde nuestra Región I ni hacia adelante ni hacia atrás en el tiempo (ni nadie de allí puede alcanzarnos).
Es otra región asintóticamente plana del Espacio-Tiempo, una imagen especular de la nuestra.
Se puede considerar que está conectado a la región I mediante un ``agujero de gusano'', una configuración en forma de cuello que une dos regiones distintas.
Considere dividir el diagrama de Kruskal en superficies espaciales de constante $v$:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven16.pdf}
\end{figure}

\noindent
Ahora podemos hacer dibujos de cada corte, restaurando una de las coordenadas angulares para mayor claridad:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven17.pdf}
\end{figure}

\noindent
Entonces, la geometría de Schwarzschild realmente describe dos regiones asintóticamente planas que se acercan, se unen a través de un agujero de gusano por un tiempo y luego se desconectan.
Pero el agujero de gusano se cierra demasiado rápido para que cualquier observador temporal pueda cruzarlo de una región a la siguiente.

Puede parecer algo inverosímil esta historia sobre dos espacio-tiempos separados que se acercan durante un tiempo y luego se sueltan.
De hecho, no se espera que esto suceda en el mundo real, ya que la métrica de Schwarzschild no modela con precisión todo el universo.
Recuerda que sólo es válido en el vacío, por ejemplo fuera de una estrella.
Si la estrella tiene un radio mayor que $2GM$, nunca debemos preocuparnos por ningún horizonte de sucesos.
Pero creemos que hay estrellas que colapsan bajo su propia atracción gravitacional, reduciéndose hasta debajo de $r=2GM$ y adquiriendo una singularidad, lo que da como resultado un agujero negro.
Sin embargo, no hay necesidad de un agujero blanco, porque el pasado de tal Espacio-Tiempo no se parece en nada al de la solución completa de Schwarzschild.
Aproximadamente, un diagrama tipo Kruskal para el colapso estelar se vería así:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven18.pdf}
\end{figure}

\noindent
Schwarzschild no describe la región sombreada, por lo que no hay necesidad de preocuparse por los agujeros blancos y los agujeros de gusano.

Ya que estamos en el tema, podemos decir algo sobre la formación de agujeros negros astrofísicos a partir de estrellas masivas.
La vida de una estrella es una lucha constante entre la atracción de la gravedad hacia adentro y la presión hacia afuera.
Cuando la estrella quema combustible nuclear en su núcleo, la presión proviene del calor producido por esta quema.
(Deberíamos poner ``quemar'' entre comillas, ya que la fusión nuclear no está relacionada con la oxidación).
Cuando se agota el combustible, la temperatura disminuye y la estrella comienza a encogerse a medida que la gravedad comienza a ganar la lucha.
Finalmente, este proceso se detiene cuando los electrones se acercan tanto que resisten una mayor compresión simplemente sobre la base del principio de exclusión de Pauli (no pueden haber dos fermiones en el mismo estado).
El objeto resultante se llama {\bf enana blanca}.
Sin embargo, si la masa es lo suficientemente alta, ni siquiera la presión de degeneración de los electrones es suficiente y los electrones se combinarán con los protones en una dramática transición de fase.
El resultado es una {\bf estrella de neutrones}, que se compone casi en su totalidad de neutrones (aunque el interior de las estrellas de neutrones no se comprende muy bien).
Dado que las condiciones en el centro de una estrella de neutrones son muy diferentes a las de la Tierra, no tenemos una comprensión perfecta de la ecuación de estado.
Sin embargo, creemos que una estrella de neutrones suficientemente masiva será incapaz de resistir la atracción de la gravedad y seguirá colapsando.
Dado que un fluido de neutrones es el material más denso que podemos concebir actualmente, se cree que el resultado inevitable de tal colapso es un agujero negro.

El proceso se resume en el siguiente diagrama de radio vs.
masa:

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven19.pdf}
\end{figure}

\noindent
El objetivo del diagrama es que, para cualquier masa dada $M$, el radio de la estrella disminuirá hasta que llegue a la línea.
Las enanas blancas se encuentran entre los puntos $A$ y $B$, y las estrellas de neutrones entre los puntos $C$ y $D$.
El punto $B$ se encuentra a una altura de algo menos de 1,4 masas solares; la altura de $D$ es menos segura, pero probablemente menos de 2 masas solares.
El proceso de colapso es complicado y durante la evolución la estrella puede perder o ganar masa, por lo que es difícil predecir el punto final de una estrella determinada.
Sin embargo, las enanas blancas están por todas partes, las estrellas de neutrones no son infrecuentes y hay una serie de sistemas que se cree firmemente que contienen agujeros negros.
(Por supuesto, no se puede ver directamente el agujero negro.
Lo que se puede ver es radiación de materia que se acumula en el agujero, que se calienta a medida que se acerca y emite radiación).

Hemos visto que el sistema de coordenadas de Kruskal proporciona una representación muy útil de la geometría de Schwarzschild.
Antes de pasar a otros tipos de agujeros negros, introduciremos una forma más de pensar sobre este Espacio-Tiempo, el diagrama de Penrose (o Carter-Penrose, o conforme).
La idea es hacer una transformación conforme que lleve toda la variedad a una región compacta de modo que podamos ajustar el Espacio-Tiempo en una hoja de papel.

Comencemos con el espacio de Minkowski, para ver cómo funciona la técnica.
La métrica en coordenadas polares es
\begin{equation}
ds^2 = - d\,t^2 +  d\,r^2 + r^2 d\Omega^2\,.\label{7.86}
\end{equation}
No sucederá nada inusual con las coordenadas $\theta, \phi$, pero queremos realizar un seguimiento cuidadoso de los rangos de las otras dos coordenadas.
En este caso por supuesto tenemos
\begin{align}
& -\infty < t < +\infty& \notag \\
& 0 \leq r < +\infty\,.&
\label{7.87}
\end{align}
Técnicamente, la línea mundial $r=0$ representa una singularidad de coordenadas y debería estar cubierta por un parche diferente, pero todos sabemos lo que está pasando, así que actuaremos como si $r=0$ se comportara bien.

Nuestra tarea se hace algo más fácil si cambiamos a coordenadas nulas:
\begin{align}
u  &=  {1\over 2}(t+r) \notag \\  v  &=  {1\over 2}(t-r)\ ,
\label{7.88}
\end{align}
con rangos correspondientes dados por
\begin{align}
&-\infty < u < +\infty & \notag \\  &-\infty < v < +\infty & \notag \\
& v \leq u\,.&\label{7.89}
\end{align}
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven20.pdf}
\end{figure}
Estos rangos son como se muestra en la figura, en la que cada punto representa dos esferas de radio $r=u-v$.
La métrica en estas coordenadas está dada por
\begin{equation}
ds^2 = -2( d\,u d\,v +  d\,v d\,u) +(u-v)^2 d\Omega^2\,.\label{7.90}
\end{equation}

Ahora queremos cambiar a coordenadas en las que ``infinito'' tome un valor de coordenada finito.
Una buena elección es
\begin{align}
U  &=  \arctan u \notag \\  V  &=  \arctan v\,.\label{7.91}
\end{align}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven21.pdf}
\end{figure}

\noindent
Los rangos son ahora
\begin{align}
&-\pi/2 < U < +\pi/2& \notag \\  &-\pi/2 < V < +\pi/2& \notag \\
& V \leq U\,.& \label{7.92}
\end{align}
Para obtener la métrica, use
\begin{equation}
 d\,U = {{ d\,u}\over{1+u^2}}\ ,%\qquad  d\,V = {{ d\,v}\over{1+v^2}}\ ,
\label{7.93}
\end{equation}
y
\begin{equation}
\cos(\arctan{u}) = {{1}\over{\sqrt{1+u^2}}}\ ,\label{7.94}
\end{equation}
y lo mismo para $v$.
somos llevados a
\begin{equation}
 d\,u d\,v +  d\,v d\,u = {{1}\over{\cos^2U \cos^2V}}
( d\,U d\,V +  d\,V d\,U)\,.\label{7.95}
\end{equation}
Mientras tanto,
\begin{align}
(u-v)^2  &=  (\tan U - \tan V)^2 \notag \\
&=  {{1}\over{\cos^2U \cos^2V}}(\sin U\cos V- \cos U \sin V)^2 \notag \\
&=  {{1}\over{\cos^2U \cos^2V}}\sin^2(U-V)\,.\label{7.96}
\end{align}
Por lo tanto, la métrica de Minkowski en estas coordenadas es
\begin{equation}
ds^2 = {{1}\over{\cos^2U \cos^2V}}\left[ -2( d\,U d\,V +  d\,V d\,U)
+\sin^2(U-V)d\Omega^2\right]\,.\label{7.97}
\end{equation}

Esto tiene cierto atractivo, ya que la métrica aparece como una expresión bastante simple multiplicada por un factor general.
Podemos hacerlo aún mejor transformándonos nuevamente a una coordenada temporal $\eta$ y una coordenada espacial (radial) $\chi$, a través de
\begin{align}
\eta  &=  U+V \notag \\  \chi  &=  U-V\ ,\label{7.98}
\end{align}
con rangos
\begin{align}
& -\pi< \eta < +\pi &  \notag \\  & 0 \leq \chi < +\pi\,. &
\label{7.99}
\end{align}
Ahora la métrica es
\begin{equation}
ds^2 = \omega^{-2}\left(- d\,\eta^2 +  d\,\chi^2 +\sin^2\chi\
d\Omega^2\right)\ ,\label{7.100}
\end{equation}
dónde
\begin{align}
\omega  &=  \cos U \cos V \notag \\   &=  {1\over 2}
(\cos\eta +\cos\chi)\,.\label{7.101}
\end{align}

Por lo tanto, se puede considerar que la métrica de Minkowski está relacionada mediante una transformación conforme con la métrica ``no física''.
\begin{align}
d\bar{s}^2  &=  \omega^2 ds^2 \notag \\
&=  - d\,\eta^2 +  d\,\chi^2 +\sin^2\chi\ d\Omega^2\,.\label{7.102}
\end{align}
Esto describe la variedad $\R\times S^3$, donde las 3 esferas son máximamente simétricas y estáticas.
Hay curvatura en esta métrica y no es una solución a las ecuaciones del vacío de Einstein.
Esto no debería molestarnos, ya que no es físico; la verdadera métrica física, obtenida mediante una transformación conforme, es simplemente el espaciotiempo plano.
De hecho, esta métrica es la del ``universo estático de Einstein'', una solución estática (pero inestable) a las ecuaciones de Einstein con un fluido perfecto y una constante cosmológica.
Por supuesto, el rango completo de coordenadas en $\R\times S^3$ normalmente sería $-\infty < \eta < +\infty$, $0\leq\chi \leq\pi$, mientras que el espacio de Minkowski se asigna al subespacio definido por (7.99).
Todo el $\R\times S^3$ se puede dibujar como un cilindro, en el que cada círculo es una triple esfera, como se muestra en la página siguiente.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/seven22.pdf}
\end{figure}

\noindent
La región sombreada representa el espacio de Minkowski.
Tenga en cuenta que cada punto $(\eta,\chi)$ de este cilindro es la mitad de una doble esfera, donde la otra mitad es el punto $(\eta,-\chi)$.
Podemos desenrollar la región sombreada para representar el espacio de Minkowski como un triángulo, como se muestra en la figura.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven23.pdf}
\end{figure}
El es el {\bf diagrama de Penrose}.
Cada punto representa dos esferas.

De hecho, el espacio de Minkowski es sólo el {\it interior} del diagrama anterior (incluido $\chi=0$); los límites no son parte del Espacio-Tiempo original.
Juntos se les conoce como {\bf infinito conforme}.
La estructura del diagrama de Penrose nos permite subdividir el infinito conforme en algunas regiones diferentes:
\begin{equation*}
\begin{aligned}
i^+  &=  {\rm future~timelike~infinity~} (\eta=\pi\ ,\ \chi=0) \\
i^0  &=  {\rm spatial~infinity~} (\eta=0\ ,\ \chi=\pi) \\
i^-  &=  {\rm past~timelike~infinity~} (\eta=-\pi\ ,\ \chi=0) \\
{\cal I}^+  &=  {\rm future~null~infinity~} (\eta=\pi-\chi\ ,\ 0<\chi<\pi) \\
{\cal I}^-  &=  {\rm past~null~infinity~} (\eta=-\pi+\chi\ ,\ 0<\chi<\pi)
\end{aligned}
\end{equation*}
(${\cal I}^+$ y ${\cal I}^-$ se pronuncian como ``scri-plus'' y ``scri-minus'', respectivamente.)
Tenga en cuenta que $i^+$, $i^0$ y $i^-$ son en realidad {\it puntos}, ya que $\chi=0$ y $\chi=\pi$ son los polos norte y sur de $S^3$.
Mientras tanto, ${\cal I}^+$ y ${\cal I}^-$ son en realidad superficies nulas, con la topología de $\R\times S^2$.

Hay una serie de características importantes del diagrama de Penrose para el Espacio-Tiempo de Minkowski.
Los puntos $i^+$ y $i^-$ pueden considerarse como los límites de superficies espaciales cuyas normales son temporales; por el contrario, $i^0$ puede considerarse como el límite de superficies temporales cuyas normales son espaciales.
Las geodésicas radiales nulas están en $\pm 45^\circ$ en el diagrama.
Todas las geodésicas temporales comienzan en $i^-$ y terminan en $i^+$; todas las geodésicas nulas comienzan en ${\cal I}^-$ y terminan en ${\cal I}^+$; todas las geodésicas espaciales comienzan y terminan en $i^0$.
Por otro lado, puede haber curvas temporales no geodésicas que terminen en el infinito nulo (si se vuelven ``asintóticamente nulas'').

Es bueno poder encajar todo el espacio de Minkowski en una pequeña hoja de papel, pero en realidad no aprendemos mucho que no supiéramos ya.
Los diagramas de Penrose son más útiles cuando queremos representar espacio-tiempos un poco más interesantes, como los de los agujeros negros.
El uso original de los diagramas de Penrose era comparar los espacios-tiempos con el espacio de Minkowski ``en el infinito'' --- la definición rigurosa de ``asintóticamente plano'' es básicamente que un Espacio-Tiempo tiene un infinito conforme al igual que el espacio de Minkowski.
No profundizaremos en estas cuestiones, sino que abordaremos directamente el análisis del diagrama de Penrose para un agujero negro de Schwarzschild.

No analizaremos en detalle las manipulaciones necesarias, ya que son paralelas al caso de Minkowski con una complejidad algebraica adicional considerable.
Comenzaríamos con la versión nula de las coordenadas de Kruskal, en la que la métrica toma la forma
\begin{equation}
ds^2 =-{{16 G^3M^3}\over{r}}e^{-r/2GM}( d\,u'  d\,v'+  d\,v'  d\,u')
+r^2 d\Omega^2\ , \label{7.103}
\end{equation}
donde $r$ se define implícitamente mediante
\begin{equation}
u'v' = \left({{r}\over{2GM}}-1\right)e^{r/2GM}\,.
\label{7.104}
\end{equation}
Entonces, esencialmente, la misma transformación que se usó en el Espacio-Tiempo plano es suficiente para llevar el infinito a valores de coordenadas finitos:
\begin{align}
u''  &=  \arctan\left({{u'}\over{\sqrt{2GM}}}\right) \notag \\
v''  &=  \arctan\left({{v'}\over{\sqrt{2GM}}}\right)\ ,
\label{7.105}
\end{align}
con rangos
\begin{equation*}
\begin{array}{c}
-\pi/2 < u'' < +\pi/2 \notag \\  -\pi/2 < v'' < +\pi/2 \notag \\
-\pi < u'' + v'' < \pi\,.
\end{array}
\end{equation*}
La parte $(u'',v'')$ de la métrica (es decir, en coordenadas angulares constantes) ahora está relacionada conformemente con el espacio de Minkowski.
En las nuevas coordenadas, las singularidades en $r=0$ son líneas rectas que se extienden desde el infinito temporal en una región asintótica hasta el infinito temporal en la otra.
El diagrama de Penrose para la solución de Schwarzschild extendida al máximo se ve así:

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/seven24.pdf}
\end{figure}

\noindent
La única sutileza real de este diagrama es la necesidad de comprender que $i^+$ y $i^-$ son distintos de $r=0$ (hay muchos caminos temporales que no llegan a la singularidad).
Observe también que la estructura del infinito conforme es igual a la del espacio de Minkowski, consistente con la afirmación de que Schwarzschild es asintóticamente plano.
Además, el diagrama de Penrose para una estrella que colapsa y forma un agujero negro es lo que cabría esperar, como se muestra en la página siguiente.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/seven25.pdf}
\end{figure}

Una vez más, los diagramas de Penrose para estos espacio-tiempos realmente no nos dicen nada que no sepamos ya; su utilidad se hará evidente cuando consideremos los agujeros negros en términos más generales.
En principio podría haber una gran variedad de tipos de agujeros negros, dependiendo del proceso mediante el cual se formaron.
Sin embargo, sorprendentemente, este no es el caso; No importa cómo se forme un agujero negro, se estabiliza (con bastante rapidez) en un estado que se caracteriza únicamente por la masa, la carga y el momento angular.
Esta propiedad, que debe demostrarse individualmente para los distintos tipos de campos que uno podría imaginar que intervienen en la construcción del agujero, a menudo se expresa como ``los agujeros negros no tienen pelo''. Se puede demostrar, por ejemplo, que un agujero que se forma a partir de un colapso inicialmente no homogéneo, ``se sacude'' cualquier grumos emitiendo radiación gravitacional.
Este es un ejemplo de un ``teorema sin pelo''. Si estamos interesados en la forma del agujero negro después de que se ha asentado, sólo necesitamos preocuparnos de los agujeros cargados y giratorios.
En ambos casos existen soluciones exactas para la métrica, que podemos examinar de cerca.

Pero primero hagamos un breve desvío hacia el mundo de la evaporación de los agujeros negros.
Es extraño pensar en un agujero negro ``evaporándose'', pero en el mundo real los agujeros negros no son verdaderamente negros: irradian energía como si fueran un cuerpo negro de temperatura $T=\hbar/8\pi kGM$, donde $M$ es la masa del agujero y $k$ es la constante de Boltzmann.
La derivación de este efecto, conocido como {\bf radiación de Hawking}, implica el uso de la teoría cuántica de campos en el Espacio-Tiempo curvo y está fuera de nuestro alcance en este momento.
Sin embargo, la idea informal es comprensible.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven26.pdf}
\end{figure}
En la teoría cuántica de campos hay ``fluctuaciones del vacío'': la creación y aniquilación espontánea de pares partícula/antipartícula en el espacio vacío.
Estas fluctuaciones son exactamente análogas a las fluctuaciones del punto cero de un oscilador armónico simple.
Normalmente, tales fluctuaciones son imposibles de detectar, ya que su promedio da cero energía total (aunque nadie sabe por qué; ese es el problema de la constante cosmológica).
Sin embargo, en presencia de un horizonte de sucesos, ocasionalmente un miembro de una pareja virtual caerá en el agujero negro mientras su compañero escapa al infinito.
La partícula que llegue al infinito tendrá que tener energía positiva, pero la energía total se conserva; por tanto, el agujero negro tiene que perder masa.
(Si lo desea, puede pensar que la partícula que cae tiene una masa negativa).
Vemos las partículas que se escapan como radiación de Hawking.
No es un efecto muy grande, y la temperatura disminuye a medida que aumenta la masa, por lo que para agujeros negros de masa comparable a la del Sol es completamente insignificante.
Aun así, en principio el agujero negro podría perder toda su masa debido a la radiación de Hawking y reducirse a nada en el proceso.
El diagrama de Penrose relevante podría verse así:

\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{imagenes/seven27.pdf}
\end{figure}

Por otro lado, puede que no sea así.
El problema con este diagrama es que ``la información se pierde'': si dibujamos una superficie espacial hacia el pasado de la singularidad y la evolucionamos hacia el futuro, parte de ella termina chocando contra la singularidad y siendo destruida.
Como resultado, la radiación misma contiene menos información que la que originalmente estaba en el Espacio-Tiempo.
(Esto es peor que la falta de pelo en el agujero negro.
Una cosa es pensar que la información ha quedado atrapada dentro del horizonte de sucesos, pero es más preocupante pensar que ha desaparecido por completo).
Pero tal proceso viola la conservación de la información implícita tanto en la Relatividad General como en la teoría cuántica de campos, las dos teorías que llevaron a la predicción.
Esta paradoja se considera muy importante hoy en día, y hay una serie de esfuerzos para comprender cómo se puede recuperar la información de alguna manera.
Una explicación actualmente popular se basa en la teoría de cuerdas y básicamente dice que los agujeros negros tienen mucho pelo, en forma de estados virtuales fibrosos que viven cerca del horizonte de sucesos.
Espero que no se sienta decepcionado al saber que no analizaremos esto muy de cerca; pero usted debe saber cuál es el problema y que es un área de investigación activa en estos días.

Una vez eliminado esto de nuestro sistema, ahora pasamos a los agujeros negros cargados eléctricamente.
Al principio, estos parecen objetos bastante razonables, ya que ciertamente no hay nada que nos impida arrojar algo de carga neta a un agujero negro previamente descargado.
Sin embargo, en una situación astrofísica, se espera que la cantidad total de carga sea muy pequeña, especialmente si se compara con la masa (en términos de los efectos gravitacionales relativos).
Sin embargo, los agujeros negros cargados proporcionan un campo de pruebas útil para diversos experimentos mentales, por lo que merecen nuestra consideración.

En este caso todavía está presente la simetría esférica total del problema; sabemos por lo tanto que podemos escribir la métrica como
\begin{equation}
ds^2 = -e^{2\alpha(r,t)} d\,t^2 + e^{2\beta(r,t)} d\,r^2 +
r^2 d\Omega^2\,.\label{7.106}
\end{equation}
Ahora, sin embargo, ya no estamos en el vacío, ya que el agujero tendrá un campo electromagnético distinto de cero, que a su vez actúa como una fuente de energía-momento.
El tensor de energía-momento para el electromagnetismo viene dado por
\begin{equation}
T_\mn = {1\over{4\pi}}(F_{\mu\rho}F_\nu{}^\rho
-{1\over 4}g_\mn F_{\rho\sigma}F^{\rho\sigma})\ ,\label{7.107}
\end{equation}
donde $F_\mn$ es el tensor de intensidad del campo electromagnético.
Como tenemos simetría esférica, el tensor de intensidad de campo más general tendrá componentes
\begin{align}
F_{tr}  &=  f(r,t) = -F_{rt} \notag \\
F_{\theta\phi}  &=  g(r,t)\sin\theta = -F_{\phi\theta}\ ,
\label{7.108}
\end{align}
donde $f(r,t)$ y $g(r,t)$ son algunas funciones que se determinarán mediante las ecuaciones de campo y los componentes no escritos son cero.
$F_{tr}$ corresponde a un campo eléctrico radial, mientras que $F_{\theta\phi}$ corresponde a un campo magnético radial.
(Para aquellos de ustedes que se preguntan sobre $\sin\theta$, recuerden que lo que debería ser independiente de $\theta$ y $\phi$ es el componente radial del campo magnético, $B^r = \epsilon^{01\mu\nu}F_{\mu\nu}$.
Para una métrica esféricamente simétrica, $\epsilon^{\rho\sigma\mu\nu}={1\over\g}\tilde\epsilon^{\rho\sigma\mu\nu}$ es proporcional a $(\sin\theta)^{-1}$, por lo que queremos un factor de $\sin\theta$ en $F_{\theta\phi}$.)
Las ecuaciones de campo en este caso son tanto las ecuaciones de Einstein como las ecuaciones de Maxwell:
\begin{align}
g^\mn \nabla_\mu F_{\nu\sigma}  &=  0 \notag \\
\nabla_{[\mu}F_{\nu\rho]}  &=  0\,.\label{7.109}
\end{align}
Los dos conjuntos están acoplados, ya que el tensor de intensidad del campo electromagnético entra en las ecuaciones de Einstein a través del tensor de energía-momento, mientras que la métrica entra explícitamente en las ecuaciones de Maxwell.

Sin embargo, las dificultades no son insuperables y un procedimiento similar al que seguimos para el caso de la aspiradora conduce a una solución también para el caso acusado.
No describiremos los pasos explícitamente, sino que simplemente citaremos la respuesta final.
La solución se conoce como {\bf Reissner-Nordstr{\o}m métrica} y viene dada por
\begin{equation}
ds^2 = -\Delta  d\,t^2 + \Delta^{-1} d\,r^2 +
r^2d\Omega^2\ ,\label{7.110}
\end{equation}
dónde
\begin{equation}
\Delta = 1-{{2GM}\over r}+{{G(p^2+q^2)}\over{r^2}}\,.
\label{7.111}
\end{equation}
En esta expresión, $M$ se interpreta una vez más como la masa del agujero; $q$ es la carga eléctrica total y $p$ es la carga magnética total.
Nunca se han observado en la naturaleza cargas magnéticas aisladas (monopolos), pero eso no nos impide escribir la métrica que producirían si existieran.
Hay buenas razones teóricas para pensar que existen monopolos, pero son extremadamente raros.
(Por supuesto, también existe la posibilidad de que un agujero negro tenga carga magnética incluso si no hay monopolos).
De hecho, las cargas eléctricas y magnéticas entran en la métrica de la misma manera, por lo que no introducimos ninguna complicación adicional al mantener $p$ en nuestras expresiones.
Los campos electromagnéticos asociados con esta solución están dados por
\begin{align}
F_{tr}  &=  - {q\over{r^2}} \notag \\
F_{\theta\phi}  &=  p\sin\theta\,.\label{7.112}
\end{align}
Los conservadores pueden establecer $p=0$ si lo desean.

La estructura de singularidades y horizontes de eventos es más complicada en esta métrica que en Schwarzschild, debido al término extra en la función $\Delta(r)$ (que puede considerarse como una medida de ``cuánto se inclinan los conos de luz'') .
Una cosa sigue igual: en $r=0$ hay una verdadera singularidad de curvatura (como podría comprobarse calculando el escalar de curvatura $R_{\mn\rho\sigma}R^{\mn\rho\sigma}$).
Mientras tanto, el equivalente a $r=2GM$ será el radio donde $\Delta$ desaparecerá.
Esto ocurrirá en
\begin{equation}
r_\pm = GM\pm \sqrt{G^2M^2 - G(p^2+q^2)}\,.\label{7.113}
\end{equation}
Esto podría constituir dos, una o cero soluciones, según los valores relativos de $GM^2$ y $p^2+q^2$.
Por lo tanto, consideraremos cada caso por separado.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven28.pdf}
\end{figure}

\vbox{\vskip .5cm}
\noindent
{\it Caso Uno} --- $GM^2<p^2+q^2$

En este caso el coeficiente $\Delta$ es siempre positivo (nunca cero), y la métrica es completamente regular en las coordenadas ($t,r,\theta,\phi$) hasta $r=0$.
La coordenada $t$ siempre es temporal y $r$ siempre es espacial.
Pero todavía existe la singularidad en $r=0$, que ahora es una línea temporal.
Dado que no existe un horizonte de sucesos, no hay ningún obstáculo para que un observador viaje a la singularidad y regrese para informar sobre lo observado.
Esto se conoce como {\bf singularidad desnuda}, aquella que no está protegida por un horizonte.
Un análisis cuidadoso de las geodésicas revela, sin embargo, que la singularidad es ``repulsiva'': las geodésicas temporales nunca se cruzan con $r=0$, sino que se acercan y luego invierten su curso y se alejan.
(Las geodésicas nulas pueden alcanzar la singularidad, al igual que las curvas temporales no geodésicas).

A medida que $r\rightarrow\infty$ la solución se acerca al Espacio-Tiempo plano, y como acabamos de ver, la estructura causal es ``normal'' en todas partes.
Por tanto, el diagrama de Penrose será igual que el del espacio de Minkowski, excepto que ahora $r=0$ es una singularidad.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{imagenes/seven29.pdf}
\end{figure}

\noindent
La desnudez de la singularidad ofende nuestro sentido de la decencia, así como la {\bf conjetura de la censura cósmica}, que afirma aproximadamente que el colapso gravitacional de las configuraciones de la materia física nunca producirá una singularidad desnuda.
(Por supuesto, es sólo una conjetura, y puede que no sea correcta; hay algunas afirmaciones a partir de simulaciones numéricas de que el colapso de configuraciones en forma de huso puede conducir a singularidades desnudas).
De hecho, nunca deberíamos esperar encontrar un agujero negro con $GM^2<p^2+q^2$ como resultado de un colapso gravitacional.
En términos generales, esta condición establece que la energía total del agujero es menor que la contribución a la energía de los campos electromagnéticos solos; es decir, la masa de la materia que transportaba la carga tendría que haber sido negativa.
Por lo tanto, esta solución generalmente se considera no física.
Observe también que no hay buenas superficies de Cauchy (cortes espaciales en los que cada línea temporal inextensible las cruza) en este Espacio-Tiempo, ya que las líneas temporales pueden comenzar y terminar en la singularidad.

\noindent
{\it Caso Dos} --- $GM^2>p^2+q^2$

Ésta es la situación que esperamos que se aplique en un colapso gravitacional real; la energía en el campo electromagnético es menor que la energía total.
En este caso, el coeficiente métrico $\Delta(r)$ es positivo en $r$ grande y $r$ pequeño, y negativo dentro de los dos puntos de fuga $r_\pm = GM\pm \sqrt{G^2M^2 - G(p^2+q^2)}$.
La métrica tiene singularidades de coordenadas tanto en $r_+$ como en $r_-$; en ambos casos estos podrían eliminarse mediante un cambio de coordenadas como hicimos con Schwarzschild.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{imagenes/seven30.pdf}
\end{figure}

Las superficies definidas por $r=r_\pm$ son nulas y, de hecho, son horizontes de sucesos (en cierto sentido, lo precisaremos más adelante).
La singularidad en $r=0$ es una línea temporal (no una superficie espacial como en Schwarzschild).
Si eres un observador que cae en el agujero negro desde lejos, $r_+$ es como $2GM$ en la métrica de Schwarzschild; en este radio, $r$ pasa de ser una coordenada espacial a una coordenada temporal, y necesariamente te mueves en la dirección de disminución de $r$.
Los testigos fuera del agujero negro también ven los mismos fenómenos que verían fuera de un agujero sin carga: se ve que el observador que cae se mueve cada vez más lentamente y está cada vez más desplazado al rojo.

Pero la inevitable caída de $r_+$ a radios cada vez menores solo dura hasta que se alcanza la superficie nula $r=r_-$, donde $r$ vuelve a ser una coordenada espacial y el movimiento en la dirección de $r$ decreciente puede detenerse. .
Por lo tanto, no es necesario alcanzar la singularidad en $r=0$; esto es de esperarse, ya que $r=0$ es una línea temporal (y por lo tanto no necesariamente en su futuro).
De hecho, puede elegir continuar hasta $r=0$ o comenzar a moverse en la dirección de aumentar $r$ de regreso a través de la superficie nula en $r=r_-$.
Entonces $r$ volverá a ser una coordenada temporal, pero con orientación invertida; te ves obligado a moverte en la dirección de {\it creciente} $r$.
Eventualmente serás escupido más allá de $r=r_+$ una vez más, lo cual es como emerger de un agujero blanco hacia el resto del universo.
Desde aquí puedes elegir volver al agujero negro (esta vez, un agujero diferente al que entraste en primer lugar) y repetir el viaje tantas veces como quieras.
Esta pequeña historia corresponde al diagrama de Penrose adjunto, que por supuesto se puede derivar de manera más rigurosa eligiendo las coordenadas apropiadas y extendiendo analíticamente la métrica de Reissner-Nordstr{\o}m tanto como sea posible.

¿Cuánto de esto es ciencia, a diferencia de ciencia ficción?
Probablemente no mucho.
Si piensa en el mundo visto desde un observador dentro del agujero negro que está a punto de cruzar el horizonte de sucesos en $r_-$, notará que puede mirar hacia atrás en el tiempo para ver la historia completa del fenómeno externo (asintóticamente plano). universo, al menos visto desde el agujero negro.
Pero ven esta historia (infinitamente larga) en una cantidad finita de su tiempo adecuado; por lo tanto, cualquier señal que les llegue cuando se acercan a $r_-$ está infinitamente desplazada hacia el azul.
Por lo tanto, es razonable creer (aunque no conozco ninguna prueba) que cualquier perturbación no esféricamente simétrica que llegue a un agujero negro de Reissner-Nordstr{\o}m perturbará violentamente la geometría que hemos descrito.
Es difícil decir cómo será la geometría real, pero no hay muy buenas razones para creer que deba contener un número infinito de regiones asintóticamente planas conectadas entre sí a través de varios agujeros de gusano.

\noindent
{\it Caso Tres} --- $GM^2=p^2+q^2$

Este caso se conoce como solución {\bf extrema} de Reissner-Nordstr{\o}m (o simplemente ``agujero negro extremo'').
La masa está exactamente equilibrada en cierto sentido por la carga; se pueden construir soluciones exactas que consistan en varios agujeros negros extremos que permanecen estacionarios entre sí durante todo el tiempo.
Por un lado, el agujero extremo es un divertido juguete teórico; Estas soluciones se examinan a menudo en estudios sobre la paradoja de la pérdida de información y el papel de los agujeros negros en la gravedad cuántica.
Por otro lado, parece muy inestable, ya que añadir sólo un poco de materia lo llevará al Caso Dos.

\begin{figure}[ht]
\centering
\includegraphics[width=0.35\linewidth]{imagenes/seven31.pdf}
\end{figure}

Los agujeros negros extremos tienen $\Delta(r)=0$ en un solo radio, $r=GM$.
Esto representa un horizonte de eventos, pero la coordenada $r$ nunca es temporal; se vuelve nulo en $r=GM$, pero tiene forma de espacio en ambos lados.
La singularidad en $r=0$ es una línea temporal, como en los otros casos.
Así que para este agujero negro se puede evitar de nuevo la singularidad y continuar moviéndose hacia el futuro a copias extra de la región asintóticamente plana, pero la singularidad está siempre ``a la izquierda''. El diagrama de Penrose es como se muestra.

Por supuesto, podríamos entrar en muchos más detalles sobre las soluciones cargadas, pero pasemos a los agujeros negros giratorios.
En este caso es mucho más difícil encontrar la solución exacta para la métrica, ya que hemos abandonado la simetría esférica.
Para empezar, todo lo que está presente es simetría axial (alrededor del eje de rotación), pero también podemos pedir soluciones estacionarias (un vector Killing temporal).
Aunque las soluciones de Schwarzschild y Reissner-Nordstr{\o}m se descubrieron poco después de que se inventara la Relatividad General, Kerr no encontró la solución para un agujero negro en rotación hasta 1963.
Su resultado, la {\bf Kerr métrica}, viene dado por el siguiente desorden:
\begin{equation}
ds^2 = - d\,t^2 + {{\rho^2}\over \Delta} d\,r^2 +\rho^2 d\,\theta^2
+(r^2+a^2)\sin^2\theta\, d\,\phi^2 +{{2GMr}\over{\rho^2}}
(a\sin^2\theta\, d\,\phi -  d\,t)^2\ ,\label{7.114}
\end{equation}
dónde
\begin{equation}
\Delta(r) = r^2 - 2GMr +a^2\ ,\label{7.115}
\end{equation}
y
\begin{equation}
\rho^2(r,\theta) = r^2+a^2\cos^2\theta\,.\label{7.116}
\end{equation}
Aquí $a$ mide la rotación del agujero y $M$ es la masa.
Es sencillo incluir las cargas eléctricas y magnéticas $q$ y $p$, simplemente reemplazando $2GMr$ por $2GMr-(q^2+p^2)/G$; el resultado es la {\bf métrica de Kerr-Newman}.
Todos los fenómenos interesantes persisten en ausencia de cargos, por lo que configuraremos $q=p=0$ de ahora en adelante.

Las coordenadas $(t,r,\theta,\phi)$ se conocen como {\bf coordenadas de Boyer-Lindquist}.
Es sencillo comprobar que a medida que $a\rightarrow 0$ se reducen a coordenadas de Schwarzschild.
Sin embargo, si mantenemos $a$ fijo y dejamos $M\rightarrow 0$, recuperamos el Espacio-Tiempo plano pero no en coordenadas polares ordinarias.
La métrica se convierte
\begin{equation}
ds^2 = - d\,t^2 + {{(r^2+a^2\cos^2\theta)^2}\over (r^2+a^2)} d\,r^2
+(r^2+a^2\cos^2\theta)^2 d\,\theta^2
+(r^2+a^2)\sin^2\theta\, d\,\phi^2\ ,\label{7.117}
\end{equation}
y reconocemos la parte espacial de este como un espacio plano en coordenadas elipsoidales.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven32.pdf}
\end{figure}

\noindent
Están relacionados con las coordenadas cartesianas en el espacio tridimensional euclidiano por
\begin{align}
x &=  (r^2+a^2)^{1/2}\sin\theta\,\cos(\phi) \notag \\
y &=  (r^2+a^2)^{1/2}\sin\theta\,\sin(\phi) \notag \\
z &=  r\cos\theta\,.\label{7.118}
\end{align}

Hay dos vectores de Muerte de la métrica (7.114), ambos manifiestos; dado que los coeficientes métricos son independientes de $t$ y $\phi$, tanto $\zeta^\mu=\p{t}$ como $\eta^\mu=\p\phi$ son vectores de muerte.
Por supuesto, $\eta^\mu$ expresa la simetría axial de la solución.
El vector $\zeta^\mu$ no es ortogonal a $t=$ hipersuperficies constantes y, de hecho, no es ortogonal a ninguna hipersuperficie en absoluto; por tanto, esta métrica es estacionaria, pero no estática.
(No cambia con el tiempo, pero gira).

Es más, la métrica de Kerr también posee algo llamado {\bf Killing tensor}.
Este es cualquier tensor $(0,n)$ simétrico $\xi_{\mu_1\cdots\mu_n}$ que satisfaga
\begin{equation}
\nabla_{(\sigma}\xi_{\mu_1\cdots\mu_n)}=0\,.\label{7.119}
\end{equation}
Ejemplos simples de tensores Killing son la métrica misma y los productos tensoriales simetrizados de los vectores Killing.
Así como un vector Killing implica una constante de movimiento geodésico, si existe un tensor Killing entonces a lo largo de una geodésica tendremos
\begin{equation}
\xi_{\mu_1\cdots\mu_n}{{dx^{\mu_1}}\over{d\lambda}}\cdots
{{dx^{\mu_n}}\over{d\lambda}} = {\rm constant}\,.\label{7.120}
\end{equation}
(A diferencia de los vectores Killing, los tensores Killing de rango superior no corresponden a simetrías de la métrica).
En la geometría de Kerr podemos definir el tensor $(0,2)$
\begin{equation}
\xi_\mn = 2\rho^2 l_{(\mu}n_{\nu)} + r^2 g_\mn\,.\label{7.121}
\end{equation}
En esta expresión, los dos vectores $l$ y $n$ están dados (con índices elevados) por
\begin{align}
l^\mu  &=  {1\over\Delta}\left(r^2+a^2, \Delta, 0, a\right) \notag \\
n^\mu  &=  {1\over{2\rho^2}}\left(r^2+a^2, -\Delta, 0, a\right)\,.
\label{7.122}
\end{align}
Ambos vectores son nulos y satisfacen
\begin{equation}
l^\mu l_\mu =0\ ,\quad n^\mu n_\mu =0\ ,\quad l^\mu n_\mu =-1\,.
\label{7.123}
\end{equation}
(Por si sirve de algo, son los ``vectores nulos especiales'' de la clasificación de Petrov para este Espacio-Tiempo.)
Con estas definiciones, puedes comprobar por ti mismo que $\xi_\mn$ es un tensor Killing.

Pensemos en la estructura de la solución Kerr completa.
Las singularidades parecen aparecer tanto en $\Delta=0$ como en $\rho=0$; dirijamos nuestra atención primero a $\Delta=0$.
Como en la solución de Reissner-Nordstr{\o}m, existen tres posibilidades: $G^2M^2>a^2$, $G^2M^2=a^2$ y $G^2M^2<a^2$.
El último caso presenta una singularidad desnuda y el caso extremo $G^2M^2=a^2$ es inestable, tal como en Reissner-Nordstr{\o}m.
Dado que estos casos son de menor interés físico y el tiempo es corto, nos concentraremos en $G^2M^2>a^2$.
Entonces hay dos radios en los que $\Delta$ desaparece, dados por
\begin{equation}
r_\pm = GM\pm\sqrt{G^2M^2 - a^2}\,.\label{7.124}
\end{equation}
Ambos radios son superficies nulas que resultarán ser horizontes de sucesos.
El análisis de estas superficies procede en estrecha analogía con el caso Reissner-Nordstr{\o}m; es sencillo encontrar coordenadas que se extiendan a través de los horizontes.

Además de los horizontes de eventos en $r_\pm$, la solución de Kerr también presenta una superficie de interés adicional.
Recuerde que en las soluciones esféricamente simétricas, el vector de muerte ``temporal'' $\zeta^\mu=\p{t}$ en realidad se volvió nulo en el horizonte de sucesos (exterior) y espacial en el interior.
Comprobando dónde sucede algo análogo para Kerr, calculamos
\begin{equation}
\zeta^\mu\zeta_\mu = -{1\over{\rho^2}}(\Delta-a^2\sin^2\theta)\,.
\label{7.125}
\end{equation}
Esto no desaparece en el horizonte de sucesos exterior; de hecho, en $r=r_+$ (donde $\Delta=0$), tenemos
\begin{equation}
\zeta^\mu\zeta_\mu={{a^2}\over{\rho^2}}\sin^2\theta \geq 0\,.
\label{7.126}
\end{equation}
Entonces, el vector Killing ya es espacial en el horizonte exterior, excepto en los polos norte y sur ($\theta=0$) donde es nulo.
El lugar geométrico de los puntos donde $\zeta^\mu\zeta_\mu =0$ se conoce como {\bf Killing horizonte} y está dado por
\begin{equation}
(r-GM)^2 = G^2M^2 - a^2\cos^2\theta\ ,\label{7.127}
\end{equation}
mientras que el horizonte de sucesos exterior está dado por
\begin{equation}
(r_+-GM)^2 = G^2M^2 - a^2\,.\label{7.128}
\end{equation}
Por tanto, existe una región entre estas dos superficies, conocida como {\bf ergosfera}.
Dentro de la ergosfera, debes moverte en la dirección de rotación del agujero negro (la dirección $\phi$); sin embargo, aún puede acercarse o alejarse del horizonte de sucesos (y no hay problemas para salir de la ergosfera).
Evidentemente es un lugar donde pueden suceder cosas interesantes incluso antes de cruzar el horizonte; Más detalles sobre esto más adelante.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven33.pdf}
\end{figure}

Antes de apresurarnos a dibujar diagramas de Penrose, debemos comprender la naturaleza de la verdadera singularidad de curvatura; esto no ocurre en $r=0$ en este Espacio-Tiempo, sino en $\rho=0$.
Dado que $\rho^2 = r^2+a^2\cos^2\theta$ es la suma de dos cantidades manifiestamente no negativas, sólo puede desaparecer cuando ambas cantidades son cero, o
\begin{equation}
r=0\ ,\qquad \theta = {\pi\over 2}\,.\label{7.129}
\end{equation}
Esto parece un resultado curioso, pero recuerda que $r=0$ no es un punto en el espacio, sino un disco; el conjunto de puntos $r=0$, $\theta=\pi/2$ es en realidad el {\it ring} en el borde de este disco.
La rotación ha ``suavizado'' la singularidad de Schwarzschild, extendiéndola sobre un anillo.

¿Qué pasa si entras al ring? Una continuación analítica cuidadosa (que no realizaremos) revelaría que sales a otro Espacio-Tiempo asintóticamente plano, pero no a una copia idéntica de aquel de donde viniste.
El nuevo Espacio-Tiempo se describe mediante la métrica de Kerr con $r<0$.
Como resultado, $\Delta$ nunca desaparece y no hay horizontes.
El diagrama de Penrose es muy parecido al de Reissner-Nordstr{\o}m, excepto que ahora se puede pasar por la singularidad.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{imagenes/seven34.pdf}
\end{figure}

No sólo tenemos la extrañeza habitual de estas distintas regiones asintóticamente planas conectadas a la nuestra a través del agujero negro, sino que la región cercana a la singularidad del anillo tiene patologías adicionales: curvas temporales cerradas.
Si considera trayectorias que giran en $\phi$ manteniendo constantes $\theta$ y $t$ y $r$ un pequeño valor negativo, el elemento de línea a lo largo de dicha ruta es
\begin{equation}
ds^2 = a^2\left(1+{{2GM}\over r}\right) d\,\phi^2\ ,\label{7.130}
\end{equation}
que es negativo para el negativo pequeño $r$.
Como estos caminos están cerrados, obviamente son CTC.
Por lo tanto, puedes encontrarte contigo mismo en el pasado, con todo lo que ello implica.

Por supuesto, todo lo que decimos sobre la extensión analítica de Kerr está sujeto a las mismas advertencias que mencionamos para Schwarzschild y Reissner-Nordstr{\o}m; Es poco probable que un colapso gravitacional realista conduzca a estos extraños espacio-tiempos.
Sin embargo, siempre es útil tener soluciones exactas.
Además, para la métrica de Kerr suceden cosas extrañas incluso si nos mantenemos fuera del horizonte de sucesos, al que nos referiremos ahora.

Comenzamos considerando más cuidadosamente la velocidad angular del agujero.
Obviamente, la definición convencional de velocidad angular tendrá que modificarse un poco antes de que podamos aplicarla a algo tan abstracto como la métrica del Espacio-Tiempo.
Consideremos el destino de un fotón que se emite en la dirección $\phi$ en algún radio $r$ en el plano ecuatorial ($\theta=\pi/2$) de un agujero negro de Kerr.
En el instante en que se emite su momentum no tiene componentes en la dirección $r$ o $\theta$, por lo que la condición de que sea nulo es
\begin{equation}
ds^2 = 0 = g_{tt} d\,t^2 + g_{t\phi}( d\,t d\,\phi+ d\,\phi  d\,t)
+g_{\phi\phi} d\,\phi^2\,.\label{7.131}
\end{equation}
Esto se puede resolver inmediatamente para obtener
\begin{equation}
{{d\phi}\over{dt}} = -{{g_{t\phi}}\over{g_{\phi\phi}}}
\pm\sqrt{\left({{g_{t\phi}}\over{g_{\phi\phi}}}\right)^2
-{{g_{tt}}\over{g_{\phi\phi}}}}\,.\label{7.132}
\end{equation}
Si evaluamos esta cantidad en el horizonte Killing de la métrica de Kerr, tenemos $g_{tt}=0$, y las dos soluciones son
\begin{equation}
{{d\phi}\over{dt}}=0\ ,\qquad {{d\phi}\over{dt}}={{2a}\over
{(2GM)^2+a^2}}\,.\label{7.133}
\end{equation}
La solución distinta de cero tiene el mismo signo que $a$; interpretamos esto como que el fotón se mueve alrededor del agujero en la misma dirección que la rotación del agujero.
La solución cero significa que el fotón dirigido contra la rotación del agujero no se mueve en absoluto en este sistema de coordenadas.
(Esta no es una solución completa a la trayectoria del fotón, sólo la afirmación de que su velocidad instantánea es cero).
Este es un ejemplo del ``arrastre de marcos inerciales'' mencionado anteriormente.
El objetivo de este ejercicio es observar que las partículas masivas, que deben moverse más lentamente que los fotones, son necesariamente arrastradas junto con la rotación del agujero una vez que están dentro del horizonte Killing.
Este arrastre continúa a medida que nos acercamos al horizonte de sucesos exterior en $r_+$; Podemos definir la velocidad angular del propio horizonte de sucesos, $\Omega_H$, como la velocidad angular mínima de una partícula en el horizonte.
Directamente de (7.132) encontramos que
\begin{equation}
\Omega_H = \left({{d\phi}\over{dt}}\right)_-(r_+)
= {a\over{r_+^2+a^2}}\,.\label{7.134}
\end{equation}

Ahora pasemos al movimiento geodésico, que sabemos se simplificará al considerar las cantidades conservadas asociadas con los vectores Killing $\zeta^\mu=\p{t}$ y $\eta^\mu=\p\phi$.
Para los propósitos que nos ocupan podemos restringir nuestra atención a las partículas masivas, para las cuales podemos trabajar con los cuatro momentos.
\begin{equation}
p^\mu = m {{dx^\mu}\over{d\tau}}\ ,\label{7.135}
\end{equation}
donde $m$ es la masa en reposo de la partícula.
Entonces podemos tomar como nuestras dos cantidades conservadas la energía real y el momento angular de la partícula,
\begin{equation}
E=-\zeta_\mu p^\mu = m\left(1-{{2GMr}\over {\rho^2}}\right)
{{dt}\over{d\tau}}
+{{2mGMar}\over{\rho^2}}\sin^2\theta\, {{d\phi}\over{d\tau}}\label{7.136}
\end{equation}
y
\begin{equation}
L=\eta_\mu p^\mu=-{{2mGMar}\over{\rho^2}}\sin^2\theta\, {{dt}\over{d\tau}}
+{{m(r^2+a^2)^2 - m\Delta a^2\sin^2\theta}\over{\rho^2}}\sin^2\theta\,
{{d\phi}\over{d\tau}}\,.\label{7.137}
\end{equation}
(Estos difieren de nuestras definiciones anteriores para las cantidades conservadas, donde $E$ y $L$ se tomaron como la energía y el momento angular {\it por unidad de masa}.
Se conservan de cualquier manera, por supuesto).

El signo menos en la definición de $E$ está ahí porque en el infinito tanto $\zeta^\mu$ como $p^\mu$ son temporales, por lo que su producto interno es negativo, pero queremos que la energía sea positiva.
Sin embargo, dentro de la ergosfera, $\zeta^\mu$ se vuelve espacial; Por lo tanto, podemos imaginar partículas para las cuales
\begin{equation}
E = -\zeta_\mu p^\mu < 0\,.\label{7.138}
\end{equation}
El grado en que esto nos molesta mejora un poco al comprender que \textit{todas} las partículas fuera del horizonte de Matanza deben tener energías positivas; por lo tanto, una partícula dentro de la ergosfera con energía negativa debe permanecer en una geodésica dentro del horizonte de Killing o ser acelerada hasta que su energía sea positiva para escapar.

Aún así, este descubrimiento conduce a una forma de extraer energía de un agujero negro en rotación; el método se conoce como {\bf proceso de Penrose}.
La idea es simple; Comenzando desde fuera de la ergosfera, te armas con una gran roca y saltas hacia el agujero negro.
Si llamamos al cuatro momento del sistema (tú + roca) $p^{(0)\mu}$, entonces la energía $E^{(0)}=-\zeta_\mu p^{(0)\mu}$ es ciertamente positiva y se conserva a medida que avanzas a lo largo de tu geodésica.
Una vez que entras en la ergosfera, lanzas la roca con todas tus fuerzas, de una forma muy específica.
Si llamamos a tu momentum $p^{(1)\mu}$ y al de la roca $p^{(2)\mu}$, entonces en el instante en que la lanzas tenemos conservación del momentum tal como en la Relatividad Especial:
\begin{equation}
p^{(0)\mu}=p^{(1)\mu}+p^{(2)\mu}\,.\label{7.139}
\end{equation}
Contraerse con el vector Killing $\zeta_\mu$ da
\begin{equation}
E^{(0)}=E^{(1)}+E^{(2)}\,.\label{7.140}
\end{equation}
Pero, si imaginamos que eres arbitrariamente fuerte (y preciso), puedes organizar tu lanzamiento de manera que $E^{(2)}<0$, según (7.158).

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/seven35.pdf}
\end{figure}
Además, Penrose pudo demostrar que se puede organizar la trayectoria inicial y el lanzamiento de manera que luego se siga una trayectoria geodésica de regreso fuera del horizonte Killing hacia el universo externo.
Dado que tu energía se conserva en el camino, al final tendremos
\begin{equation}
E^{(1)}>E^{(0)}\,.\label{7.141}
\end{equation}
Por lo tanto, has salido con más energía de la que entraste.

No existe nada parecido a un almuerzo gratis; la energía que obtuviste vino de algún lugar, y ese lugar es el agujero negro.
De hecho, el proceso de Penrose extrae energía del agujero negro en rotación disminuyendo su momento angular; Tienes que lanzar la piedra en contra de la rotación del agujero para que el truco funcione.
Para ver esto con mayor precisión, defina un nuevo vector Killing.
\begin{equation}
\chi^\mu = \zeta^\mu + \Omega_H\eta^\mu\,.\label{7.142}
\end{equation}
En el horizonte exterior $\chi^\mu$ es nulo y tangente al horizonte.
(Esto se puede ver en $\zeta^\mu = \p{t}$, $\eta^\mu=\p\phi$ y la definición (7.134) de $\Omega_H$.)
La afirmación de que la partícula con momentum $p^{(2)\mu}$ cruza el horizonte de sucesos ``avanzando en el tiempo'' es simplemente
\begin{equation}
p^{(2)\mu}\chi_\mu <0\,.\label{7.143}
\end{equation}
Al conectar las definiciones de $E$ y $L$, vemos que esta condición es equivalente a
\begin{equation}
L^{(2)} < \frac{E^{(2)}}{\Omega_H} \,.\label{7.144}
\end{equation}
Como hemos dispuesto que $E^{(2)}$ sea negativo y $\Omega_H$ es positivo, vemos que la partícula debe tener un momento angular negativo: se mueve en contra de la rotación del agujero.
Una vez que has escapado de la ergosfera y la roca ha caído dentro del horizonte de sucesos, la masa y el momento angular del agujero son los que solían ser más las contribuciones negativas de la roca:
\begin{align}
\delta M  &=  E^{(2)} \notag \\  \delta J  &=  L^{(2)}.
\label{7.145}
\end{align}
Aquí hemos introducido la notación $J$ para el momento angular del agujero negro; esta dado por
\begin{equation}
J=Ma\,.\label{7.146}
\end{equation}
No justificaremos esto, pero puedes buscar una explicación en Wald.
Entonces (7.144) se convierte en un límite de cuánto se puede disminuir el momento angular:
\begin{equation}
\delta J < \frac{\delta M}{\Omega_H}\,.
\label{7.147}
\end{equation}
Si alcanzamos exactamente este límite, a medida que la piedra que arrojamos se vuelve cada vez más nula, tenemos el proceso ``ideal'', en el que $\delta J=\delta M/\Omega_H$.

Ahora usaremos estas ideas para demostrar un resultado poderoso: aunque se puede usar el proceso de Penrose para extraer energía del agujero negro, nunca se puede disminuir el área del horizonte de sucesos.
Para una métrica de Kerr, se puede realizar un cálculo sencillo (proyectando la métrica y el elemento de volumen, etc.) para calcular el área del horizonte de eventos:
\begin{equation}
A = 4\pi(r_+^2 + a^2)\,.\label{7.148}
\end{equation}
Para demostrar que esto no disminuye, lo más conveniente es trabajar en términos de la {\bf masa irreducible} del agujero negro, definida por
\begin{align}
M_{\rm irr}^2  &=  {{A}\over {16\pi G^2}} \notag \\
&=  {{1\over {4G^2}}}(r_+^2 + a^2) \notag \\
&=  {1\over {2}}\left(M^2 + \sqrt{M^4-(Ma/G)^2}\right) \notag \\
&=  {1\over {2}}\left(M^2 + \sqrt{M^4-(J/G)^2}\right)\,.
\label{7.149}
\end{align}
Podemos diferenciar para obtener, tras un poco de trabajo,
\begin{equation}
\delta M_{\rm irr} = {{a}\over{4G\sqrt{G^2M^2-a^2}M_{\rm irr}}}
(\Omega_H^{-1}\delta M - \delta J)\,.\label{7.150}
\end{equation}
(Creo que tengo los factores de $G$ correctos, pero no estaría de más comprobarlo).
Entonces nuestro límite (7.147) se convierte en
\begin{equation}
\delta M_{\rm irr} >0\,.\label{7.151}
\end{equation}
La masa irreducible nunca puede reducirse; de ahí el nombre.
De ello se deduce que la cantidad máxima de energía que podemos extraer de un agujero negro antes de reducir su rotación a cero es
\begin{equation}
M-M_{\rm irr} = M-{1\over\sqrt2}
\left(M^2 + \sqrt{M^4-(J/G)^2}\right)^{1/2}\,.\label{7.152}
\end{equation}
El resultado de esta extracción completa es un agujero negro de Schwarzschild de masa $M_{\rm irr}$.
Resulta que lo mejor que podemos hacer es empezar con un agujero negro de Kerr extremo; entonces podemos sacar aproximadamente $29\%$ de su energía total.

La irreductibilidad de $M_{\rm irr}$ lleva inmediatamente al hecho de que el área $A$ nunca puede disminuir.
De (7.149) y (7.150) tenemos
\begin{equation}
\delta A = 8\pi G{a\over{\Omega_H\sqrt{G^2M^2-a^2}}}
(\delta M - \Omega_H\delta_J)\ ,\label{7.153}
\end{equation}
que puede reformularse como
\begin{equation}
\delta M = {{\kappa}\over{8\pi G}}\delta A +\Omega_H
\delta J\ ,\label{7.154}
\end{equation}
donde hemos introducido
\begin{equation}
\kappa = {{\sqrt{G^2M^2-a^2}}\over{2GM(GM+
\sqrt{G^2M^2-a^2})}}				\,.\label{7.155}
\end{equation}
La cantidad $\kappa$ se conoce como {\bf gravedad superficial} del agujero negro.

Fueron ecuaciones como (7.154) las que hicieron que la gente pensara por primera vez en la relación entre los agujeros negros y la termodinámica.
Considere la primera ley de la termodinámica,
\begin{equation}
dU = TdS + {\rm work~ terms}\,.\label{7.156}
\end{equation}
Es natural pensar en el término $\Omega_H\delta J$ como ``trabajo'' que realizamos en el agujero negro arrojándole piedras.
Entonces la analogía termodinámica comienza a tomar forma si pensamos en identificar el área $A$ como la entropía $S$, y la gravedad superficial $\kappa$ como $8\pi G$ multiplicada por la temperatura $T$.
De hecho, en el contexto de la Relatividad General clásica la analogía es esencialmente perfecta.
La ley ``cero'' de la termodinámica establece que en equilibrio térmico la temperatura es constante en todo el sistema; la afirmación análoga para los agujeros negros es que los agujeros negros estacionarios tienen una gravedad superficial constante en todo el horizonte (verdadero).
Como hemos visto, la primera ley (7.156) es equivalente a (7.154).
La segunda ley, que dice que la entropía nunca disminuye, es simplemente la afirmación de que el área del horizonte nunca disminuye.
Finalmente, la tercera ley es que es imposible lograr $T=0$ en cualquier proceso físico, lo que debería implicar que es imposible lograr $\kappa=0$ en cualquier proceso físico.
Resulta que $\kappa=0$ corresponde a los agujeros negros extremos (ya sea en Kerr o en Reissner-Nordstr{\o}m) --- donde aparecerían las singularidades desnudas.
Entonces, de alguna manera la tercera ley está relacionada con la censura cósmica.

La pieza que falta es que los cuerpos termodinámicos {\it reales} no se quedan ahí sentados; Emiten radiación de cuerpo negro con un espectro que depende de su temperatura.
Los agujeros negros, se pensaba antes de que Hawking descubriera su radiación, no hagan eso, ya que son verdaderamente negros.
Históricamente, a Bekenstein se le ocurrió la idea de que los agujeros negros deberían ser en realidad cuerpos negros honestos, incluida la radiación a la temperatura adecuada.
Esto molestó a Hawking, quien se propuso demostrar que estaba equivocado y terminó demostrando que, después de todo, habría radiación.
Así que la analogía termodinámica es incluso mejor de lo que teníamos derecho a esperar, aunque es seguro decir que nadie sabe realmente por qué.





\chapter{Cosmología}
%\addcontentsline{toc}{chapter}{Cosmología}


Los modelos cosmológicos contemporáneos se basan en la idea de que el universo es más o menos igual en todas partes, una postura a veces conocida como el {\bf principio copernicano}.
A primera vista, tal afirmación parece absurda; el centro del sol, por ejemplo, se parece poco al frío desolado del espacio interestelar.
Pero consideramos que el principio copernicano sólo se aplica en las escalas más grandes, donde se promedian las variaciones locales en la densidad.
Su validez en tales escalas se manifiesta en una serie de observaciones diferentes, como recuentos de galaxias y observaciones de fondos difusos de rayos X y de rayos $\gamma$, pero es más clara en la radiación de fondo de microondas $3^\circ$.
Aunque ahora sabemos que el fondo de microondas no es perfectamente uniforme (y nadie esperaba que lo fuera), las desviaciones de la regularidad son del orden de $10^{-5}$ o menos, sin duda una base adecuada para una descripción aproximada del Espacio-Tiempo a gran escala. .

El principio copernicano está relacionado con dos propiedades matemáticamente más precisas que podría tener una variedad: isotropía y homogeneidad.
{\bf Isotropía} se aplica en algún punto específico del espacio y establece que el espacio se ve igual sin importar en qué dirección se mire.
Más formalmente, una variedad $M$ es isotrópica alrededor de un punto $p$ si, para dos vectores cualesquiera $V$ y $W$ en $T_pM$, existe una isometría de $M$ tal que el avance de $W$ bajo la isometría es paralelo a $V$ (no empujado hacia adelante).
Es la isotropía la que se indica mediante las observaciones del fondo de microondas.

{\bf Homogeneidad} es la afirmación de que la métrica es la misma en todo el espacio.
En otras palabras, dados dos puntos cualesquiera $p$ y $q$ en $M$, existe una isometría que lleva a $p$ a $q$.
Tenga en cuenta que no existe una relación necesaria entre homogeneidad e isotropía; una variedad puede ser homogénea pero en ninguna parte isotrópica (como $\R\times S^2$ en la métrica habitual), o puede ser isotrópica alrededor de un punto sin ser homogénea (como un cono, que es isotrópico alrededor de su vértice pero ciertamente no homogéneo).
Por otro lado, si un espacio es isotrópico {\it en todas partes} entonces es homogéneo.
(Del mismo modo, si es isotrópico alrededor de un punto y también homogéneo, será isotrópico alrededor de todos los puntos).
Dado que existe amplia evidencia observacional de la isotropía, y el principio copernicano nos haría creer que no somos el centro del universo y, por lo tanto, los observadores en otros lugares también deberían observar la isotropía, de ahora en adelante asumiremos tanto la homogeneidad como la isotropía.

Hay un inconveniente.
Cuando miramos galaxias distantes, parece que se alejan de nosotros; Al parecer, el universo no es estático, sino que cambia con el tiempo.
Por tanto, comenzamos la construcción de modelos cosmológicos con la idea de que el universo es homogéneo e isotrópico en el espacio, pero no en el tiempo.
En la Relatividad General, esto se traduce en la afirmación de que el universo puede dividirse en porciones espaciales, de modo que cada porción sea homogénea e isotrópica.
Por lo tanto, consideramos que nuestro Espacio-Tiempo es $\R\times\Sigma$, donde $\R$ representa la dirección del tiempo y $\Sigma$ es una triple variedad homogénea e isotrópica.
La utilidad de la homogeneidad y la isotropía es que implican que $\Sigma$ debe ser un espacio máximamente simétrico.
(Piense en la isotropía como invariancia bajo rotaciones y en la homogeneidad como invariancia bajo traslaciones.
Entonces, la homogeneidad y la isotropía juntas implican que un espacio tiene el máximo número posible de vectores Killing.)
Por lo tanto, podemos tomar nuestra métrica como de la forma
\begin{equation}
ds^2 = -dt^2 + a^2(t)\gamma_{ij}(u) d\,u^i d\,u^j\,.\label{8.1}
\end{equation}
Aquí $t$ es la coordenada temporal y $(u^1, u^2, u^3)$ son las coordenadas en $\Sigma$; $\gamma_{ij}$ es la métrica de máxima simetría en $\Sigma$.
Esta fórmula es un caso especial de (7.2), que utilizamos para derivar la métrica de Schwarzschild, excepto que hemos escalado $t$ de manera que $g_{tt}=-1$.
La función $a(t)$ se conoce como {\bf factor de escala} y nos dice ``qué tan grande'' es la porción espacial $\Sigma$ en el momento $t$.
Las coordenadas utilizadas aquí, en las que la métrica está libre de términos cruzados $ d\,t\, d\,u^i$ y los componentes espaciales son proporcionales a una única función de $t$, se conocen como {\bf coordenadas comoving}, y un observador que permanece en constante $u^i$ también se denomina ``comomovimiento''.
Sólo un observador en movimiento pensará que el universo parece isotrópico; de hecho, en la Tierra no nos movemos del todo y, como resultado, vemos una anisotropía dipolar en el fondo cósmico de microondas como resultado del efecto Doppler convencional.

Por lo tanto, nuestro interés está en las tres métricas euclidianas máximamente simétricas $\gamma_{ij}$.
Sabemos que las métricas máximamente simétricas obedecen
\begin{equation}
^{(3)}R_{ijkl} = k(\gamma_{ik}\gamma_{jl}
-\gamma_{il}\gamma_{jk})\ ,\label{8.2}
\end{equation}
donde $k$ es una constante, y ponemos un superíndice $^{(3)}$ en el tensor de Riemann para recordarnos que está asociado con las tres métricas $\gamma_{ij}$, no con la métrica de todo el Espacio-Tiempo.
El tensor de Ricci es entonces
\begin{equation}
^{(3)}R_{jl} = 2k\gamma_{jl}\,.\label{8.3}
\end{equation}
Si el espacio ha de ser máximamente simétrico, entonces ciertamente será esféricamente simétrico.
Ya sabemos algo sobre espacios esféricamente simétricos gracias a nuestra exploración de la solución de Schwarzschild; la métrica se puede poner en la forma
\begin{equation}
d\sigma^2 = \gamma_{ij} d\,u^i\, d\,u^j =
e^{2\beta(r)} d\,r^2 + r^2( d\,\theta^2 +
\sin^2\theta\, d\,\phi^2)\,.\label{8.4}
\end{equation}
Los componentes del tensor de Ricci para dicha métrica se pueden obtener de (7.16), el tensor de Ricci para un espaciotiempo esféricamente simétrico, estableciendo $\alpha=0$ y $\p0\beta=0$, lo que da
\begin{align}
^{(3)}R_{11}  &=  {2\over r}\p1\beta \notag \\
^{(3)}R_{22}  &=  e^{-2\beta}(r\p1 \beta-1)+1 \notag \\
^{(3)}R_{33}  &=  [e^{-2\beta}(r\p1 \beta-1)+1]\sin^2\theta\, \,.
\label{8.5}
\end{align}
Los establecemos proporcionales a la métrica usando (8.3) y podemos resolver para $\beta(r)$:
\begin{equation}
\beta = -{1\over 2}\ln(1-kr^2)\,.\label{8.6}
\end{equation}
Esto nos da la siguiente métrica sobre el Espacio-Tiempo:
\begin{equation}
ds^2 = -dt^2 + a^2(t)\left[{{ d\,r^2}\over{1-kr^2}}
+ r^2( d\,\theta^2 +\sin^2\theta\, d\,\phi^2)\right]\,.\label{8.7}
\end{equation}
Esta es la {\bf métrica de Robertson-Walker}.
Todavía no hemos utilizado las ecuaciones de Einstein; éstos determinarán el comportamiento del factor de escala $a(t)$.

Tenga en cuenta que las sustituciones
\begin{align}
k&\rightarrow{k\over{|k|}} \notag \\
r &\rightarrow \sqrt{|k|}\, r \notag \\
a &\rightarrow{a\over{\sqrt{|k|}}}\label{8.8}
\end{align}
Deje (8.7) invariante.
Por lo tanto, el único parámetro relevante es $k/|k|$ y hay tres casos de interés: $k=-1$, $k=0$ y $k=+1$.
El caso $k=-1$ corresponde a una curvatura negativa constante en $\Sigma$, y se denomina {\bf open}; el caso $k=0$ corresponde a ninguna curvatura en $\Sigma$, y se llama {\bf plano}; el caso $k=+1$ corresponde a la curvatura positiva en $\Sigma$ y se denomina {\bf cerrado}.

Examinemos cada una de estas posibilidades.
Para el caso plano $k=0$, la métrica en $\Sigma$ es
\begin{align}
d\sigma^2  &=   d\,r^2 + r^2 d\Omega^2 \notag \\
&=   d\,x^2 +  d\,y^2 + d\,z^2\ ,\label{8.9}
\end{align}
que es simplemente un espacio euclidiano plano.
Globalmente, podría describir $\R^3$ o una variedad más complicada, como los tres toros $S^1\times S^1 \times S^1$.
Para el caso cerrado $k=+1$ podemos definir $r=\sin\chi$ para escribir la métrica en $\Sigma$ como
\begin{equation}
d\sigma^2 =  d\,\chi^2 + \sin^2\chi\, d\Omega^2\ ,\label{8.10}
\end{equation}
que es la métrica de tres esferas.
En este caso, la única estructura global posible es en realidad la de tres esferas (a excepción de la variedad no orientable $\R$P$^3$).
Finalmente, en el caso abierto $k=-1$ podemos configurar $r=\sinh\psi$ para obtener
\begin{equation}
d\sigma^2 =  d\,\psi^2 + \sinh^2\psi\, d\Omega^2\,.\label{8.11}
\end{equation}
Ésta es la métrica de un espacio tridimensional de curvatura negativa constante; Es difícil de visualizar, pero pensemos en el ejemplo de la silla de montar del que hablamos en la Sección Tres.
Globalmente, tal espacio podría extenderse para siempre (que es el origen de la palabra ``abierto''), pero también podría describir un espacio compacto no simplemente conectado (por lo que ``abierto'' no es realmente la descripción más precisa) .

Con la métrica en mano, podemos comenzar a calcular los coeficientes de conexión y el tensor de curvatura.
Configurando $\dot a\equiv da/dt$, los símbolos de Christoffel vienen dados por
\begin{align}
\Gamma^0_{11}  &=  {{a\dot a}\over{1-kr^2}}\qquad
\Gamma^0_{22} = a\dot a r^2
\qquad \Gamma^0_{33} = a\dot a r^2\sin^2\theta \notag \\
\Gamma^1_{01}  &=  \Gamma^1_{10} = \Gamma^2_{02}= \Gamma^2_{20}
= \Gamma^3_{03}= \Gamma^3_{30}={{\dot a}\over a} \notag \\
\Gamma^1_{22}  &=  -r(1-kr^2) \qquad
\Gamma^1_{33} = -r(1-kr^2)\sin^2\theta \notag \\
\Gamma^2_{12}  &=  \Gamma^2_{21} = \Gamma^3_{13} = \Gamma^3_{31} =
{1\over r} \notag \\
\Gamma^2_{33}  &=  -\sin\theta\,\cos\theta \qquad
\Gamma^3_{23} = \Gamma^3_{32} = \cot\theta\,.\label{8.12}
\end{align}
Los componentes distintos de cero del tensor de Ricci son
\begin{align}
R_{00}  &=  -3{{\ddot a}\over a} \notag \\
R_{11}  &=  {{a\ddot a + 2\dot a^2 +2k}\over{1-kr^2}} \notag \\
R_{22}  &=  r^2(a\ddot a+ 2\dot a^2 +2k) \notag \\
R_{33}  &=  r^2(a\ddot a+ 2\dot a^2 +2k)\sin^2\theta \ ,\label{8.13}
\end{align}
y el escalar de Ricci es entonces
\begin{equation}
R = {{6}\over{a^2}}(a\ddot a+ \dot a^2 +k)\,.\label{8.14}
\end{equation}

El universo no está vacío, por lo que no nos interesan las soluciones en vacío de las ecuaciones de Einstein.
Elegiremos modelar la materia y la energía del universo mediante un fluido perfecto.
Hablamos de los fluidos perfectos en la Sección Uno, donde se definieron como fluidos que son isotrópicos en su estado de reposo.
El tensor de energía-momento de un fluido perfecto se puede escribir
\begin{equation}
T_{\mn} = (p+\rho)U_\mu U_\nu + pg_\mn\ ,\label{8.15}
\end{equation}
donde $\rho$ y $p$ son la densidad de energía y la presión (respectivamente) medidas en el marco de reposo, y $U^\mu$ son las cuatro velocidades del fluido.
Está claro que, si un fluido que es isotrópico en algún marco conduce a una métrica que es isotrópica en algún marco, los dos marcos coincidirán; es decir, el fluido estará en reposo en coordenadas comoving.
Las cuatro velocidades son entonces
\begin{equation}
U^\mu = (1,0,0,0)\ ,\label{8.16}
\end{equation}
y el tensor de energía-momento es
\begin{equation}
T_\mn = \left(\mqty{\rho &0&0&0 \\  0& & &  \\
0& & g_{ij} p&  \\  0& & &  \\ }\right)\,.\label{8.17}
\end{equation}
Con un índice elevado esto toma la forma más conveniente
\begin{equation}
T^\mu{}_\nu = {\rm diag}(-\rho,p,p,p)\,.\label{8.18}
\end{equation}
Tenga en cuenta que la traza está dada por
\begin{equation}
T = T^\mu{}_\mu = -\rho +3p\,.\label{8.19}
\end{equation}

Antes de abordar las ecuaciones de Einstein, es educativo considerar el componente cero de la ecuación de conservación de la energía:
\begin{align}
0  &=  \nabla_\mu T^\mu{}_0 \notag \\
&=  \p\mu T^\mu{}_0 +\Gamma^\mu_{\mu 0}T^0{}_0
-\Gamma^\lambda_{\mu 0}T^\mu{}_\lambda \notag \\
&=  -\p0\rho -3{{\dot a}\over a}(\rho+p)\,.
\label{8.20}
\end{align}
Para avanzar es necesario elegir una {\bf ecuación de estado}, una relación entre $\rho$ y $p$.
Básicamente, todos los fluidos perfectos relevantes para la cosmología obedecen a la simple ecuación de estado.
\begin{equation}
p=w\rho\ ,\label{8.21}
\end{equation}
donde $w$ es una constante independiente del tiempo.
La ecuación de conservación de la energía se convierte en
\begin{equation}
{{\dot \rho}\over\rho} = -3(1+w){{\dot a}\over{a}}\ ,\label{8.22}
\end{equation}
que se pueden integrar para obtener
\begin{equation}
\rho \propto a^{-3(1+w)}\,.\label{8.23}
\end{equation}

Los dos ejemplos más populares de fluidos cosmológicos se conocen como {\bf polvo} y {\bf radiación}.
El polvo es materia no relativista y sin colisiones, que obedece a $w=0$.
Por ejemplo, las estrellas y galaxias ordinarias, en las que la presión es insignificante en comparación con la densidad de energía.
El polvo también se conoce como ``materia'', y los universos cuya densidad de energía se debe principalmente al polvo se conocen como {\bf dominados por la materia}.
La densidad de energía en la materia cae como
\begin{equation}
\rho\propto a^{-3}\,.\label{8.24}
\end{equation}
Esto se interpreta simplemente como la disminución de la densidad numérica de partículas a medida que el universo se expande.
(Para el polvo, la densidad de energía está dominada por la energía en reposo, que es proporcional a la densidad numérica). ``Radiación'' puede usarse para describir la radiación electromagnética real o partículas masivas que se mueven a velocidades relativas suficientemente cercanas a la velocidad de luz que se vuelven indistinguibles de los fotones (al menos en lo que respecta a su ecuación de estado).
Aunque la radiación es un fluido perfecto y por lo tanto tiene un tensor de energía-momento dado por (8.15), también sabemos que $T_\mn$ se puede expresar en términos de la intensidad del campo como
\begin{equation}
T^\mn = {1\over{4\pi}}(F^{\mu\lambda}F^\nu{}_\lambda
-{1\over 4}g^{\mu\nu} F^{\lambda\sigma}F_{\lambda\sigma})\,.
\label{8.25}
\end{equation}
La huella de esto está dada por
\begin{equation}
T^\mu{}_\mu = {1\over{4\pi}}\left[F^{\mu\lambda}F_{\mu\lambda}
-{1\over 4}(4)F^{\lambda\sigma}F_{\lambda\sigma}\right] = 0\,.\label{8.26}
\end{equation}
Pero esto también debe ser igual a (8.19), por lo que la ecuación de estado es
\begin{equation}
p = {1\over 3}\rho\,.\label{8.27}
\end{equation}
Un universo en el que la mayor parte de la densidad de energía se encuentra en forma de radiación se conoce como {\bf dominado por la radiación}.
La densidad de energía en la radiación disminuye a medida que
\begin{equation}
\rho \propto a^{-4}\,.\label{8.28}
\end{equation}
Así, la densidad de energía en la radiación disminuye ligeramente más rápido que en la materia; esto se debe a que la densidad de número de fotones disminuye de la misma manera que la densidad de número de partículas no relativistas, pero los fotones individuales también pierden energía como $a^{-1}$ a medida que se desplazan al rojo, como veremos más adelante.
(Del mismo modo, las partículas masivas pero relativistas perderán energía a medida que ``se desaceleren'' en coordenadas en movimiento).
Creemos que hoy la densidad de energía del universo está dominada por la materia, con $\rho_{\rm mat}/\rho_{\rm rad}\sim10^6$.
Sin embargo, en el pasado el universo era mucho más pequeño y la densidad de energía de la radiación habría dominado en épocas muy tempranas.

Hay otra forma de energía-momento que a veces se considera: la del vacío mismo.
Introducir energía en el vacío equivale a introducir una constante cosmológica.
Las ecuaciones de Einstein con una constante cosmológica son
\begin{equation}
G_\mn = 8\pi GT_\mn -\Lambda g_\mn \ ,\label{8.29}
\end{equation}
que es claramente la misma forma que las ecuaciones sin constante cosmológica pero con un tensor de energía-momento para el vacío,
\begin{equation}
T^{\rm (vac)}_\mn = -{{\Lambda}\over{8\pi G}}g_\mn\,.\label{8.30}
\end{equation}
Este tiene la forma de un fluido perfecto con
\begin{equation}
\rho = -p = {{\Lambda}\over{8\pi G}}\,.\label{8.31}
\end{equation}
Por lo tanto, tenemos $w=-1$ y la densidad de energía es independiente de $a$, que es lo que esperaríamos para la densidad de energía del vacío.
Dado que la densidad de energía en la materia y la radiación disminuye a medida que el universo se expande, si hay una energía en el vacío distinta de cero, esta tiende a prevalecer en el largo plazo (siempre que el universo no comience a contraerse).
Si esto sucede, decimos que el universo queda {\bf dominado por el vacío}.

Pasemos ahora a las ecuaciones de Einstein.
Recuerde que se pueden escribir en la forma (4.45):
\begin{equation}
R_\mn = 8\pi G\left(T_\mn - {1\over 2}g_\mn T\right)\,.
\label{8.32}
\end{equation}
La ecuación $\mn = 00$ es
\begin{equation}
-3{{\ddot a}\over a}=4\pi G(\rho+3p)\ ,\label{8.33}
\end{equation}
y las ecuaciones $\mn = ij$ dan
\begin{equation}
{{\ddot a}\over a} + 2\left({{\dot a}\over a}\right)^2
+2{k\over{a^2}}= 4\pi G(\rho-p)\,.\label{8.34}
\end{equation}
(Solo hay una ecuación distinta de $\mn = ij$, debido a la isotropía).
Podemos usar (8.33) para eliminar las segundas derivadas en (8.34) y hacer un poco de limpieza para obtener
\begin{equation}
{{\ddot a}\over a}=-{{4\pi G}\over 3}(\rho+3p)\ ,\label{8.35}
\end{equation}
y
\begin{equation}
\left({{\dot a}\over a}\right)^2={{8\pi G}\over 3}\rho
-{{k}\over a^2}\,.\label{8.36}
\end{equation}
En conjunto, se conocen como {\bf ecuaciones de Friedmann}, y las métricas de la forma (8.7) que obedecen a estas ecuaciones definen los universos de Friedmann-Robertson-Walker (FRW).

Hay mucha terminología asociada con los parámetros cosmológicos, y aquí solo presentaremos los conceptos básicos.
La tasa de expansión se caracteriza por el {\bf parámetro de Hubble},
\begin{equation}
H ={{\dot a}\over a}\,.\label{8.37}
\end{equation}
El valor del parámetro de Hubble en la época actual es la constante de Hubble, $H_0$.
Actualmente existe una gran controversia sobre cuál es su valor real, con mediciones que oscilan entre 40 y 90 km/seg/Mpc.
(``Mpc'' significa ``megaparsec'', que es $3\times 10^{24}$ cm.)
Tenga en cuenta que tenemos que dividir $\dot a$ por $a$ para obtener una cantidad mensurable, ya que la escala general de $a$ es irrelevante.
También existe el {\bf parámetro de desaceleración},
\begin{equation}
q = -{{a\ddot a}\over {\dot a^2}}\ ,\label{8.38}
\end{equation}
que mide la tasa de cambio de la tasa de expansión.

Otra cantidad útil es el {\bf parámetro de densidad},
\begin{align}
\Omega  &=  {{8\pi G}\over {3H^2}}\rho \notag \\
&=  {\rho\over{\rho_{\rm crit}}}\ ,\label{8.39}
\end{align}
donde la {\bf densidad crítica} está definida por
\begin{equation}
\rho_{\rm crit} = {{3H^2}\over{8\pi G}}\,.\label{8.40}
\end{equation}
Esta cantidad (que generalmente cambiará con el tiempo) se llama densidad ``crítica'' porque la ecuación de Friedmann (8.36) se puede escribir
\begin{equation}
\Omega-1={{k}\over {H^2 a^2}} \,.\label{8.41}
\end{equation}
Por lo tanto, el signo de $k$ está determinado por si $\Omega$ es mayor, igual o menor que uno.
Tenemos
\begin{equation*}
\mqty{\rho<\rho_{\rm crit} & \leftrightarrow & \Omega < 1 &
\leftrightarrow & k=-1 & \leftrightarrow & {\rm open} \\
\rho=\rho_{\rm crit} & \leftrightarrow & \Omega = 1 &
\leftrightarrow & k=0 & \leftrightarrow & {\rm flat} \\
\rho>\rho_{\rm crit} & \leftrightarrow & \Omega > 1 &
\leftrightarrow & k=+1 & \leftrightarrow & {\rm closed}\,. \\ }
\end{equation*}
Entonces, el parámetro de densidad nos dice cuál de las tres geometrías de Robertson-Walker describe nuestro universo.
Determinarlo observacionalmente es un área de intensa investigación.

Es posible resolver las ecuaciones de Friedmann exactamente en varios casos simples, pero suele ser más útil conocer el comportamiento cualitativo de varias posibilidades.
Fijemos por el momento $\Lambda=0$ y consideremos el comportamiento de universos llenos de fluidos de energía positiva ($\rho > 0$) y presión no negativa ($p\geq 0$).
Luego por (8.35) debemos tener $\ddot a<0$.
Como sabemos por observaciones de galaxias distantes que el universo se está expandiendo ($\dot a>0$), esto significa que el universo se está ``desacelerando''.
Esto es lo que deberíamos esperar, ya que la atracción gravitacional de la materia en el universo actúa en contra de la expansión.
El hecho de que el universo sólo pueda desacelerarse significa que debe haberse expandido aún más rápido en el pasado; Si rastreamos la evolución hacia atrás en el tiempo, necesariamente alcanzamos una singularidad en $a=0$.
Observe que si $\ddot a$ fuera exactamente cero, $a(t)$ sería una línea recta y la edad del universo sería $H_0^{-1}$.
Dado que $\ddot a$ es en realidad negativo, el universo debe ser algo más joven que eso.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/eight01.pdf}
\end{figure}

Esta singularidad en $a=0$ es el {\bf Big Bang}.
Representa la creación del universo a partir de un estado singular, no una explosión de materia en un Espacio-Tiempo preexistente.
Se podría esperar que la perfecta simetría de nuestros universos FRW fuera la responsable de esta singularidad, pero en realidad no es cierto; Los teoremas de singularidad predicen que cualquier universo con $\rho>0$ y $p\geq 0$ debe haber comenzado en una singularidad.
Por supuesto, la densidad de energía se vuelve arbitrariamente alta como $a\rightarrow 0$, y no esperamos que la Relatividad General clásica sea una descripción precisa de la naturaleza en este régimen; Es de esperar que una teoría consistente de la gravedad cuántica pueda arreglar las cosas.

La evolución futura es diferente para diferentes valores de $k$.
Para los casos abierto y plano, $k\leq 0$, (8.36) implica
\begin{equation}
\dot a^2 = {{8\pi G}\over 3}\rho a^2 + |k|\,.\label{8.42}
\end{equation}
El lado derecho es {\it estrictamente} positivo (ya que suponemos $\rho>0$), por lo que $\dot a$ nunca pasa por cero.
Como sabemos que hoy $\dot a>0$, debe ser positivo para todos los tiempos.
Por lo tanto, los universos abiertos y planos se expanden para siempre: están abiertos tanto temporal como espacialmente.
(Tenga en cuenta las suposiciones que se incluyen en esto, es decir, que existe una densidad de energía positiva distinta de cero.
Los universos con densidad de energía negativa no tienen que expandirse eternamente, incluso si están ``abiertos''.)

¿A qué velocidad se siguen expandiendo estos universos? Considere la cantidad $\rho a^3$ (que es constante en universos dominados por la materia).
Por la ecuación de conservación de energía (8.20) tenemos
\begin{align}
{{d}\over {dt}}(\rho a^3)  &=
a^3\left(\dot\rho + 3\rho{{\dot a}\over a}\right) \notag \\
&=  -3p a^2\dot a\,.\label{8.43}
\end{align}
El lado derecho es cero o negativo; por lo tanto
\begin{equation}
{{d}\over {dt}}(\rho a^3)\leq 0\,.\label{8.44}
\end{equation}
Esto implica a su vez que $\rho a^2$ debe llegar a cero en un universo en constante expansión, donde $a\rightarrow\infty$.
Así (8.42) nos dice que
\begin{equation}
\dot a^2\rightarrow |k|\,.\label{8.45}
\end{equation}
(Recuerde que esto es cierto para $k\leq 0$.)
Así, para $k=-1$ la expansión se acerca al valor límite $\dot a\rightarrow 1$, mientras que para $k=0$ el universo sigue expandiéndose, pero cada vez más lentamente.

Para los universos cerrados ($k=+1$), (8.36) se convierte en
\begin{equation}
\dot a^2 = {{8\pi G}\over 3}\rho a^2 -1\,.\label{8.46}
\end{equation}
El argumento de que $\rho a^2\rightarrow 0$ como $a\rightarrow\infty$ todavía se aplica; pero en ese caso (8.46) se volvería negativo, lo cual no puede suceder.
Por tanto, el universo no se expande indefinidamente; $a$ posee un límite superior $a_{\rm max}$.
A medida que $a$ se acerca a $a_{\rm max}$, (8.35) implica
\begin{equation}
\ddot a \rightarrow -{{4\pi G}\over 3}(\rho +3p)a_{\rm max} <0
\,.\label{8.47}
\end{equation}
Por lo tanto, $\ddot a$ es finito y negativo en este punto, por lo que $a$ llega a $a_{\rm max}$ y comienza a disminuir, después de lo cual (desde $\ddot a <0$) inevitablemente continuará contrayéndose hasta cero: el Big Crunch.
Por lo tanto, los universos cerrados (nuevamente, según nuestros supuestos de $\rho$ positivo y $p$ no negativo) están cerrados tanto en el tiempo como en el espacio.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{imagenes/eight02.pdf}
\end{figure}

Ahora enumeraremos algunas de las soluciones exactas correspondientes a un solo tipo de densidad de energía.
Para universos de solo polvo ($p=0$), es conveniente definir un {\bf ángulo de desarrollo} $\phi(t)$, en lugar de usar $t$ como parámetro directamente.
Las soluciones son entonces, para universos abiertos,
\begin{equation}
\begin{cases}
a={C\over 2}(\cosh\phi-1) \\
t={C\over 2}(\sinh\phi-\phi)
\end{cases} \qquad (k=-1)\,,
\label{8.48}
\end{equation}
para universos planos,
\begin{equation}
a = \left({{9C}\over 4}\right)^{1/3} t^{2/3}\qquad (k=0)\ ,
\label{8.49}
\end{equation}
y para universos cerrados,
\begin{equation}
\begin{cases}
a = {C\over 2}(1-\cos\phi) \\
t = {C\over 2}(\phi-\sin\phi)\
\end{cases}\qquad (k=+1)\ ,\label{8.50}
\end{equation}
donde hemos definido
\begin{equation}
C={{8\pi G}\over 3}\rho a^3 = {\rm ~constant}\,.\label{8.51}
\end{equation}
Para los universos llenos únicamente de radiación, $p={1\over 3}\rho$, tenemos nuevamente universos abiertos,
\begin{equation}
a=\sqrt{C'}\left[\left(1+{t\over{\sqrt{C'}}}\right)^2-1\right]^{1/2}
\qquad (k=-1)\ ,\label{8.52}
\end{equation}
universos planos,
\begin{equation}
a=(4C')^{1/4} t^{1/2}\qquad (k=0)\ ,\label{8.53}
\end{equation}
y universos cerrados,
\begin{equation}
a=\sqrt{C'}\left[1-\left(1-{t\over{\sqrt{C'}}}\right)^2\right]^{1/2}
\qquad (k=+1)\ ,\label{8.54}
\end{equation}
donde esta vez definimos
\begin{equation}
C'={{8\pi G}\over 3}\rho a^4 = {\rm ~constant}\,.\label{8.55}
\end{equation}
Pueden comprobar por sí mismos que estas soluciones exactas tienen las propiedades que, según creemos, tendrían en general.

Para universos que están vacíos salvo por la constante cosmológica, $\rho$ o $p$ serán negativos, en violación de los supuestos que utilizamos anteriormente para derivar el comportamiento general de $a(t)$.
En este caso, se pierde la conexión entre abrir/cerrar y expandirse para siempre/contraerse.
Comenzamos considerando $\Lambda<0$.
En este caso $\Omega$ es negativo, y desde (8.41) esto sólo puede suceder si $k=-1$.
La solución en este caso es
\begin{equation}
a = \sqrt{{-3}\over\Lambda}\sin\left(\sqrt{{-\Lambda}\over 3} \, t
\right)\,.\label{8.56}
\end{equation}
También existe una solución abierta ($k=-1$) para $\Lambda>0$, dada por
\begin{equation}
a = \sqrt{{3}\over\Lambda}\sinh\left(\sqrt{{\Lambda}\over 3} \, t
\right)\,.\label{8.57}
\end{equation}
Un universo plano dominado por el vacío debe tener $\Lambda>0$, y la solución es
\begin{equation}
a\propto \exp\left(\pm\sqrt{{\Lambda}\over 3} \, t
\right)\ ,\label{8.58}
\end{equation}
mientras que el universo cerrado también debe tener $\Lambda>0$, y satisface
\begin{equation}
a= \sqrt{{3}\over\Lambda}\cosh\left(\sqrt{{\Lambda}\over 3} \, t
\right)\,.\label{8.59}
\end{equation}
Estas soluciones son un poco engañosas.
De hecho, las tres soluciones para $\Lambda>0$ --- (8.57), (8.58) y (8.59) --- representan el mismo Espacio-Tiempo, solo que en diferentes coordenadas.
Este Espacio-Tiempo, conocido como {\bf de Sitter space}, en realidad es máximamente simétrico como Espacio-Tiempo.
(Ver Hawking y Ellis para más detalles.)
La solución $\Lambda<0$ (8.56) también es máximamente simétrica y se conoce como {\bf espacio anti-de Sitter}.

Está claro que nos gustaría determinar observacionalmente una serie de cantidades para decidir cuál de los modelos FRW corresponde a nuestro universo.
Obviamente nos gustaría determinar $H_0$, ya que está relacionado con la edad del universo.
(Para un universo $k=0$ puramente dominado por la materia, (8.49) implica que la edad es $2/(3H_0)$.
Otras posibilidades predecirían relaciones similares.)
También nos gustaría saber $\Omega$, que determina $k$ hasta (8.41).
Dada la definición (8.39) de $\Omega$, esto significa que queremos saber tanto $H_0$ como $\rho_0$.
Lamentablemente, ambas cantidades son difíciles de medir con precisión, especialmente $\rho$.
Pero observe que el parámetro de desaceleración $q$ se puede relacionar con $\Omega$ usando (8.35):
\begin{align}
q  &=  -{{a\ddot a}\over {\dot a^2}} \notag \\
&=  -H^{-2}{{\ddot a}\over a} \notag \\
&=  {{4\pi G}\over {3H^2}}(\rho+3p) \notag \\
&=  {{4\pi G}\over {3H^2}}\rho(1+3w) \notag \\
&=  {{1+3w}\over 2}\Omega\,.\label{8.60}
\end{align}
Por lo tanto, si creemos que sabemos qué es $w$ ({\it es decir}, de qué tipo de material está hecho el universo), podemos determinar $\Omega$ midiendo $q$.
(Desafortunadamente, no estamos completamente seguros de conocer $w$ y $q$ es en sí mismo difícil de medir.
Pero la gente lo está intentando.)

Para comprender cómo podrían medirse estas cantidades, consideremos el movimiento geodésico en un universo FRW.
Hay varios vectores Killing de tipo espacial, pero ningún vector de Killing de tipo temporal que nos dé una noción de energía conservada.
Sin embargo, existe un tensor asesino.
Si $U^\mu=(1,0,0,0)$ es la cuatro velocidades de los observadores comoving, entonces el tensor
\begin{equation}
K_\mn = a^2(g_\mn + U_\mu U_\nu)\label{8.61}
\end{equation}
satisface $\nabla_{(\sigma}K_{\mn)}=0$ (como puede comprobar) y, por tanto, es un tensor Killing.
Esto significa que si una partícula tiene cuatro velocidades $V^\mu = dx^\mu/d\lambda$, la cantidad
\begin{equation}
K^2 = K_\mn V^\mu V^\nu = a^2[V_\mu V^\mu + (U_\mu V^\mu)^2]\label{8.62}
\end{equation}
será una constante a lo largo de las geodésicas.
Pensemos en esto, primero en el caso de las partículas masivas.
Entonces tendremos $V_\mu V^\mu =-1$, o
\begin{equation}
(V^0)^2 = 1+|\vec V|^2\ ,\label{8.63}
\end{equation}
donde $|\vec V|^2 = g_{ij}V^iV^j$.
Entonces (8.61) implica
\begin{equation}
|\vec V| = {{K}\over a}\,.\label{8.64}
\end{equation}
Por lo tanto, la partícula ``se desacelera'' con respecto a las coordenadas comomoves a medida que el universo se expande.
De hecho, se trata de una desaceleración real, en el sentido de que un gas de partículas con velocidades relativas inicialmente altas se enfriará a medida que el universo se expanda.

Algo similar ocurre con las geodésicas nulas.
En este caso $V_\mu V^\mu =0$, y (8.62) implica
\begin{equation}
U_\mu V^\mu = {{K}\over{a}}\,.\label{8.65}
\end{equation}
Pero la frecuencia del fotón medida por un observador en movimiento es $\omega=-U_\mu V^\mu$.
Por lo tanto, la frecuencia del fotón emitido con frecuencia $\omega_1$ se observará con una frecuencia más baja $\omega_0$ a medida que el universo se expande:
\begin{equation}
{{\omega_0}\over{\omega_1}} = {{a_1}\over {a_0}}\,.\label{8.66}
\end{equation}
A los cosmólogos les gusta hablar de esto en términos del {\bf redshift} $z$ entre los dos eventos, definido por el cambio fraccionario en la longitud de onda:
\begin{align}
z  &=  {{\lambda_0-\lambda_1}\over{\lambda_1}} \notag \\
&=  {{a_0}\over {a_1}}-1\,.\label{8.67}
\end{align}
Observe que este corrimiento al rojo no es lo mismo que el efecto Doppler convencional; es la expansión del espacio, no las velocidades relativas del observador y el emisor, lo que conduce al corrimiento al rojo.

El desplazamiento al rojo es algo que podemos medir; conocemos las longitudes de onda en reposo de varias líneas espectrales en la radiación de galaxias lejanas, por lo que podemos saber cuánto han cambiado sus longitudes de onda a lo largo del camino desde el tiempo $t_1$ en que fueron emitidas hasta el tiempo $t_0$ en que fueron observadas.
Por tanto, conocemos la relación de los factores de escala en estos dos momentos.
Pero no conocemos los tiempos mismos; los fotones no son lo suficientemente inteligentes como para decirnos cuánto tiempo coordinado ha transcurrido en su viaje.
Tenemos que trabajar más duro para extraer esta información.

En términos generales, dado que un fotón se mueve a la velocidad de la luz, su tiempo de viaje debería ser simplemente su distancia.
Pero ¿cuál es la ``distancia'' de una galaxia lejana en un universo en expansión?
La distancia de comovimiento no es especialmente útil, ya que no es mensurable y, además, porque las galaxias no necesitan estar en comovimiento en general.
En lugar de eso, podemos definir la {\bf distancia de luminosidad} como
\begin{equation}
d^2_L = {{L}\over{4\pi F}}\ ,\label{8.68}
\end{equation}
donde $L$ es la luminosidad absoluta de la fuente y $F$ es el flujo medido por el observador (la energía por unidad de tiempo por unidad de área de algún detector).
La definición proviene del hecho de que en el espacio plano, para una fuente a la distancia $d$ el flujo sobre la luminosidad es solo uno sobre el área de una esfera centrada alrededor de la fuente, $F/L=1/A(d)=1/4\pi d^2$.
Sin embargo, en un universo FRW, el flujo se diluirá.
La conservación de fotones nos dice que el número total de fotones emitidos por la fuente eventualmente pasará a través de una esfera a una distancia comoving $r$ del emisor.
Dicha esfera está a una distancia física $d=a_0r$, donde $a_0$ es el factor de escala cuando se observan los fotones.
Pero el flujo se diluye por dos efectos adicionales: los fotones individuales se desplazan al rojo en un factor $(1+z)$, y los fotones golpean la esfera con menos frecuencia, ya que dos fotones emitidos con un intervalo de tiempo $\delta t$ se medirán con un intervalo de tiempo $(1+z)\delta t$.
Por lo tanto tendremos
\begin{equation}
{F\over L} = {1\over{4\pi a_0^2 r^2 (1+z)^2}}\ ,\label{8.69}
\end{equation}
o
\begin{equation}
d_L = a_0 r (1+z)\,.\label{8.70}
\end{equation}
La distancia de luminosidad $d_L$ es algo que podríamos esperar medir, ya que hay algunas fuentes astrofísicas cuyas luminosidades absolutas se conocen (``candelas estándar'').
Pero $r$ no es observable, por lo que tenemos que eliminarlo de nuestra ecuación.
En una geodésica nula (elegida como radial por conveniencia) tenemos
\begin{equation}
0 =ds^2 = - d\,t^2 + {{a^2}\over {1-kr^2}} d\,r^2\ ,\label{8.71}
\end{equation}
o
\begin{equation}
\int_{t_1}^{t_0} {{dt}\over{a(t)}}
= \int_{0}^{r} {{dr}\over{(1-kr^2)^{1/2}}}\,.
\label{8.72}
\end{equation}
Para galaxias no muy lejanas, podemos expandir el factor de escala en una serie de Taylor alrededor de su valor actual:
\begin{equation}
a(t_1)=a_0 + (\dot a)_0(t_1-t_0) +{1\over 2}(\ddot a)_0
(t_1-t_0)^2 + \ldots \,.\label{8.73}
\end{equation}
Luego podemos expandir ambos lados de (8.72) para encontrar
\begin{equation}
r= a_0^{-1}\left[(t_0-t_1)+{1\over 2}H_0(t_0-t_1)^2 +\ldots
\right]\,.
\label{8.74}
\end{equation}
Recordando ahora (8.67), el desarrollo (8.73) es el mismo que
\begin{equation}
{1\over {1+z}} = 1+H_0(t_1-t_0) -{1\over 2}q_0 H_0^2
(t_1-t_0)^2 + \ldots \,.\label{8.75}
\end{equation}
Para $H_0(t_1-t_0)$ pequeño, esto se puede invertir para producir
\begin{equation}
t_0-t_1 = H_0^{-1}\left[z-\left(1+{{q_0}\over 2}\right)
z^2 + \ldots\right]\,.\label{8.76}
\end{equation}
Sustituyendo esto nuevamente en (8.74) se obtiene
\begin{equation}
r = {1\over{a_0 H_0}}\left[z-{1\over 2}\left(1+q_0\right)
z^2 + \ldots\right]\,.\label{8.77}
\end{equation}
Finalmente, usando esto en (8.70) se obtiene {\bf Ley de Hubble}:
\begin{equation}
d_L = H_0^{-1}\left[ z+{1\over 2}(1-q_0)z^2 + \ldots\right]\,.
\label{8.78}
\end{equation}
Por lo tanto, la medición de las distancias de luminosidad y los corrimientos al rojo de un número suficiente de galaxias nos permite determinar $H_0$ y $q_0$ y, por lo tanto, nos lleva un largo camino para decidir en qué tipo de universo FRW vivimos.
Las observaciones en sí son extremadamente difíciles y los valores de estos parámetros en el mundo real todavía son muy discutidos.
Durante la próxima década, una variedad de nuevas estrategias y una aplicación más precisa de las antiguas podrían muy bien responder estas preguntas de una vez por todas.


%\setlength{\bibsep}{4pt}


\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Referencias}
%\baselineskip=12pt
%\parindent= 0pt
%\parskip = 8pt


\bibitem{bib001} B. F. Schutz, \textit{Un primer curso de relatividad general} (Cambridge, 1985) [*]. Este es un texto introductorio muy bonito. Especialmente útil si, por ejemplo, no tienes muy claro qué significa realmente el tensor de energía-momento.

\bibitem{bib002} S. Weinberg, \textit{Gravitación y cosmología} (Wiley, 1972) [**]. Un libro realmente bueno en lo que hace, especialmente sólido en astrofísica, cosmología y pruebas experimentales. Sin embargo, adopta un enfoque no geométrico inusual para el material y no analiza los agujeros negros.

\bibitem{bib003} C. Misner, K. Thorne y J. Wheeler, \textit{Gravitación} (Freeman, 1973) [**]. Un libro pesado, en varios sentidos. La mayoría de las cosas que quieres saber están aquí, aunque es posible que tengas que esforzarte mucho para llegar a ellas (quizás aprendiendo algo inesperado en el proceso).

\bibitem{bib004} R. Wald, \textit{Relatividad General} (Chicago, 1984) [***]. Discusiones exhaustivas sobre una serie de temas avanzados, incluidos los agujeros negros, la estructura global y los espinores. El enfoque es matemáticamente más exigente que los libros anteriores y los conceptos básicos se cubren con bastante rapidez.

\bibitem{bib005} E. Taylor y J. Wheeler, \textit{Física del espacio-tiempo} (Freeman, 1992) [*]. Una buena introducción a la relatividad especial.

\bibitem{bib006} R. D'Inverno, \textit{Introducción a la relatividad de Einstein} (Oxford, 1992) [**]. Un libro que no he leído con mucha atención, pero parece que se tratan todos los temas correctos sin distorsiones ideológicas perceptibles.

\bibitem{bib007} A. P. Lightman, W. H. Press, R. H. Price y S. A. Teukolsky, \textit{Problem Book in Relativity and Gravitation} (Princeton, 1975) [**]. Una colección considerable de problemas en todas las áreas de GR, con soluciones completamente elaboradas, lo que hace aún más difícil para los instructores inventar problemas cuyas respuestas los estudiantes no pueden encontrar fácilmente.

\bibitem{bib008} N. Straumann, \textit{Relatividad general y astrofísica relativista} (Springer-Verlag, 1984) [***]. Un libro de bastante alto nivel, que comienza con una gran cantidad de geometría abstracta y continúa con discusiones detalladas sobre la estructura estelar y otros temas astrofísicos.

\bibitem{bib009} F. de Felice y C. Clarke, \textit{Relatividad sobre variedades curvas} (Cambridge, 1990) [***]. Un enfoque matemático, pero con un excelente énfasis en cantidades físicamente mensurables.

\bibitem{bib010} S. Hawking y G. Ellis, \textit{La estructura a gran escala del espacio-tiempo} (Cambridge, 1973) [***]. Un libro avanzado que enfatiza las técnicas globales y los teoremas de singularidad.

\bibitem{bib011} R. Sachs y H. Wu, \textit{Relatividad general para matemáticos} (Springer-Verlag, 1977) [***]. Justo lo que dice el título, aunque el estilo típicamente seco de la prosa matemática está aquí animado por frecuentes comentarios obstinados sobre la física y las matemáticas (y el estado del mundo).

\bibitem{bib012} B. Schutz,\textit{ Métodos geométricos de física matemática} (Cambridge, 1980) [**]. Otro buen libro de Schutz, que cubre algunos puntos matemáticos que quedan fuera del libro GR (pero a un nivel muy accesible). Se incluyen discusiones sobre derivados de Lie, formas diferenciales y aplicaciones a física distinta de GR.

\bibitem{bib013} V. Guillemin y A. Pollack,\textit{ Topología diferencial} (Prentice-Hall, 1974) [**]. Un entretenido estudio de variedades, topología, formas diferenciales y teoría de la integración.

\bibitem{bib014} C. Nash y S. Sen, \textit{Topología y geometría para físicos} (Academic Press, 1983) [***]. Incluye homotopía, homología, haces de fibras y teoría Morse, con aplicaciones a la física; algo conciso.

\bibitem{bib015} F. W. Warner, \textit{Fundamentos de variedades diferenciables y grupos de Lie} (Springer-Verlag, 1983) [***]. El texto estándar en el campo incluye temas básicos como variedades y campos tensoriales, así como temas más avanzados.

\vspace{1cm}
El nivel típico de dificultad (especialmente matemático) de los libros se indica mediante una serie de asteriscos, uno de los cuales significa principalmente introductorio y tres son avanzados. Los asteriscos están normalizados para estas notas de clase, que se darían [**]. Los primeros cuatro libros fueron consultados con frecuencia en la preparación de estas notas, los siete siguientes son otros textos sobre relatividad que he encontrado útiles y los últimos cuatro son referencias de antecedentes matemáticos.

\end{thebibliography}

\end{document}
